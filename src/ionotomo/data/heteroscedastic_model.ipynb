{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook shows how to performa heteroscedastic lengthscale, variance, and noise solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/anaconda3/envs/kerastf/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/josh/anaconda3/envs/kerastf/lib/python3.6/site-packages/cmocean/tools.py:76: MatplotlibDeprecationWarning: The is_string_like function was deprecated in version 2.1.\n",
      "  if not mpl.cbook.is_string_like(rgbin[0]):\n",
      "/home/josh/anaconda3/envs/kerastf/lib/python3.6/site-packages/multipledispatch-0.4.9-py3.6.egg/multipledispatch/dispatcher.py:24: AmbiguityWarning: \n",
      "Ambiguities exist in dispatched function _expectation\n",
      "\n",
      "The following signatures may result in ambiguous behavior:\n",
      "\t[Gaussian, Linear, NoneType, Sum, InducingPoints], [Gaussian, Identity, NoneType, Kernel, InducingPoints]\n",
      "\n",
      "\n",
      "Consider making the following additions:\n",
      "\n",
      "@dispatch(Gaussian, Identity, NoneType, Sum, InducingPoints)\n",
      "def _expectation(...)\n",
      "  warn(warning_text(dispatcher.name, ambiguities), AmbiguityWarning)\n"
     ]
    }
   ],
   "source": [
    "from ionotomo import *\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "plt.style.use('ggplot')\n",
    "import astropy.units as au\n",
    "import os\n",
    "\n",
    "import gpflow as gp\n",
    "from doubly_stochastic_dgp.dgp import DGP\n",
    "from scipy.cluster.vq import kmeans2\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(message)s')\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpflow import settings,  densities, transforms, kullback_leiblers, features, conditionals\n",
    "from gpflow.core.compilable import Build\n",
    "from gpflow.params import Parameterized, ParamList, DataHolder, Parameter, Minibatch, DataHolder\n",
    "from gpflow.decors import autoflow,params_as_tensors, params_as_tensors_for\n",
    "from gpflow.mean_functions import Zero\n",
    "from gpflow.models import SVGP\n",
    "from gpflow.quadrature import hermgauss\n",
    "from gpflow.likelihoods import Likelihood, SwitchedLikelihood\n",
    "from gpflow.actions import Loop, Action\n",
    "from gpflow.training import NatGradOptimizer, AdamOptimizer, XiSqrtMeanVar\n",
    "from gpflow.kernels import Kernel\n",
    "\n",
    "class NonstationaryKernel(Kernel):\n",
    "    def __init__(self,input_dim, Z=None, hetero_sigma=True, hetero_ls=True,  active_dims=None, name=None):\n",
    "        super(NonstationaryKernel, self).__init__(input_dim, active_dims, name=name)\n",
    "        self.hetero_sigma = hetero_sigma\n",
    "        self.hetero_ls = hetero_ls\n",
    "        if hetero_sigma:\n",
    "            assert Z is not None, \"Requires inducing points\"\n",
    "            mean = gp.mean_functions.Constant(1.)\n",
    "            kern = gp.kernels.RBF(input_dim,active_dims=active_dims,variance=0.1,name='hetero_sigma_kern')\n",
    "            kern.variance.set_trainable(False)# = gp.priors.Gaussian(0.,0.1)\n",
    "            self.sigma_latent = PositiveHeteroscedasticLatent(Z, mean, kern, feat=None, q_diag=False, whiten=True, name=\"hetero_sigma_latent\")\n",
    "        else:\n",
    "            self.stationary_sigma = Parameter(1., transform=transforms.positive)\n",
    "        if hetero_ls:\n",
    "            assert Z is not None, \"Requires inducing points\"\n",
    "            mean = gp.mean_functions.Constant(1.)\n",
    "            kern = gp.kernels.RBF(input_dim,active_dims=active_dims,variance=0.1,name='hetero_ls_kern')\n",
    "            kern.variance.set_trainable(False)# = gp.priors.Gaussian(0.,0.1)\n",
    "            self.ls_latent = PositiveHeteroscedasticLatent(Z, mean, kern, feat=None, q_diag=False, whiten=True, name=\"hetero_ls_latent\")\n",
    "        else:\n",
    "            self.stationary_ls = Parameter(1., transform=transforms.positive)\n",
    "            \n",
    "    @params_as_tensors\n",
    "    def Kdiag(self, X, presliced=False):\n",
    "        if not presliced:\n",
    "            X, _ = self._slice(X, None)\n",
    "            \n",
    "        if self.hetero_sigma:\n",
    "            sigma = self.sigma_latent.predict_f_samples(X)[0,:,0]#K\n",
    "        else:\n",
    "            sigma = self.stationary_sigma * tf.ones(tf.stack([tf.shape(X)[0]]),dtype=settings.float_type)\n",
    "        \n",
    "        Kdiag = sigma**2 #tf.fill(tf.stack([tf.shape(X)[0]]), 1.)\n",
    "        return Kdiag\n",
    "    \n",
    "    @params_as_tensors\n",
    "    def K(self, X, X2=None, presliced=False):\n",
    "        if not presliced:\n",
    "            X, X2 = self._slice(X, X2)\n",
    "        \n",
    "        if X2 is not None:\n",
    "            X_ = [X,X2]\n",
    "        else:\n",
    "            X_ = X\n",
    "        X_len = tf.shape(X)[0]\n",
    "        if self.hetero_ls:\n",
    "            ls_ = self.ls_latent.predict_f_samples(X_)#SN1\n",
    "            if X2 is not None:\n",
    "                ls = ls_[0,:X_len,:]#N1\n",
    "                ls2 = tf.transpose(ls_[0,X_len:,:])#1K\n",
    "            else:\n",
    "                ls = ls_[0,:,:]#N1\n",
    "                ls2 = tf.transpose(ls)#1N\n",
    "        else:\n",
    "            ls = self.stationary_ls\n",
    "            ls2 = ls\n",
    "        if self.hetero_sigma:\n",
    "            sigma_ = self.sigma_latent.predict_f_samples(X_)#M1\n",
    "            if X2 is not None:\n",
    "                sigma = sigma_[0,:X_len,:]#N1\n",
    "                sigma2 = tf.transpose(sigma_[0,X_len:,:])#N1)#1K\n",
    "            else:\n",
    "                sigma = sigma_[0,:,:]#N1\n",
    "                sigma2 = tf.transpose(sigma)#1N\n",
    "        else:\n",
    "            sigma = self.stationary_sigma\n",
    "            sigma2 = sigma\n",
    "\n",
    "#         if X2 is None:\n",
    "#             X2 = X\n",
    "#         dist = (X-tf.transpose(X2))**2\n",
    "#         lmag = ls**2 + ls2**2\n",
    "#         K = tf.sqrt(2. * (ls*ls2)/lmag)*tf.exp(-dist/lmag)\n",
    "#         return K\n",
    "\n",
    "        dist = self.square_dist(X,X2)#NK\n",
    "        l_mag = ls**2 + ls2**2#NK\n",
    "        K = sigma*sigma2*tf.sqrt(2.*(ls*ls2)/l_mag) * tf.exp(-dist/l_mag)\n",
    "        return K\n",
    "                   \n",
    "    @params_as_tensors       \n",
    "    def square_dist(self, X, X2):\n",
    "        \"\"\"\n",
    "        Returns ((X - X2ᵀ)/lengthscales)².\n",
    "        Due to the implementation and floating-point imprecision, the\n",
    "        result may actually be very slightly negative for entries very\n",
    "        close to each other.\n",
    "        \"\"\"\n",
    "        X = X\n",
    "        Xs = tf.reduce_sum(tf.square(X), axis=1)\n",
    "\n",
    "        if X2 is None:\n",
    "            dist = -2 * tf.matmul(X, X, transpose_b=True)\n",
    "            dist += tf.reshape(Xs, (-1, 1))  + tf.reshape(Xs, (1, -1))\n",
    "            return dist\n",
    "\n",
    "        X2 = X2\n",
    "        X2s = tf.reduce_sum(tf.square(X2), axis=1)\n",
    "        dist = -2 * tf.matmul(X, X2, transpose_b=True)\n",
    "        dist += tf.reshape(Xs, (-1, 1)) + tf.reshape(X2s, (1, -1))\n",
    "        return dist\n",
    "\n",
    "    \n",
    "class WrappedPhaseGaussian(Likelihood):\n",
    "    def __init__(self, freq, var=1.0, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.freq = freq # frequency the phase is defined at\n",
    "        self.tec_conversion = -8.4480e9 # rad Hz/ tecu\n",
    "        self.tec2phase = tf.convert_to_tensor(self.tec_conversion / self.freq,dtype=settings.float_type,name='tec2phase')\n",
    "        self.variance = gp.params.Parameter(\n",
    "            var, transform=transforms.positive, dtype=settings.float_type)\n",
    "        \n",
    "    def wrap(self, f):\n",
    "        \"\"\"Wraps f into (-pi, pi)\"\"\"\n",
    "        return tf.cast(tf.atan2(tf.sin(f),tf.cos(f)),settings.float_type)\n",
    "    \n",
    "    @params_as_tensors\n",
    "    def logp(self, F, Y):\n",
    "        \"\"\"The log-likelihood function.\"\"\"\n",
    "        phase = self.wrap(F*self.tec2phase)\n",
    "        dphase = self.wrap(phase - self.wrap(Y)) # Ito theorem\n",
    "        return densities.gaussian(dphase, tf.fill(tf.shape(F),tf.cast(0.,settings.float_type)), self.variance)\n",
    "\n",
    "    @params_as_tensors\n",
    "    def conditional_mean(self, F):  # pylint: disable=R0201\n",
    "        \"\"\"The mean of the likelihood conditioned on latent.\"\"\"\n",
    "        phase = F*self.tec2phase\n",
    "        return phase\n",
    "\n",
    "    @params_as_tensors\n",
    "    def conditional_variance(self, F):\n",
    "        return tf.fill(tf.shape(F), tf.squeeze(self.variance))\n",
    "\n",
    "class HeteroscedasticLikelihood_old(Likelihood):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "    @params_as_tensors\n",
    "    def logp(self, F, Y, hetero_variance, *unused_args):\n",
    "        \"\"\"The log-likelihood function.\"\"\"\n",
    "        raise NotImplemented(\"sub class must...\")\n",
    "\n",
    "    @params_as_tensors\n",
    "    def conditional_mean(self, F, *unused_args):  # pylint: disable=R0201\n",
    "        \"\"\"The mean of the likelihood conditioned on latent.\"\"\"\n",
    "        raise NotImplemented(\"sub class must...\")\n",
    "\n",
    "    @params_as_tensors\n",
    "    def conditional_variance(self, F, hetero_variance, *unused_args): # pylint: disable=R0201\n",
    "        \"\"\"The var of the likelihood conditioned on latent.\"\"\"\n",
    "        raise NotImplemented(\"sub class must...\")\n",
    "\n",
    "    def predict_mean_and_var(self, Fmu, Fvar,hetero_variance, *args):\n",
    "        \"\"\"\n",
    "        Given a Normal distribution for the latent function,\n",
    "        return the mean of Y\n",
    "        if\n",
    "            q(f) = N(Fmu, Fvar)\n",
    "        and this object represents\n",
    "            p(y|f)\n",
    "        then this method computes the predictive mean\n",
    "           \\int\\int y p(y|f)q(f) df dy\n",
    "        and the predictive variance\n",
    "           \\int\\int y^2 p(y|f)q(f) df dy  - [ \\int\\int y p(y|f)q(f) df dy ]^2\n",
    "        Here, we implement a default Gauss-Hermite quadrature routine, but some\n",
    "        likelihoods (e.g. Gaussian) will implement specific cases.\n",
    "        \"\"\"\n",
    "        gh_x, gh_w = hermgauss(self.num_gauss_hermite_points)\n",
    "        gh_w /= np.sqrt(np.pi)\n",
    "        gh_w = gh_w.reshape(-1, 1)\n",
    "        shape = tf.shape(Fmu)\n",
    "        Fmu, Fvar,hetero_variance = [tf.reshape(e, (-1, 1)) for e in (Fmu, Fvar,hetero_variance)]\n",
    "        X = gh_x[None, :] * tf.sqrt(2.0 * Fvar) + Fmu\n",
    "\n",
    "        # here's the quadrature for the mean\n",
    "        E_y = tf.reshape(tf.matmul(self.conditional_mean(X), gh_w), shape)\n",
    "\n",
    "        # here's the quadrature for the variance\n",
    "        integrand = self.conditional_variance(X,hetero_variance, *args) \\\n",
    "            + tf.square(self.conditional_mean(X, *args))\n",
    "        V_y = tf.reshape(tf.matmul(integrand, gh_w), shape) - tf.square(E_y)\n",
    "\n",
    "        return E_y, V_y\n",
    "    \n",
    "    def predict_density(self, Fmu, Fvar, Y,hetero_variance, *args):\n",
    "        \"\"\"\n",
    "        Given a Normal distribution for the latent function, and a datum Y,\n",
    "        compute the (log) predictive density of Y.\n",
    "        i.e. if\n",
    "            q(f) = N(Fmu, Fvar)\n",
    "        and this object represents\n",
    "            p(y|f)\n",
    "        then this method computes the predictive density\n",
    "           \\int p(y=Y|f)q(f) df\n",
    "        Here, we implement a default Gauss-Hermite quadrature routine, but some\n",
    "        likelihoods (Gaussian, Poisson) will implement specific cases.\n",
    "        \"\"\"\n",
    "        gh_x, gh_w = hermgauss(self.num_gauss_hermite_points)\n",
    "\n",
    "        gh_w = gh_w.reshape(-1, 1) / np.sqrt(np.pi)\n",
    "        shape = tf.shape(Fmu)\n",
    "        Fmu, Fvar, Y, hetero_variance = [tf.reshape(e, (-1, 1)) for e in (Fmu, Fvar, Y, hetero_variance)]\n",
    "        X = gh_x[None, :] * tf.sqrt(2.0 * Fvar) + Fmu\n",
    "\n",
    "        Y = tf.tile(Y, [1, self.num_gauss_hermite_points])  # broadcast Y to match X\n",
    "\n",
    "        logp = self.logp(X, Y, hetero_variance, *args)\n",
    "        return tf.reshape(tf.log(tf.matmul(tf.exp(logp), gh_w)), shape)\n",
    "\n",
    "    @params_as_tensors\n",
    "    def variational_expectations(self, Fmu, Fvar, Y, hetero_variance, *args):\n",
    "        \"\"\"\n",
    "        Compute the expected log density of the data, given a Gaussian\n",
    "        distribution for the function values.\n",
    "        if\n",
    "            q(f) = N(Fmu, Fvar)\n",
    "        and this object represents\n",
    "            p(y|f)\n",
    "        then this method computes\n",
    "           \\int (\\log p(y|f)) q(f) df.\n",
    "        Here, we implement a default Gauss-Hermite quadrature routine, but some\n",
    "        likelihoods (Gaussian, Poisson) will implement specific cases.\n",
    "        \"\"\"\n",
    "        gh_x, gh_w = hermgauss(self.num_gauss_hermite_points)\n",
    "        gh_x = gh_x.reshape(1, -1)\n",
    "        gh_w = gh_w.reshape(-1, 1) / np.sqrt(np.pi)\n",
    "        shape = tf.shape(Fmu)\n",
    "        Fmu, Fvar, Y, hetero_variance = [tf.reshape(e, (-1, 1)) for e in (Fmu, Fvar, Y, hetero_variance)]\n",
    "        X = gh_x * tf.sqrt(2.0 * Fvar) + Fmu\n",
    "        Y = tf.tile(Y, [1, self.num_gauss_hermite_points])  # broadcast Y to match X\n",
    "        logp = self.logp(X, Y,hetero_variance, *args)\n",
    "        return tf.reshape(tf.matmul(logp, gh_w), shape)\n",
    "\n",
    "class HeteroscedasticLikelihood(Likelihood):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "    @params_as_tensors\n",
    "    def logp(self, F, Y, *args):\n",
    "        \"\"\"The log-likelihood function.\"\"\"\n",
    "        raise NotImplemented(\"sub class must...\")\n",
    "\n",
    "    @params_as_tensors\n",
    "    def conditional_mean(self, F, *args):  # pylint: disable=R0201\n",
    "        \"\"\"The mean of the likelihood conditioned on latent.\"\"\"\n",
    "        raise NotImplemented(\"sub class must...\")\n",
    "\n",
    "    @params_as_tensors\n",
    "    def conditional_variance(self, F, *args): # pylint: disable=R0201\n",
    "        \"\"\"The var of the likelihood conditioned on latent.\"\"\"\n",
    "        raise NotImplemented(\"sub class must...\")\n",
    "\n",
    "    def predict_mean_and_var(self, Fmu, Fvar,*args):\n",
    "        \"\"\"\n",
    "        Given a Normal distribution for the latent function,\n",
    "        return the mean of Y\n",
    "        if\n",
    "            q(f) = N(Fmu, Fvar)\n",
    "        and this object represents\n",
    "            p(y|f)\n",
    "        then this method computes the predictive mean\n",
    "           \\int\\int y p(y|f)q(f) df dy\n",
    "        and the predictive variance\n",
    "           \\int\\int y^2 p(y|f)q(f) df dy  - [ \\int\\int y p(y|f)q(f) df dy ]^2\n",
    "        Here, we implement a default Gauss-Hermite quadrature routine, but some\n",
    "        likelihoods (e.g. Gaussian) will implement specific cases.\n",
    "        \"\"\"\n",
    "        gh_x, gh_w = hermgauss(self.num_gauss_hermite_points)\n",
    "        gh_w /= np.sqrt(np.pi)\n",
    "        gh_w = gh_w.reshape(-1, 1)\n",
    "        shape = tf.shape(Fmu)\n",
    "        Fmu, Fvar = [tf.reshape(e, (-1, 1)) for e in (Fmu, Fvar)]\n",
    "        # each element of arg must have same number of latent\n",
    "        # TODO  tile to match latent if not already\n",
    "        args = [tf.reshape(e, (-1, 1)) if isinstance(e,tf.Tensor) else e for e in args]\n",
    "        X = gh_x[None, :] * tf.sqrt(2.0 * Fvar) + Fmu\n",
    "\n",
    "        # here's the quadrature for the mean\n",
    "        conditional_mean = self.conditional_mean(X,*args)\n",
    "        E_y = tf.reshape(tf.matmul(conditional_mean, gh_w), shape)\n",
    "\n",
    "        # here's the quadrature for the variance\n",
    "        integrand = self.conditional_variance(X, *args) \\\n",
    "            + tf.square(conditional_mean)\n",
    "        V_y = tf.reshape(tf.matmul(integrand, gh_w), shape) - tf.square(E_y)\n",
    "\n",
    "        return E_y, V_y\n",
    "    \n",
    "    def predict_density(self, Fmu, Fvar, Y, *args):\n",
    "        \"\"\"\n",
    "        Given a Normal distribution for the latent function, and a datum Y,\n",
    "        compute the (log) predictive density of Y.\n",
    "        i.e. if\n",
    "            q(f) = N(Fmu, Fvar)\n",
    "        and this object represents\n",
    "            p(y|f)\n",
    "        then this method computes the predictive density\n",
    "           \\int p(y=Y|f)q(f) df\n",
    "        Here, we implement a default Gauss-Hermite quadrature routine, but some\n",
    "        likelihoods (Gaussian, Poisson) will implement specific cases.\n",
    "        \"\"\"\n",
    "        gh_x, gh_w = hermgauss(self.num_gauss_hermite_points)\n",
    "\n",
    "        gh_w = gh_w.reshape(-1, 1) / np.sqrt(np.pi)\n",
    "        shape = tf.shape(Fmu)\n",
    "        Fmu, Fvar, Y = [tf.reshape(e, (-1, 1)) for e in (Fmu, Fvar, Y)]\n",
    "        args = [tf.reshape(e, (-1, 1)) if isinstance(e,tf.Tensor) else e for e in args]\n",
    "        X = gh_x[None, :] * tf.sqrt(2.0 * Fvar) + Fmu\n",
    "\n",
    "        Y = tf.tile(Y, [1, self.num_gauss_hermite_points])  # broadcast Y to match X\n",
    "\n",
    "        logp = self.logp(X, Y, *args)\n",
    "        return tf.reshape(tf.log(tf.matmul(tf.exp(logp), gh_w)), shape)\n",
    "\n",
    "    @params_as_tensors\n",
    "    def variational_expectations(self, Fmu, Fvar, Y, *args):\n",
    "        \"\"\"\n",
    "        Compute the expected log density of the data, given a Gaussian\n",
    "        distribution for the function values.\n",
    "        if\n",
    "            q(f) = N(Fmu, Fvar)\n",
    "        and this object represents\n",
    "            p(y|f)\n",
    "        then this method computes\n",
    "           \\int (\\log p(y|f)) q(f) df.\n",
    "        Here, we implement a default Gauss-Hermite quadrature routine, but some\n",
    "        likelihoods (Gaussian, Poisson) will implement specific cases.\n",
    "        \"\"\"\n",
    "        gh_x, gh_w = hermgauss(self.num_gauss_hermite_points)\n",
    "        gh_x = gh_x.reshape(1, -1)\n",
    "        gh_w = gh_w.reshape(-1, 1) / np.sqrt(np.pi)\n",
    "        shape = tf.shape(Fmu)\n",
    "        Fmu, Fvar, Y = [tf.reshape(e, (-1, 1)) for e in (Fmu, Fvar, Y)]\n",
    "        args = [tf.reshape(e, (-1, 1)) if isinstance(e,tf.Tensor) else e for e in args]\n",
    "        X = gh_x * tf.sqrt(2.0 * Fvar) + Fmu\n",
    "        Y = tf.tile(Y, [1, self.num_gauss_hermite_points])  # broadcast Y to match X\n",
    "        logp = self.logp(X, Y, *args)\n",
    "        return tf.reshape(tf.matmul(logp, gh_w), shape)\n",
    "    \n",
    "    \n",
    "class HeteroscedasticSwitchedLikelihood(HeteroscedasticLikelihood):\n",
    "    def __init__(self, likelihood_list):\n",
    "        \"\"\"\n",
    "        In this likelihood, we assume at extra column of Y, which contains\n",
    "        integers that specify a likelihood from the list of likelihoods.\n",
    "        \"\"\"\n",
    "        Likelihood.__init__(self)\n",
    "        for l in likelihood_list:\n",
    "            assert isinstance(l, Likelihood)\n",
    "        self.likelihood_list = ParamList(likelihood_list)\n",
    "        self.num_likelihoods = len(self.likelihood_list)\n",
    "\n",
    "    def _partition_and_stitch(self, args, func_name):\n",
    "        \"\"\"\n",
    "        args is a list of tensors, to be passed to self.likelihoods.<func_name>\n",
    "        args[-2] is the 'Y' argument, which contains the indexes to self.likelihoods.\n",
    "        This function splits up the args using dynamic_partition, calls the\n",
    "        relevant function on the likelihoods, and re-combines the result.\n",
    "        \"\"\"\n",
    "        # get the index from Y\n",
    "        Y = args[-2]\n",
    "        ind = Y[:, -1]\n",
    "        ind = tf.cast(ind, tf.int32)\n",
    "        Y = Y[:, :-1]\n",
    "        args[-2] = Y\n",
    "\n",
    "        # split up the arguments into chunks corresponding to the relevant likelihoods\n",
    "        args = zip(*[tf.dynamic_partition(X, ind, self.num_likelihoods) for X in args])\n",
    "\n",
    "        # apply the likelihood-function to each section of the data\n",
    "        with params_as_tensors_for(self, convert=False):\n",
    "            funcs = [getattr(lik, func_name) for lik in self.likelihood_list]\n",
    "        results = [f(*args_i) for f, args_i in zip(funcs, args)]\n",
    "\n",
    "        # stitch the results back together\n",
    "        partitions = tf.dynamic_partition(tf.range(0, tf.size(ind)), ind, self.num_likelihoods)\n",
    "        results = tf.dynamic_stitch(partitions, results)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def logp(self, F, Y, hetero_variance):\n",
    "        return self._partition_and_stitch([F, Y, hetero_variance], 'logp')\n",
    "\n",
    "    def predict_density(self, Fmu, Fvar, Y, hetero_variance):\n",
    "        return self._partition_and_stitch([Fmu, Fvar, Y, hetero_variance], 'predict_density')\n",
    "\n",
    "    def variational_expectations(self, Fmu, Fvar, Y, hetero_variance):\n",
    "        return self._partition_and_stitch([Fmu, Fvar, Y, hetero_variance], 'variational_expectations')\n",
    "\n",
    "    def predict_mean_and_var(self, Fmu, Fvar, hetero_variance):\n",
    "        mvs = [lik.predict_mean_and_var(Fmu, Fvar, hetero_variance) for lik in self.likelihood_list]\n",
    "        mu_list, var_list = zip(*mvs)\n",
    "        mu = tf.concat(mu_list, 1)\n",
    "        var = tf.concat(var_list, 1)\n",
    "        return mu, var\n",
    "\n",
    "class HeteroscedasticWrappedPhaseGaussian(HeteroscedasticLikelihood):\n",
    "    def __init__(self, freq=140e6, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.freq = tf.convert_to_tensor(freq,dtype=settings.float_type,name='test_freq') # frequency the phase is calculated at for the predictive distribution\n",
    "        self.tec_conversion = tf.convert_to_tensor(-8.4480e9,dtype=settings.float_type,name='tec_conversion') # rad Hz/ tecu\n",
    "        self.tec2phase = tf.convert_to_tensor(self.tec_conversion / self.freq,dtype=settings.float_type,name='tec2phase')\n",
    "        \n",
    "    def wrap(self, f):\n",
    "        \"\"\"Wraps f into (-pi, pi)\"\"\"\n",
    "        return tf.cast(tf.atan2(tf.sin(f),tf.cos(f)),settings.float_type)\n",
    "    \n",
    "    @params_as_tensors\n",
    "    def logp(self, F, Y, hetero_variance, freqs, *unused_args):\n",
    "        \"\"\"The log-likelihood function.\"\"\"\n",
    "        tec2phase = self.tec_conversion/freqs\n",
    "        phase = self.wrap(F*tec2phase)\n",
    "        dphase = self.wrap(phase - self.wrap(Y)) # Ito theorem\n",
    "        return densities.gaussian(dphase, tf.fill(tf.shape(F),tf.cast(0.,settings.float_type)), hetero_variance)\n",
    "\n",
    "    @params_as_tensors\n",
    "    def conditional_mean(self, F, unused_hetero_variance, freq, *unused_args):  # pylint: disable=R0201\n",
    "        \"\"\"The mean of the likelihood conditioned on latent.\"\"\"\n",
    "        freq = self.freq if freq is None else tf.convert_to_tensor(freq,dtype=settings.float_type)\n",
    "        tec2phase = self.tec_conversion/freq\n",
    "        phase = F*tec2phase\n",
    "        return phase\n",
    "\n",
    "    @params_as_tensors\n",
    "    def conditional_variance(self, F, hetero_variance, *unused_args):\n",
    "        return hetero_variance\n",
    "\n",
    "def make_hetero_likelihood(likelihood,freqs):\n",
    "    return HeteroscedasticSwitchedLikelihood([likelihood(f,name='freq_{}_likelihood'.format(i)) for i,f in enumerate(freqs)])\n",
    "\n",
    "def make_likelihood(likelihood,freqs):\n",
    "    return SwitchedLikelihood([likelihood(f,name='freq_{}_likelihood'.format(i)) for i,f in enumerate(freqs)])\n",
    "\n",
    "class HeteroscedasticLatent(Parameterized):\n",
    "    def __init__(self, Z, mean_function, kern, feat=None, q_diag=False, whiten=True, name=None):\n",
    "        super(HeteroscedasticLatent, self).__init__(name=name)\n",
    "        self.mean_function = mean_function\n",
    "        self.kern = kern\n",
    "        self.num_latent = 1\n",
    "        \n",
    "        self.feature = features.inducingpoint_wrapper(feat, Z)\n",
    "        num_inducing = len(self.feature)\n",
    "        self.whiten = whiten\n",
    "        \n",
    "        self.q_mu = Parameter(np.zeros((num_inducing, self.num_latent), dtype=settings.float_type))\n",
    "        self.q_diag = q_diag\n",
    "        if self.q_diag:\n",
    "            self.q_sqrt = Parameter(np.ones((num_inducing, self.num_latent), dtype=settings.float_type),\n",
    "                                    transforms.positive)\n",
    "        else:\n",
    "            q_sqrt = np.array([np.eye(num_inducing, dtype=settings.float_type)\n",
    "                               for _ in range(self.num_latent)])\n",
    "            self.q_sqrt = Parameter(q_sqrt, transform=transforms.LowerTriangular(num_inducing, self.num_latent))\n",
    "            \n",
    "    #@autoflow((settings.float_type, [None, None]), (tf.int32, []))\n",
    "    @params_as_tensors\n",
    "    def predict_f_samples(self, Xnew, num_samples=1):\n",
    "        \"\"\"\n",
    "        Produce samples from the posterior latent function(s) at the points\n",
    "        Xnew.\n",
    "        \"\"\"\n",
    "        if isinstance(Xnew,(tuple,list)):\n",
    "            assert len(Xnew) == 2\n",
    "            mu0, var0 = self._build_predict(Xnew[0], full_cov=True)\n",
    "            mu1, var1 = self._build_predict(Xnew[1], full_cov=True)\n",
    "            mu = tf.concat([mu0,mu1],axis=0)\n",
    "            N = tf.shape(var0)[0]\n",
    "            M = tf.shape(var1)[0]\n",
    "            K = tf.shape(var0)[-1]\n",
    "            zeros = tf.zeros([N,M,K],dtype=settings.float_type)\n",
    "            var = tf.concat([\n",
    "                tf.concat([var0, zeros],axis=1),\n",
    "                tf.concat([tf.transpose(zeros,(1,0,2)), var1],axis=1)\n",
    "            ], axis=0)\n",
    "        else:\n",
    "            mu, var = self._build_predict(Xnew, full_cov=True)\n",
    "        jitter = tf.eye(tf.shape(mu)[0], dtype=settings.float_type) * settings.numerics.jitter_level\n",
    "        samples = []\n",
    "        for i in range(self.num_latent):\n",
    "            L = tf.cholesky(var[:, :, i] + jitter)\n",
    "            shape = tf.stack([tf.shape(L)[0], num_samples])\n",
    "            V = tf.random_normal(shape, dtype=settings.float_type)\n",
    "            samples.append(mu[:, i:i + 1] + tf.matmul(L, V))\n",
    "        return tf.transpose(tf.stack(samples))# S,N,D\n",
    "    \n",
    "    @autoflow((settings.float_type, [None, None]))\n",
    "    def predict_f(self, Xnew):\n",
    "        \"\"\"\n",
    "        Compute the mean and variance of the latent function(s) at the points\n",
    "        Xnew.\n",
    "        \"\"\"\n",
    "        return self._build_predict(Xnew)\n",
    "\n",
    "    @autoflow((settings.float_type, [None, None]))\n",
    "    def predict_f_full_cov(self, Xnew):\n",
    "        \"\"\"\n",
    "        Compute the mean and covariance matrix of the latent function(s) at the\n",
    "        points Xnew.\n",
    "        \"\"\"\n",
    "        return self._build_predict(Xnew, full_cov=True)\n",
    "\n",
    "    \n",
    "    @params_as_tensors\n",
    "    def _build_predict(self, Xnew, full_cov=False):\n",
    "        mu, var = features.conditional(self.feature, self.kern, Xnew, self.q_mu,\n",
    "                                       q_sqrt=self.q_sqrt, full_cov=full_cov, white=self.whiten)\n",
    "        return mu + self.mean_function(Xnew), var\n",
    "        \n",
    "class PositiveHeteroscedasticLatent(HeteroscedasticLatent):\n",
    "    def __init__(self, Z, mean_function, kern, feat=None, q_diag=False, whiten=True, name=None):\n",
    "        super(PositiveHeteroscedasticLatent, self).__init__(Z, mean_function, kern, feat=feat, q_diag=q_diag, whiten=whiten, name=name)\n",
    "        \n",
    "    #@autoflow((settings.float_type, [None, None]), (tf.int32, []))\n",
    "    @params_as_tensors\n",
    "    def predict_f_samples(self, Xnew, num_samples=1):\n",
    "        \"\"\"\n",
    "        Produce samples from the posterior latent function(s) at the points\n",
    "        Xnew.\n",
    "        \"\"\"\n",
    "        return 1e-6 + tf.nn.softplus(\n",
    "            super(PositiveHeteroscedasticLatent,self).predict_f_samples(Xnew, num_samples))\n",
    "    \n",
    "    @autoflow((settings.float_type, [None, None]))\n",
    "    def predict_f(self, Xnew):\n",
    "        \"\"\"\n",
    "        Compute the mean and variance of the latent function(s) at the points\n",
    "        Xnew.\n",
    "        \"\"\"\n",
    "        return self._build_predict(Xnew)\n",
    "\n",
    "    @autoflow((settings.float_type, [None, None]))\n",
    "    def predict_f_full_cov(self, Xnew):\n",
    "        \"\"\"\n",
    "        Compute the mean and covariance matrix of the latent function(s) at the\n",
    "        points Xnew.\n",
    "        \"\"\"\n",
    "        return self._build_predict(Xnew, full_cov=True)\n",
    "\n",
    "    \n",
    "    @params_as_tensors\n",
    "    def _build_predict(self, Xnew, full_cov=False):\n",
    "        mu, var = features.conditional(self.feature, self.kern, Xnew, self.q_mu,\n",
    "                                       q_sqrt=self.q_sqrt, full_cov=full_cov, white=self.whiten)\n",
    "        return mu + self.mean_function(Xnew), var\n",
    "        \n",
    "\n",
    "class HeteroscedasticSVGP(SVGP):\n",
    "    def __init__(self,noise_latent, X, Y, kern, likelihood, feat=None,\n",
    "                 mean_function=None,\n",
    "                 num_latent=None,\n",
    "                 q_diag=False,\n",
    "                 whiten=True,\n",
    "                 minibatch_size=None,\n",
    "                 Z=None,\n",
    "                 num_data=None,\n",
    "                 **kwargs):\n",
    "        super(HeteroscedasticSVGP,self).__init__(X, Y, kern, likelihood, feat=feat,\n",
    "                 mean_function=mean_function,\n",
    "                 num_latent=num_latent,\n",
    "                 q_diag=q_diag,\n",
    "                 whiten=whiten,\n",
    "                 minibatch_size=minibatch_size,\n",
    "                 Z=Z,\n",
    "                 num_data=num_data,\n",
    "                 **kwargs)\n",
    "        \n",
    "        self.noise_latent = noise_latent\n",
    "        assert isinstance(self.likelihood, HeteroscedasticLikelihood)\n",
    "        assert isinstance(noise_latent, HeteroscedasticLatent)\n",
    "        \n",
    "    @params_as_tensors\n",
    "    def _build_likelihood(self):\n",
    "        \"\"\"\n",
    "        This gives a variational bound on the model likelihood.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get prior KL.\n",
    "        KL = self.build_prior_KL()\n",
    "\n",
    "        # Get conditionals\n",
    "        fmean, fvar = self._build_predict(self.X, full_cov=False)\n",
    "\n",
    "        # Get variational expectations.\n",
    "        error = self.noise_latent.predict_f_samples(self.X)\n",
    "        hetero_variance = tf.square(error)[0,:,:] # SN1\n",
    "        \n",
    "        lik_freqs = self.Y[:,-1:]\n",
    "        \n",
    "\n",
    "        var_exp = self.likelihood.variational_expectations(fmean, fvar, self.Y[:,:-1], hetero_variance, lik_freqs)\n",
    "\n",
    "        # re-scale for minibatch size\n",
    "        scale = tf.cast(self.num_data, settings.float_type) / tf.cast(tf.shape(self.X)[0], settings.float_type)\n",
    "\n",
    "        return tf.reduce_sum(var_exp) * scale - KL\n",
    "    \n",
    "    @autoflow((settings.float_type, [None, None]), (settings.float_type, [None, None]))\n",
    "    def predict_density(self, Xnew, Ynew):\n",
    "        \"\"\"\n",
    "        Compute the (log) density of the data Ynew at the points Xnew\n",
    "        Note that this computes the log density of the data individually,\n",
    "        ignoring correlations between them. The result is a matrix the same\n",
    "        shape as Ynew containing the log densities.\n",
    "        \"\"\"\n",
    "        pred_f_mean, pred_f_var = self._build_predict(Xnew)\n",
    "        error = self.noise_latent.predict_f_samples(Xnew)\n",
    "        hetero_variance = tf.square(error)[0,:,:] # N1\n",
    "        lik_freqs = Ynew[:,-1:]\n",
    "        \n",
    "        return self.likelihood.predict_density(pred_f_mean, pred_f_var, Ynew[:,:-1], hetero_variance, lik_freqs)\n",
    "    \n",
    "    @autoflow((settings.float_type, [None, None]))\n",
    "    def predict_y(self, Xnew, freq=None):\n",
    "        \"\"\"\n",
    "        Compute the mean and variance of held-out data at the points Xnew\n",
    "        \"\"\"\n",
    "        pred_f_mean, pred_f_var = self._build_predict(Xnew)\n",
    "        error = self.noise_latent.predict_f_samples(Xnew)\n",
    "        hetero_variance = tf.square(error)[0,:,:]\n",
    "        return self.likelihood.predict_mean_and_var(pred_f_mean, pred_f_var, hetero_variance, freq)\n",
    "    \n",
    "    def _build_objective(self, likelihood_tensor, prior_tensor):\n",
    "        return super(HeteroscedasticSVGP,self)._build_objective(likelihood_tensor, prior_tensor)\n",
    "    \n",
    "class WeightedHeteroSVGP(HeteroscedasticSVGP):\n",
    "    def __init__(self,weights, noise_latent, X, Y, kern, likelihood, feat=None,\n",
    "                 mean_function=None,\n",
    "                 num_latent=None,\n",
    "                 q_diag=False,\n",
    "                 whiten=True,\n",
    "                 minibatch_size=None,\n",
    "                 Z=None,\n",
    "                 num_data=None,\n",
    "                 **kwargs):\n",
    "        super(WeightedHeteroSVGP,self).__init__( noise_latent, X, Y, kern, likelihood, feat=feat,\n",
    "                 mean_function=mean_function,\n",
    "                 num_latent=num_latent,\n",
    "                 q_diag=q_diag,\n",
    "                 whiten=whiten,\n",
    "                 minibatch_size=minibatch_size,\n",
    "                 Z=Z,\n",
    "                 num_data=num_data,\n",
    "                 **kwargs)\n",
    "        if minibatch_size:\n",
    "            self.weights = Minibatch(weights, minibatch_size, seed=0)\n",
    "        else:\n",
    "            self.weights = DataHolder(weights)\n",
    "            \n",
    "    @params_as_tensors\n",
    "    def _build_likelihood(self):\n",
    "        \"\"\"\n",
    "        This gives a variational bound on the model likelihood.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get prior KL.\n",
    "        KL = self.build_prior_KL()\n",
    "\n",
    "        # Get conditionals\n",
    "        fmean, fvar = self._build_predict(self.X, full_cov=False)\n",
    "\n",
    "        # Get variational expectations.\n",
    "        error = self.noise_latent.predict_f_samples(self.X)\n",
    "        hetero_variance = tf.square(error)[0,:,:]\n",
    "\n",
    "        var_exp = self.likelihood.variational_expectations(fmean, fvar, self.Y, hetero_variance)\n",
    "        \n",
    "        var_exp = var_exp * (self.weights/tf.reduce_mean(self.weights))\n",
    "\n",
    "        # re-scale for minibatch size\n",
    "        scale = tf.cast(self.num_data, settings.float_type) / tf.cast(tf.shape(self.X)[0], settings.float_type)\n",
    "\n",
    "        return tf.reduce_sum(var_exp) * scale - KL\n",
    "    \n",
    "class WeightedSVGP(SVGP):\n",
    "    def __init__(self,weights, X, Y, kern, likelihood, feat=None,\n",
    "                 mean_function=None,\n",
    "                 num_latent=None,\n",
    "                 q_diag=False,\n",
    "                 whiten=True,\n",
    "                 minibatch_size=None,\n",
    "                 Z=None,\n",
    "                 num_data=None,\n",
    "                 **kwargs):\n",
    "        super(WeightedSVGP,self).__init__( X, Y, kern, likelihood, feat=feat,\n",
    "                 mean_function=mean_function,\n",
    "                 num_latent=num_latent,\n",
    "                 q_diag=q_diag,\n",
    "                 whiten=whiten,\n",
    "                 minibatch_size=minibatch_size,\n",
    "                 Z=Z,\n",
    "                 num_data=num_data,\n",
    "                 **kwargs)\n",
    "        if minibatch_size:\n",
    "            self.weights = Minibatch(weights, minibatch_size, seed=0)\n",
    "        else:\n",
    "            self.weights = DataHolder(weights)\n",
    "            \n",
    "    @params_as_tensors\n",
    "    def _build_likelihood(self):\n",
    "        \"\"\"\n",
    "        This gives a variational bound on the model likelihood.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get prior KL.\n",
    "        KL = self.build_prior_KL()\n",
    "\n",
    "        # Get conditionals\n",
    "        fmean, fvar = self._build_predict(self.X, full_cov=False)\n",
    "\n",
    "        # Get variational expectations.\n",
    "        var_exp = self.likelihood.variational_expectations(fmean, fvar, self.Y)\n",
    "        \n",
    "        var_exp = var_exp * (self.weights/tf.reduce_mean(self.weights))\n",
    "\n",
    "        # re-scale for minibatch size\n",
    "        scale = tf.cast(self.num_data, settings.float_type) / tf.cast(tf.shape(self.X)[0], settings.float_type)\n",
    "\n",
    "        return tf.reduce_sum(var_exp) * scale - KL\n",
    "\n",
    "\n",
    "class HeteroDGP(DGP):\n",
    "    def __init__(self,log_noise_latent, *args, **kwargs):\n",
    "        super(HeteroDGP,self).__init__(*args, **kwargs)\n",
    "        self.log_noise_latent = log_noise_latent\n",
    "        assert isinstance(log_noise_latent, HeteroscedasticLatent)\n",
    "    \n",
    "    def E_log_p_Y(self, X, Y):\n",
    "        \"\"\"\n",
    "        Calculate the expectation of the data log likelihood under the variational distribution\n",
    "         with MC samples\n",
    "        \"\"\"\n",
    "        Fmean, Fvar = self._build_predict(X, full_cov=False, S=self.num_samples)\n",
    "        log_error = self.log_noise_latent.predict_f_samples(X)\n",
    "        hetero_variance = tf.square(tf.exp(log_error))[0,:,:]\n",
    "        var_exp = self.likelihood.variational_expectations(Fmean, Fvar, Y, hetero_variance)  # S, N, D\n",
    "        return tf.reduce_mean(var_exp, 0)  # N, D\n",
    "\n",
    "    @autoflow((settings.float_type, [None, None]), (tf.int32, []))\n",
    "    def predict_y(self, Xnew, num_samples):\n",
    "        Fmean, Fvar = self._build_predict(Xnew, full_cov=False, S=num_samples)\n",
    "        log_error = self.log_noise_latent.predict_f_samples(Xnew)\n",
    "        hetero_variance = tf.square(tf.exp(log_error))[0,:,:]\n",
    "        return self.likelihood.predict_mean_and_var(Fmean, Fvar, hetero_variance)\n",
    "\n",
    "    @autoflow((settings.float_type, [None, None]), (settings.float_type, [None, None]), (tf.int32, []))\n",
    "    def predict_density(self, Xnew, Ynew, num_samples):\n",
    "        Fmean, Fvar = self._build_predict(Xnew, full_cov=False, S=num_samples)\n",
    "        log_error = self.log_noise_latent.predict_f_samples(Xnew)\n",
    "        hetero_variance = tf.square(tf.exp(log_error))[0,:,:]\n",
    "        l = self.likelihood.predict_density(Fmean, Fvar, Ynew,hetero_variance)\n",
    "        log_num_samples = tf.log(tf.cast(num_samples, float_type))\n",
    "        return tf.reduce_logsumexp(l - log_num_samples, axis=0)\n",
    "    \n",
    "class WeightedDGP(DGP):\n",
    "    def __init__(self,weights, *args, **kwargs):\n",
    "        super(WeightedDGP,self).__init__(*args, **kwargs)\n",
    "        minibatch_size = kwargs.get('minibatch_size',None)\n",
    "        if minibatch_size:\n",
    "            self.weights = Minibatch(weights, minibatch_size, seed=0)\n",
    "        else:\n",
    "            self.weights = DataHolder(weights)\n",
    "            \n",
    "    def E_log_p_Y(self, X, Y):\n",
    "        \"\"\"\n",
    "        Calculate the expectation of the data log likelihood under the variational distribution\n",
    "         with MC samples\n",
    "        \"\"\"\n",
    "        return self.weights*super(WeightedDGP,self).E_log_p_Y(X,Y)\n",
    "    \n",
    "class PrintAction(Action):\n",
    "    def __init__(self, model, text):\n",
    "        self.model = model\n",
    "        self.text = text\n",
    "        \n",
    "    def run(self, ctx):\n",
    "        likelihood = ctx.session.run(self.model.likelihood_tensor)\n",
    "        logging.warning('{}: iteration {} likelihood {:.4f}'.format(self.text, ctx.iteration, likelihood))\n",
    "        \n",
    "def run_nat_grads_with_adam(model, lr, gamma, iterations, var_list=None, callback=None):\n",
    "    # we'll make use of this later when we use a XiTransform\n",
    "    if var_list is None:\n",
    "        var_list = [(model.q_mu, model.q_sqrt,XiSqrtMeanVar())]\n",
    "#     model.q_mu.set_trainable(False)\n",
    "#     model.q_sqrt.set_trainable(False)\n",
    "#    for layer in model.layers[:-1]:\n",
    "#         layer.q_sqrt.set_trainable(False)\n",
    "#         layer.q_mu.set_trainable(False)\n",
    "\n",
    "    adam = AdamOptimizer(lr).make_optimize_action(model)\n",
    "    natgrad = NatGradOptimizer(gamma).make_optimize_action(model, var_list=var_list)\n",
    "    \n",
    "    actions = [adam]#natgrad,\n",
    "    actions = actions if callback is None else actions + [callback]\n",
    "\n",
    "    Loop(actions, stop=iterations)()\n",
    "    model.anchor(model.enquire_session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data \n",
    "\n",
    "We will take a portion of the data and perform jointly directional and temporal solving for a variety of parameters and plot the results against the original data, as well as study the change in log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = DataPack(filename='./rvw_datapack_full_phase_dec27_unwrap.hdf5')\n",
    "\n",
    "ant_idx = -1\n",
    "time_idx = -1\n",
    "dir_idx = -1\n",
    "freq_idx = -1\n",
    "\n",
    "DTEC_prior_var = 0.05**2#TECU^2\n",
    "DTEC_prior_time_ls = 50.#seconds\n",
    "DTEC_prior_dir_ls = 1.#degree\n",
    "iterations=1000\n",
    "\n",
    "\n",
    "phase = dp.get_phase(ant_idx,time_idx,dir_idx,freq_idx)\n",
    "variance = dp.get_variance(ant_idx,time_idx,dir_idx,freq_idx)\n",
    "# prop = np.zeros(phase.shape[:-1])\n",
    "# dp.set_prop(prop,ant_idx,time_idx,dir_idx)\n",
    "\n",
    "times,_ = dp.get_times(time_idx)\n",
    "antennas,antenna_labels = dp.get_antennas(ant_idx)\n",
    "freqs = dp.get_freqs(freq_idx)\n",
    "directions,patch_names = dp.get_directions(dir_idx)\n",
    "Na,Nt,Nd,Nf = phase.shape\n",
    "\n",
    "X_d = np.array([directions.ra.deg,directions.dec.deg]).T\n",
    "X_t = times.mjd[:,None]*86400.\n",
    "enu = ENU(obstime=times[0],location=dp.radio_array.get_center())\n",
    "ant_enu = antennas.transform_to(enu)\n",
    "X_a = np.array([ant_enu.east.to(au.km).value, ant_enu.north.to(au.km).value]).T\n",
    "\n",
    "d_std = X_d.std(0).mean() + 1e-6\n",
    "t_std = X_t.std() + 1e-6\n",
    "a_std = X_a.std(0).mean() + 1e-6\n",
    "\n",
    "X_a = (X_a - X_a.mean(0)) / a_std\n",
    "X_t = (X_t - X_t.mean()) / t_std\n",
    "X_d = (X_d - X_d.mean(0)) / d_std\n",
    "\n",
    "DTEC_prior_time_ls /= t_std\n",
    "DTEC_prior_dir_ls /= d_std\n",
    "\n",
    "def make_coord_array(*X):\n",
    "    \"\"\"\n",
    "    Return the design matrix from coordinates.\n",
    "    \"\"\"\n",
    "    def add_dims(x,where,sizes):\n",
    "        shape = []\n",
    "        tiles = []\n",
    "        for i in range(len(sizes)):\n",
    "            if i not in where:\n",
    "                shape.append(1)\n",
    "                tiles.append(sizes[i])\n",
    "            else:\n",
    "                shape.append(-1)\n",
    "                tiles.append(1)\n",
    "        return np.tile(np.reshape(x,shape),tiles)\n",
    "    N = [x.shape[0] for x in X]\n",
    "    X_ = []\n",
    "\n",
    "    for i,x in enumerate(X):\n",
    "        for dim in range(x.shape[1]):\n",
    "            X_.append(add_dims(x[:,dim],[i], N))\n",
    "    X = np.stack(X_,axis=-1)\n",
    "    return np.reshape(X,(-1,X.shape[-1]))\n",
    "\n",
    "def make_data_vec_old(Y):\n",
    "    \"\"\"\n",
    "    Takes Y of shape [..., Nf, N]\n",
    "    returns [...,N+1] where last is freq idx of observation\"\"\"\n",
    "    shape = Y.shape\n",
    "    idx = np.arange(shape[-2])\n",
    "    idx = np.reshape(idx,\n",
    "                     np.concatenate([np.ones(len(shape)-2,dtype=np.int32), [shape[-2], 1]],axis=0))#[...,Nf, 1]\n",
    "    tiles = list(shape)\n",
    "    tiles[-2] = 1\n",
    "    idx = np.tile(idx,tiles)\n",
    "    return np.concatenate([Y.reshape((-1,shape[-1])), idx.reshape((-1,1))],axis=-1)\n",
    "\n",
    "def make_data_vec(Y,freqs):\n",
    "    \"\"\"\n",
    "    Takes Y of shape [..., Nf, N]\n",
    "    returns [...,N+1] where last is freq of observation\"\"\"\n",
    "    shape = Y.shape\n",
    "    freqs = np.reshape(freqs,\n",
    "                     np.concatenate([np.ones(len(shape)-2,dtype=np.int32), [shape[-2], 1]],axis=0))#[...,Nf, 1]\n",
    "    tiles = list(shape)\n",
    "    tiles[-2] = 1\n",
    "    freqs = np.tile(freqs,tiles)\n",
    "    return np.concatenate([Y.reshape((-1,shape[-1])), freqs.reshape((-1,1))],axis=-1)\n",
    "\n",
    "\n",
    "def batch_predict_y_dgp(model,X,batch_size=1000,S=100):\n",
    "    ystar,varstar = [],[]\n",
    "    for i in range(X.shape[0])[::batch_size]:\n",
    "        start = i\n",
    "        stop = min(X.shape[0],start+batch_size)\n",
    "        y_,v_ = model.predict_y(X[start:stop,:],S)\n",
    "        ystar.append(y_.mean(0))\n",
    "        varstar.append(v_.mean(0))\n",
    "    return np.concatenate(ystar,axis=0), np.concatenate(varstar,axis=0)\n",
    "\n",
    "def batch_predict_density_dgp(model,X,Y,batch_size=1000,S=100):\n",
    "    l = []\n",
    "    for i in range(X.shape[0])[::batch_size]:\n",
    "        start = i\n",
    "        stop = min(X.shape[0],start+batch_size)\n",
    "        l_ = model.predict_density(X[start:stop,:],Y[start:stop,:],S)\n",
    "        l.append(l_.mean())\n",
    "    return np.mean(l)\n",
    "\n",
    "def batch_predict_y_svgp(model,X,freq=None,batch_size=1000):\n",
    "    ystar,varstar = [],[]\n",
    "    for i in range(X.shape[0])[::batch_size]:\n",
    "        start = i\n",
    "        stop = min(X.shape[0],start+batch_size)\n",
    "        y_,v_ = model.predict_y(X[start:stop,:],freq)\n",
    "        ystar.append(y_)\n",
    "        varstar.append(v_)\n",
    "    return np.concatenate(ystar,axis=0), np.concatenate(varstar,axis=0)\n",
    "\n",
    "def batch_predict_density_svgp(model,X,Y,batch_size=1000):\n",
    "    l = []\n",
    "    for i in range(X.shape[0])[::batch_size]:\n",
    "        start = i\n",
    "        stop = min(X.shape[0],start+batch_size)\n",
    "        l_ = model.predict_density(X[start:stop,:],Y[start:stop,:])\n",
    "        l.append(l_.mean())\n",
    "    return np.mean(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 time-only solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_single(X,Y,Ystar,error, ax = None):\n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots(1,1)\n",
    "    ax.plot(X[:,0],Y[:,0],label='data')\n",
    "    ax.plot(X[:,0],Ystar[:,0],label='inferred')\n",
    "    ax.fill_between(X[:,0],Ystar[:,0]+error[:,0],Ystar[:,0]-error[:,0],alpha=0.25)\n",
    "    ax.legend()\n",
    "    \n",
    "def make_error_latent(Z=None,M=None):\n",
    "    Z = np.linspace(-2,2,M)[:,None] if M is not None else Z\n",
    "\n",
    "    mean = gp.mean_functions.Constant(0.3)\n",
    "    #mean.c.prior = gp.priors.Gaussian(np.log(0.2),np.log(0.5)-np.log(0.1))\n",
    "    kern = gp.kernels.RBF(1,lengthscales=[1.],variance=0.1)\n",
    "    kern.variance.set_trainable(False)\n",
    "    \n",
    "    error_latent = PositiveHeteroscedasticLatent(Z, mean_function=mean, \n",
    "                                kern= kern, feat=None, q_diag=False, whiten=True, name='error_latent')        \n",
    "    return error_latent\n",
    "    \n",
    "def make_weighted_svgp_model(weights, X,Y,freqs,M=None,minibatch=None):\n",
    "    N, num_latent = Y.shape\n",
    "    M = M or N\n",
    "    Z = kmeans2(X, M, minit='points')[0] if N < 10000 else X[np.random.choice(N,size=M,replace=False),:]\n",
    "    Z_kern = kmeans2(X, 10, minit='points')[0] if N < 10000 else X[np.random.choice(N,size=10,replace=False),:]\n",
    "    with gp.defer_build():\n",
    "        periodic_kern = gp.kernels.Periodic(1,active_dims=[0],variance=1.)\n",
    "        periodic_kern.variance.set_trainable(False)\n",
    "        matern_kern = gp.kernels.Matern32(1,active_dims=[0],lengthscales=[DTEC_prior_time_ls],\n",
    "                                          variance=DTEC_prior_var)\n",
    "        matern_kern.variance.prior = gp.priors.Gaussian(0.,DTEC_prior_var)\n",
    "        matern_kern.lengthscales.prior = gp.priors.Gaussian(0.,DTEC_prior_time_ls)\n",
    "        kern = matern_kern#*periodic_kern\n",
    "        \n",
    "        kern = NonstationaryKernel(1, Z=Z_kern, hetero_sigma=True, hetero_ls=True,  active_dims=[0], name=\"nonstationary_kernel\")\n",
    "        \n",
    "        likelihood = make_likelihood(WrappedPhaseGaussian,freqs)\n",
    "        mean = gp.mean_functions.Zero()\n",
    "        \n",
    "        model = WeightedSVGP(weights,X, Y, kern, likelihood, Z=Z, \n",
    "                    mean_function = mean, minibatch_size=minibatch,\n",
    "                    num_latent=num_latent-1)\n",
    "        \n",
    "        var_list = [(model.q_mu, model.q_sqrt,XiSqrtMeanVar())]\n",
    "\n",
    "        for lik in model.likelihood.likelihood_list:\n",
    "            lik.variance = 0.2\n",
    "            lik.variance.prior = gp.priors.Gaussian(0.,0.2)\n",
    "        model.compile()\n",
    "    return model, var_list\n",
    "\n",
    "def make_hetero_svgp_model(X,Y,freqs,M=None,minibatch=None):\n",
    "    N, num_latent = Y.shape\n",
    "    M = M or N\n",
    "    Z = kmeans2(X, M, minit='points')[0] if N < 10000 else X[np.random.choice(N,size=M,replace=False),:]\n",
    "    Z_kern = kmeans2(X, 10, minit='points')[0] if N < 10000 else X[np.random.choice(N,size=10,replace=False),:]\n",
    "    with gp.defer_build():\n",
    "#         periodic_kern = gp.kernels.Periodic(1,active_dims=[0],variance=1.)\n",
    "#         periodic_kern.variance.set_trainable(False)\n",
    "        matern_kern = gp.kernels.Matern32(1,active_dims=[0],variance=DTEC_prior_var,lengthscales=[DTEC_prior_time_ls])\n",
    "        kern = matern_kern#periodic_kern\n",
    "        \n",
    "#         Z = kmeans2(X, 10, minit='points')[0] if N < 10000 else X[np.random.choice(N,size=10,replace=False),:]\n",
    "#         kern = NonstationaryKernel(1, Z=Z_kern, hetero_sigma=False, hetero_ls=True,  active_dims=[0], name=\"nonstationary_kernel\")\n",
    "        likelihood = HeteroscedasticWrappedPhaseGaussian(140e6,name='hetero_likelihood')\n",
    "    #make_hetero_likelihood(HeteroscedasticWrappedPhaseGaussian, freqs)\n",
    "        mean = gp.mean_functions.Zero()\n",
    "        error_latent = make_error_latent(M=10)\n",
    "        model = HeteroscedasticSVGP(error_latent,X, Y, kern, likelihood, Z=Z, \n",
    "                    mean_function = mean, minibatch_size=minibatch,\n",
    "                    num_latent=num_latent-1)\n",
    "        \n",
    "        var_list = [(model.q_mu, model.q_sqrt,XiSqrtMeanVar())]\n",
    "\n",
    "#         model.likelihood.variance = 0.2\n",
    "#         model.likelihood.variance.set_trainable(False)\n",
    "        model.compile()\n",
    "    return model, var_list\n",
    "\n",
    "def make_weighted_dgp_model(weights, X,Y,freqs,M=None,minibatch=None,n_kern=1):\n",
    "    N, num_latent = Y.shape\n",
    "    M = M or N\n",
    "    Z = kmeans2(X, M, minit='points')[0] if N < 10000 else X[np.random.choice(N,size=M,replace=False),:]\n",
    "    with gp.defer_build():\n",
    "        \n",
    "        kernels = []\n",
    "        for i in range(n_kern):\n",
    "            periodic_kern = gp.kernels.Periodic(1,active_dims=[0],variance=1.)\n",
    "            periodic_kern.variance.set_trainable(False)\n",
    "            matern_kern = gp.kernels.Matern32(1,active_dims=[0],variance=DTEC_prior_var)\n",
    "            kern = matern_kern*periodic_kern\n",
    "            kernels.append(kern)\n",
    "        \n",
    "        likelihood = make_likelihood(freqs)\n",
    "        mean = gp.mean_functions.Constant()\n",
    "        \n",
    "        model = WeightedDGP(weights,X, Y, Z, kern, likelihood,\n",
    "                    mean_function = mean, minibatch_size=minibatch,\n",
    "                    num_outputs=num_latent-1)\n",
    "        \n",
    "        var_list = []\n",
    "        for layer in model.layers[:-1]:\n",
    "            layer.q_sqrt = layer.q_sqrt.value * 1e-5 \n",
    "            var_list.append((layer.q_mu, layer.q_sqrt,XiSqrtMeanVar()))\n",
    "\n",
    "        for lik in model.likelihood.likelihood_list:\n",
    "            lik.variance.set_value(0.2)\n",
    "            lik.variance.set_trainable(False)\n",
    "        model.compile()\n",
    "    return model, var_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if False:\n",
    "    X = make_coord_array(X_t,freqs[:,None])[:,:1]\n",
    "    Y = make_data_vec(np.angle(np.exp(1j*phase[51,:,0,:,None])),freqs)#N2\n",
    "    diff = np.zeros(Y[:,:-1].shape)\n",
    "    def w(x):\n",
    "        return np.angle(np.exp(1j*x))\n",
    "    diff[:-1,:] = w(w(Y[1:,:-1]) - w(Y[:-1,:-1]))\n",
    "    weights = np.ones(diff.shape)\n",
    "    weights[np.abs(diff) > 2.5*np.std(diff)] = 0.\n",
    "\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "    #     model, var_list = make_weighted_svgp_model(weights,X,Y,freqs,M=100,minibatch=128)\n",
    "        model, var_list = make_hetero_svgp_model(X,Y,freqs,M=100,minibatch=128)\n",
    "    #     ls= sess.run(model.kern.log_ls_latent.predict_f_samples(X)[0,:,0])\n",
    "    #     plt.plot(np.exp(ls))\n",
    "    #     plt.show()\n",
    "    #     K = gp.kernels.RBF(1).compute_K(X,X)\n",
    "    #     K = model.kern.compute_K(X,None)\n",
    "    #     plt.imshow(K)\n",
    "    #     plt.colorbar()\n",
    "    #     plt.show()\n",
    "    #     L = np.linalg.cholesky(K+1e-3*np.eye(K.shape[0]))\n",
    "    #     plt.imshow(L)\n",
    "    #     plt.show()\n",
    "        run_nat_grads_with_adam(model, 1e-2, 1e-3, iterations, \n",
    "                                var_list=var_list, callback=PrintAction(model, '{} nat grads with Adam'.format(model.name)))\n",
    "\n",
    "        logging.warning(model)\n",
    "\n",
    "        ystar,varstar = batch_predict_y_svgp(model,X_t,batch_size=1000)\n",
    "        l = batch_predict_density_svgp(model,X,Y,batch_size=1000)\n",
    "        logging.warning(\"likelihood {}\".format(l))\n",
    "#         plot_single(X_t,Y,ystar,np.sqrt(varstar), ax = None)\n",
    "#         plt.show()\n",
    "        \n",
    "        mean_tec, var_tec = model.predict_f(X_t)\n",
    "        mean_tec = np.squeeze(mean_tec)\n",
    "        std_tec = np.squeeze(np.sqrt(var_tec))\n",
    "        \n",
    "        fig,axs = plt.subplots(nrows=2,sharex=True,figsize=(12,12))\n",
    "        plot_single(X_t,phase[i,:,k,0,None],ystar,np.sqrt(varstar), ax = axs[0])\n",
    "        axs[1].plot(X_t[:,0],mean_tec,label='dTEC')\n",
    "        axs[1].fill_between(X_t[:,0],mean_tec+std_tec,mean_tec-std_tec,alpha=0.3)\n",
    "        axs[1].legend()\n",
    "        plt.show()\n",
    "    #     tec = model.predict_f_samples(X_t,1000)\n",
    "        \n",
    "        \n",
    "    #     mean_log_error, var_log_error = model.log_noise_latent.predict_f(X_t)\n",
    "    #     mean_error, std_error = np.squeeze(np.exp(mean_log_error)), np.squeeze(np.sqrt(np.exp(var_log_error)))\n",
    "    # #     ls= sess.run(model.kern.log_ls_latent.predict_f_samples(X)[0,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-26 16:09:59,297 working on CS001HBA1 [Patch_0]\n",
      "2018-04-26 16:10:48,140 working on CS001HBA1 [Patch_1]\n",
      "2018-04-26 16:11:29,227 working on CS001HBA1 [Patch_10]\n",
      "2018-04-26 16:12:23,940 working on CS001HBA1 [Patch_11]\n",
      "2018-04-26 16:13:12,657 working on CS001HBA1 [Patch_12]\n",
      "2018-04-26 16:13:56,617 working on CS001HBA1 [Patch_13]\n",
      "2018-04-26 16:14:44,735 working on CS001HBA1 [Patch_14]\n",
      "2018-04-26 16:15:29,151 working on CS001HBA1 [Patch_15]\n",
      "2018-04-26 16:16:14,516 working on CS001HBA1 [Patch_16]\n",
      "2018-04-26 16:17:09,889 working on CS001HBA1 [Patch_17]\n",
      "2018-04-26 16:17:54,679 working on CS001HBA1 [Patch_18]\n",
      "2018-04-26 16:18:39,592 working on CS001HBA1 [Patch_19]\n",
      "2018-04-26 16:19:24,300 working on CS001HBA1 [Patch_20]\n",
      "2018-04-26 16:20:10,983 working on CS001HBA1 [Patch_21]\n",
      "2018-04-26 16:20:56,346 working on CS001HBA1 [Patch_22]\n",
      "2018-04-26 16:21:44,093 working on CS001HBA1 [Patch_23]\n",
      "2018-04-26 16:22:29,632 working on CS001HBA1 [Patch_24]\n",
      "2018-04-26 16:23:11,185 working on CS001HBA1 [Patch_25]\n",
      "2018-04-26 16:23:55,786 working on CS001HBA1 [Patch_26]\n",
      "2018-04-26 16:24:40,165 working on CS001HBA1 [Patch_27]\n",
      "2018-04-26 16:25:28,324 working on CS001HBA1 [Patch_28]\n"
     ]
    }
   ],
   "source": [
    "project_dir = os.path.abspath(\"project/time_only\")\n",
    "fig_savedir = os.path.join(project_dir,\"fig_saves\")\n",
    "save_dp = os.path.join(project_dir,'rvw_datapack_full_phase_dec27_time_smooth.hdf5')\n",
    "save_file = os.path.join(project_dir,\"results.hdf5\")\n",
    "def W(f):\n",
    "    return np.angle(np.exp(1j*f))\n",
    "try:\n",
    "    os.makedirs(fig_savedir)\n",
    "except:\n",
    "    pass\n",
    "iterations = 1000\n",
    "X = make_coord_array(X_t,freqs[:,None])[:,:1]\n",
    "with h5py.File(save_file) as f:\n",
    "    try:\n",
    "        f['times'] = times.mjd*86400.\n",
    "        f['directions/ra'] = directions.ra.deg\n",
    "        f['directions/dec'] = directions.dec.deg\n",
    "        f['patch_names'] = patch_names.astype(np.string_)\n",
    "        f['antennas/east'] = ant_enu.east.si.value\n",
    "        f['antennas/north'] = ant_enu.north.si.value\n",
    "        f['antennas/up'] = ant_enu.up.si.value\n",
    "        f['antenna_labels'] = antenna_labels.astype(np.string_)\n",
    "        f['phase'] = np.zeros([Na,Nt,Nd,Nf],dtype=np.float64)\n",
    "        f['phase_error'] = np.zeros([Na,Nt,Nd,Nf],dtype=np.float64)\n",
    "        f['dtec'] = np.zeros([Na,Nt,Nd],dtype=np.float64)\n",
    "        f['dtec_error'] = np.zeros([Na,Nt,Nd],dtype=np.float64)\n",
    "        f['completed'] = np.zeros([Na,Nt,Nd,Nf],dtype=np.bool)\n",
    "        f['completed'][0,...] = True\n",
    "    except Exception as e:\n",
    "        logging.warning(e)\n",
    "    for i in range(0,Na):\n",
    "        for k in range(Nd):\n",
    "            proceed = not np.all(f['completed'][i,:,k,:])\n",
    "            patience_count = 0\n",
    "            while proceed:\n",
    "                logging.warning(\"working on {} {}\".format(antenna_labels[i], patch_names[k]))\n",
    "                Y = make_data_vec(W(phase[i,:,k,:,None]),freqs)#N2\n",
    "                try:\n",
    "                    with tf.Session(graph=tf.Graph()) as sess:\n",
    "\n",
    "                        model, var_list = make_hetero_svgp_model(X,Y,freqs,M=100,minibatch=128)\n",
    "                        gp.train.AdamOptimizer(1e-2).minimize(model)\n",
    "\n",
    "                        for l in range(Nf):\n",
    "                            ystar,varstar = batch_predict_y_svgp(model,X_t,freqs[l],batch_size=1000)\n",
    "                            f['phase'][i,:,k,l] = ystar[:,0]\n",
    "                            f['phase_error'][i,:,k,l] = np.sqrt(varstar[:,0])\n",
    "\n",
    "                        mean_tec, var_tec = model.predict_f(X_t)\n",
    "                        mean_tec = np.squeeze(mean_tec)\n",
    "                        std_tec = np.squeeze(np.sqrt(var_tec))\n",
    "                        f['dtec'][i,:,k] = mean_tec\n",
    "                        f['dtec_error'][i,:,k] = std_tec\n",
    "\n",
    "                        # Plotting and saving data\n",
    "                        fig,axs = plt.subplots(nrows=2,sharex=True,figsize=(12,12))\n",
    "                        plot_single(X_t,W(phase[i,:,k,0,None]),ystar,np.sqrt(varstar), ax = axs[0])\n",
    "                        axs[0].set_title(\"{} {}\".format(antenna_labels[i], patch_names[k]))\n",
    "                        axs[1].plot(X_t[:,0],mean_tec,label='dTEC')\n",
    "                        axs[1].fill_between(X_t[:,0],mean_tec+std_tec,mean_tec-std_tec,alpha=0.3)\n",
    "                        axs[1].legend()\n",
    "                        plt.tight_layout()\n",
    "                        plt.savefig(os.path.join(fig_savedir,\"{}_{}.png\".format(antenna_labels[i],patch_names[k])))\n",
    "                        plt.close(fig)\n",
    "    #                         dp.set_phase(phase,ant_idx,time_idx,dir_idx,freq_idx)\n",
    "    #                         dp.set_variance(phase,ant_idx,time_idx,dir_idx,freq_idx)\n",
    "                        f['completed'][i,:,k,:] = True\n",
    "                        proceed = False\n",
    "                except Exception as e:\n",
    "                    logging.warning(\"Heteroscedastic solver failed {} {}. Trying once again.\".format(antenna_labels[i], patch_names[k]))\n",
    "                    patience_count += 1\n",
    "                    logging.warning(e)\n",
    "                    if patience_count > 5:\n",
    "                        logging.warning(\"By-passing {} {}\".format(antenna_labels[i], patch_names[k]))\n",
    "                        proceed = False\n",
    "\n",
    "\n",
    "    #         dp.set_prop(phase,ant_idx,time_idx,dir_idx,freq_idx)\n",
    "#             dp.save(save_dp)\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEJCAYAAACdePCvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8f79fba2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if False:\n",
    "    true_tec = 0.1\n",
    "    freqs_ = np.linspace(110e6,170e6,500)\n",
    "    X = freqs_[:,None]\n",
    "    X = X - X.mean()\n",
    "    X /= X.std()\n",
    "    Y = np.angle(np.exp(1j*true_tec*-8.4480e9/freqs_[:,None]))\n",
    "    Y += 0.1*np.random.normal(size=Y.shape)\n",
    "    Y = make_data_vec(Y, freqs_)\n",
    "    plt.plot(freqs_,Y[:,0])\n",
    "    plt.show()\n",
    "\n",
    "    def make_hetero_svgp_model(X,Y,freqs,M=None,minibatch=None):\n",
    "        N, num_latent = Y.shape\n",
    "        M = M or N\n",
    "        Z = kmeans2(X, M, minit='points')[0] if N < 10000 else X[np.random.choice(N,size=M,replace=False),:]\n",
    "        Z_kern = kmeans2(X, 10, minit='points')[0] if N < 10000 else X[np.random.choice(N,size=10,replace=False),:]\n",
    "        with gp.defer_build():\n",
    "    #         periodic_kern = gp.kernels.Periodic(1,active_dims=[0],variance=1.)\n",
    "    #         periodic_kern.variance.set_trainable(False)\n",
    "            kern = gp.kernels.Polynomial(1,degree=1,active_dims=[0])\n",
    "\n",
    "    #         Z = kmeans2(X, 10, minit='points')[0] if N < 10000 else X[np.random.choice(N,size=10,replace=False),:]\n",
    "    #         kern = NonstationaryKernel(1, Z=Z_kern, hetero_sigma=False, hetero_ls=True,  active_dims=[0], name=\"nonstationary_kernel\")\n",
    "            likelihood = HeteroscedasticWrappedPhaseGaussian( 140e6,name='hetero_likelihood')\n",
    "            mean = gp.mean_functions.Constant()\n",
    "            error_latent = make_error_latent(M=10)\n",
    "            model = HeteroscedasticSVGP(error_latent,X, Y, kern, likelihood, Z=Z, \n",
    "                        mean_function = mean, minibatch_size=minibatch,\n",
    "                        num_latent=num_latent-1)\n",
    "\n",
    "            var_list = [(model.q_mu, model.q_sqrt,XiSqrtMeanVar())]\n",
    "\n",
    "    #         model.likelihood.variance = 0.2\n",
    "    #         model.likelihood.variance.set_trainable(False)\n",
    "            model.compile()\n",
    "        return model, var_list\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        model, vars = make_hetero_svgp_model(X,Y,freqs_,10,128)\n",
    "\n",
    "    #     run_nat_grads_with_adam(model, 1e-2, 1e-3, iterations, \n",
    "    #                                 var_list=var_list, callback=PrintAction(model, '{} nat grads with Adam'.format(model.name)))\n",
    "\n",
    "    #     print(model)\n",
    "        gp.train.AdamOptimizer(1e-2).minimize(model)\n",
    "\n",
    "        ystar,varstar = batch_predict_y_svgp(model,X,batch_size=1000)\n",
    "        l = batch_predict_density_svgp(model,X,Y,batch_size=1000)\n",
    "        print(\"likelihood {}\".format(l))\n",
    "        plot_single(X,Y,ystar,np.sqrt(varstar)*0, ax = None)\n",
    "        plt.show()\n",
    "        #     tec = model.predict_f_samples(X_t,1000)\n",
    "        mean_tec, var_tec = model.predict_f(X)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
