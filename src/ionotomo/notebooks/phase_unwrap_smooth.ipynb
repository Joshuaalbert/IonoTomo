{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on antenna RS210HBA\n",
      "Working on freq 140721130.37109375\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: Optimization terminated successfully.\n",
      "  Objective function value: 1513.168114\n",
      "  Number of iterations: 19\n",
      "  Number of functions evaluations: 24\n",
      "None\n",
      "                                      class prior transform  trainable  \\\n",
      "GPR/kern/rbf_1/variance           Parameter  None       +ve       True   \n",
      "GPR/kern/rbf_1/lengthscales       Parameter  None       +ve       True   \n",
      "GPR/kern/rbf_2/variance           Parameter  None       +ve       True   \n",
      "GPR/kern/rbf_2/lengthscales       Parameter  None       +ve       True   \n",
      "GPR/likelihood/relative_variance  Parameter  None       +ve      False   \n",
      "GPR/likelihood/variance           Parameter  None       +ve      False   \n",
      "\n",
      "                                    shape  fixed_shape  \\\n",
      "GPR/kern/rbf_1/variance                ()         True   \n",
      "GPR/kern/rbf_1/lengthscales          (1,)         True   \n",
      "GPR/kern/rbf_2/variance                ()         True   \n",
      "GPR/kern/rbf_2/lengthscales          (1,)         True   \n",
      "GPR/likelihood/relative_variance  (1260,)         True   \n",
      "GPR/likelihood/variance                ()         True   \n",
      "\n",
      "                                                                              value  \n",
      "GPR/kern/rbf_1/variance                                          2.1583002975670866  \n",
      "GPR/kern/rbf_1/lengthscales                                         [1.49463098663]  \n",
      "GPR/kern/rbf_2/variance                                           2.158300297556647  \n",
      "GPR/kern/rbf_2/lengthscales                                         [2.59230116273]  \n",
      "GPR/likelihood/relative_variance  [0.0029602093189, 0.00148810121621, 0.00136041...  \n",
      "GPR/likelihood/variance                                                         1.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/anaconda3/envs/kerastf/lib/python3.6/site-packages/gpflow-1.1.0-py3.6.egg/gpflow/misc.py:30: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
      "  df = df.reindex_axis(keys, axis=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pylab as plt\n",
    "import cmocean\n",
    "from scipy.spatial import cKDTree\n",
    "from ionotomo.tomography.pipeline import Pipeline\n",
    "from ionotomo.settings import TFSettings\n",
    "from timeit import default_timer\n",
    "from ionotomo import *\n",
    "import gpflow as gp\n",
    "import sys\n",
    "#%matplotlib notebook\n",
    "\n",
    "\n",
    "class PhaseUnwrap(object):\n",
    "    \"\"\"The phase unwrapper object.\n",
    "    Unwraps 2D phase in batches.\n",
    "    Args:\n",
    "    directions : array (num_directions, 2) the directions in the 2D plane wth phases assigned\n",
    "    shape : tuple of int (num_directions, batch_dim)\n",
    "    redundancy: int (see self._create_triplets) (leave 2)\n",
    "    sess : tf.Session object to use or None for own\n",
    "    \"\"\"\n",
    "    def __init__(self,directions, shape, redundancy=2, graph = None,max_wrap_K=2, max_epochs=1000):\n",
    "        self.max_epochs = int(max_epochs)\n",
    "        self.num_logits = int(max_wrap_K)*2 + 1\n",
    "        self.directions = directions\n",
    "        self.redundancy = redundancy\n",
    "        self.shape = tuple(shape)\n",
    "        assert len(self.shape)==2, \"shape should be (num_dir, batch_size)\"\n",
    "        self.path, self.triplets,self.adjoining_map = self._create_triplets(self.directions,redundancy=self.redundancy, adjoining=True)\n",
    "        self.pairs = \\\n",
    "                np.unique(np.sort(np.concatenate([self.triplets[:,[0,1]],\n",
    "                    self.triplets[:,[1,2]], self.triplets[:,[2,0]]],axis=0),axis=1),axis=0)\n",
    "        \n",
    "        if graph is None:\n",
    "            self.graph = tf.Graph()\n",
    "        else:\n",
    "            self.graph = graph\n",
    "        self.pipeline = Pipeline()\n",
    "        self.pipeline.add_graph('phase_unwrap',self.graph)\n",
    "        \n",
    "        self.model_scope = \"phase_unwrap\"\n",
    "        with self.graph.as_default():\n",
    "            with tf.variable_scope(self.model_scope):\n",
    "                phi_wrap_placeholder = \\\n",
    "                        tf.placeholder(tf.float32,shape=shape,name='phi_wrap')\n",
    "                phi_mask_placeholder = \\\n",
    "                        tf.placeholder(tf.float32,shape=shape[1:],name='phi_mask')\n",
    "                keep_prob_placeholder = tf.placeholder(tf.float32,shape=(),name='keep_prob')\n",
    "                ent_w = tf.placeholder(tf.float32,shape=(),name='ent_w')\n",
    "                learning_rate_placeholder = tf.placeholder(tf.float32,shape=(),name='learning_rate')\n",
    "                directions = self.pipeline.add_variable(name='directions',trainable=False,init_value=self.directions,dtype=tf.float32)\n",
    "                adjoining_map = self.pipeline.add_variable(name='adjoining_map',trainable=False,init_value=self.adjoining_map,dtype=tf.int32)\n",
    "                \n",
    "                phase_unwrap_op, losses, K_greedy = self._build_phase_unwrap(phi_wrap_placeholder,phi_mask_placeholder,keep_prob_placeholder,learning_rate_placeholder,ent_w,\n",
    "                                                                          directions,adjoining_map)\n",
    "                \n",
    "                self.pipeline.store_ops([adjoining_map,directions,ent_w,phi_wrap_placeholder,phi_mask_placeholder, phase_unwrap_op, losses, K_greedy, keep_prob_placeholder, learning_rate_placeholder],\n",
    "                                        [\"adjoining_map\",\"directions\",\"ent_w\",\"phi_wrap\",\"phi_mask\",\"phase_unwrap\",\"losses\",\"K_greedy\",\"keep_prob\",\"learning_rate\"], \n",
    "                                        self.model_scope)\n",
    "                \n",
    "    def set_logits(self,sess,logits):\n",
    "        \"\"\"Set logics which are of shape (num_dir, batch_size, self.num_logits)\"\"\"\n",
    "        _ = sess.run(self.zero_logits_op,feed_dict={self.K_zero_logits_placeholder: logits})\n",
    "        \n",
    "    def maybe_unwrap(self,sess,phi_wrap):\n",
    "        \"\"\"Return a mask of shape (batch_size) with True where\n",
    "        unwrapping needed\"\"\"\n",
    "        with self.graph.as_default():\n",
    "            losses,phi_wrap_placeholder,phi_mask_placeholder, keep_prob_placeholder = \\\n",
    "                    self.pipeline.grab_ops([\"losses\",\"phi_wrap\",\"phi_mask\",\"keep_prob\"], self.model_scope)\n",
    "            #self.sess.run(tf.global_variables_initializer())\n",
    "            self.pipeline.initialize_graph(sess)\n",
    "            K_zero_logits = np.zeros(self.shape+(self.num_logits,),dtype=np.float32)\n",
    "            K_zero_logits[...,self.num_logits>>1] += 1000\n",
    "            \n",
    "            self.set_logits(sess,K_zero_logits)\n",
    "            \n",
    "            consistency_val,losses_val = sess.run([self.consistency,losses],\n",
    "                            feed_dict={phi_wrap_placeholder:phi_wrap,\n",
    "                                phi_mask_placeholder:np.ones(self.shape[1:]),\n",
    "                                keep_prob_placeholder:1.})\n",
    "            \n",
    "            mean_con = []\n",
    "            for k in range(self.shape[0]):\n",
    "                c1 = consistency_val[self.pairs[:,0] == k,:]\n",
    "                c2 = consistency_val[self.pairs[:,1] == k,:]\n",
    "                c = np.zeros(self.shape[1])\n",
    "                if c1.size > 0:\n",
    "                    c += np.mean(c1,axis=0)\n",
    "                if c2.size > 0:\n",
    "                    c += np.mean(c2,axis=0)\n",
    "                mean_con.append(c)\n",
    "                \n",
    "                \n",
    "            mask = ((losses_val[0] + losses_val[1]) > 0.01).astype(np.bool)\n",
    "            #mask_2 = (np.sum(consistency_val>0.1,axis=0) > 0).astype(np.bool)\n",
    "            return mask,np.stack(mean_con,axis=0)\n",
    "        \n",
    "    def stuck_directions(self,sess, phi):\n",
    "        _,mean_con = self.maybe_unwrap(sess,phi)\n",
    "        return np.abs(mean_con) > 0.1\n",
    "        \n",
    "        \n",
    "    def phase_unwrap(self, sess, phi_wrap, phi_mask = None):\n",
    "        \"\"\"Runs the training loop until termination\"\"\"\n",
    "        \n",
    "        if len(phi_wrap.shape) == 1:\n",
    "            phi_wrap = phi_wrap[:,None,None]\n",
    "        if phi_mask is None:\n",
    "            phi_mask,mean_con = self.maybe_unwrap(sess,phi_wrap)\n",
    "            phi_mask = np.any(np.abs(mean_con) > 0.1,axis=0)\n",
    "            #[print(m,c) for m,c in zip(phi_mask,mean_con.T)]\n",
    "            #print(phi_mask)\n",
    "            print(\"Number that need unwrapping: {}\".format(np.sum(phi_mask)))\n",
    "            print(\"Indices: {}\".format(np.where(phi_mask)))\n",
    "        assert phi_mask.shape == self.shape[1:]\n",
    "        #self._numpy_unwrap(phi_wrap, phi_mask)\n",
    "        with self.graph.as_default():\n",
    "            ent_w, phi_wrap_placeholder,phi_mask_placeholder, phase_unwrap_op, losses, K_greedy,keep_prob_placeholder, learning_rate_placeholder = \\\n",
    "                    self.pipeline.grab_ops([\"ent_w\",\"phi_wrap\",\"phi_mask\",\"phase_unwrap\",\"losses\",\"K_greedy\",\"keep_prob\",\"learning_rate\"], self.model_scope)\n",
    "            #self.sess.run(tf.global_variables_initializer())\n",
    "            self.pipeline.initialize_graph(sess)\n",
    "\n",
    "            last_losses_val_max = [np.inf]*4\n",
    "            last_losses_val_95 = [np.inf]*4\n",
    "            last_losses_val_median = [np.inf]*4\n",
    "            last_losses_val_min = [np.inf]*4\n",
    "            \n",
    "            t0 = default_timer()-10.\n",
    "            for epoch in range(self.max_epochs):\n",
    "                lr = 0.1\n",
    "                dp = 0.5\n",
    "                ew = 1e-3\n",
    "                last_lse,last_residue = last_losses_val_max[0:2]\n",
    "                if last_lse < 0.5 and last_residue < 0.5:\n",
    "                    ew = 1e-2\n",
    "                    lr = 0.1\n",
    "                    dp = 0.5\n",
    "                if last_lse < 0.3 and last_residue < 0.3:\n",
    "                    ew = 1e-2\n",
    "                    lr = 0.06\n",
    "                    dp = 0.5\n",
    "                if last_lse < 0.15 and last_residue < 0.15:\n",
    "                    ew = 1e-2\n",
    "                    lr = 0.03\n",
    "                    dp = 1.\n",
    "                if last_lse < 0.05 and last_residue < 0.05:\n",
    "                    ew = 5e-1\n",
    "                    lr = 0.01\n",
    "                    dp = 1.\n",
    "                \n",
    "                _, losses_val,K_greedy_val = sess.run([phase_unwrap_op,losses,K_greedy],\n",
    "                        feed_dict={phi_wrap_placeholder:phi_wrap,\n",
    "                phi_mask_placeholder:phi_mask,keep_prob_placeholder:dp,\n",
    "                learning_rate_placeholder:lr,\n",
    "                                  ent_w:ew})\n",
    "                #print(\"Epoch : {} loss={:.4f} | LSE: {:.4f} | Residue: {:.4f} | Entropy: {:.4f} | TV: {:.4f} \".format(epoch,np.sum(losses_val),*losses_val))\n",
    "                losses_val_max = [np.max(l) for l in losses_val]\n",
    "                losses_val_95 = [np.percentile(l,95) for l in losses_val]\n",
    "                losses_val_median = [np.percentile(l,50) for l in losses_val]\n",
    "                losses_val_min = [np.min(l) for l in losses_val]\n",
    "                \n",
    "                if (epoch + 1) % 1000 == 0 or default_timer() - t0 > 10.:\n",
    "                    print(\"Epoch : {} loss={:.4f} | LSE: {:.4f} | Residue: {:.4f} | Entropy: {:.4f} | TV: {:.4f}\".format(epoch,np.sum(losses_val_max),*losses_val_max))\n",
    "                    t0 = default_timer()\n",
    "                \n",
    "                def _relative_change(cur,last):\n",
    "                    if last is np.inf:\n",
    "                        return np.inf\n",
    "                    return np.abs((cur[0]+cur[1] - last[0]-last[1])/(last[0]+last[1]))\n",
    "                \n",
    "                no_change = _relative_change(losses_val_max, last_losses_val_max) < 1e-3 \\\n",
    "                    and _relative_change(losses_val_median, last_losses_val_median) < 1e-3 \\\n",
    "                    and _relative_change(losses_val_min, last_losses_val_min) < 1e-3\n",
    "                \n",
    "                if losses_val_max[0]+losses_val_max[1] < 0.1:# or no_change:\n",
    "                    print(\"Epoch : {} loss={:.4f} | LSE: {:.4f} | Residue: {:.4f} | Entropy: {:.4f} | TV: {:.4f}\".format(epoch,np.sum(losses_val_max),*losses_val_max))\n",
    "                    break\n",
    "                    \n",
    "                last_losses_val_max = losses_val_max\n",
    "                last_losses_val_95 = losses_val_95\n",
    "                last_losses_val_median = losses_val_median\n",
    "                last_losses_val_min = losses_val_min\n",
    "                                                            \n",
    "                         \n",
    "            f_rec = np.zeros_like(phi_wrap)\n",
    "            f_rec[self.path[0][0],...] = phi_wrap[self.path[0][0],...]\n",
    "            for i,p in enumerate(self.path):\n",
    "                df = phi_wrap[p[1],...] - phi_wrap[p[0],...] + K_greedy_val[p[1],...] - K_greedy_val[p[0],...]\n",
    "                f_rec[p[1],...] = f_rec[p[0],...] + df\n",
    "            same_mask = np.bitwise_not(phi_mask)\n",
    "            f_rec[:,same_mask] = phi_wrap[:,same_mask]\n",
    "#             sc=plt.scatter(self.directions[:,0],self.directions[:,1],c=K_cum.flatten())\n",
    "#             plt.colorbar(sc)\n",
    "#             plt.show()\n",
    "        return f_rec\n",
    "\n",
    "    def _create_triplets(self,X,redundancy=2,adjoining=True):\n",
    "        \"\"\"Create the path of closed small triplets.\n",
    "        Args:\n",
    "            X : array (N,2) the coordinates in image\n",
    "            redundancy : int the number of unique triplets \n",
    "            to makes for each segment of path.\n",
    "            adjoining: return a list of triplets that share same leg of path.\n",
    "                currently requires redundance = 2\n",
    "        Returns:\n",
    "            path, triplets\n",
    "            path, triplets, adjoining_map if adjoining=True\n",
    "        \"\"\"\n",
    "        kt = cKDTree(X)\n",
    "        #get center of map\n",
    "        C = np.mean(X,axis=0)\n",
    "        _,idx0 = kt.query(C,k=1)\n",
    "        #define unique path\n",
    "        dist, idx = kt.query(X[idx0,:],k=2)\n",
    "        path = [(idx0, idx[1])]\n",
    "        included = [idx0, idx[1]]\n",
    "        while len(included) < X.shape[0]:\n",
    "            dist,idx = kt.query(X[included,:],k = len(included)+1)\n",
    "            mask = np.where(np.isin(idx,included,invert=True))\n",
    "            argmin = np.argmin(dist[mask])\n",
    "            idx_from = included[mask[0][argmin]]\n",
    "            idx_to = idx[mask[0][argmin]][mask[1][argmin]]\n",
    "            path.append((idx_from,idx_to))\n",
    "            included.append(idx_to)\n",
    "        #If redundancy is 2 then build adjoining triplets\n",
    "        if adjoining:\n",
    "            assert redundancy == 2\n",
    "            adjoining_map = []\n",
    "        M = np.mean(X[path,:],axis=1)\n",
    "        _,idx = kt.query(M,k=2 + redundancy)\n",
    "        triplets = []\n",
    "        for i,p in enumerate(path):\n",
    "            count = 0\n",
    "            for c in range(2 + redundancy):\n",
    "                if idx[i][c] not in p:\n",
    "                    triplets.append(p + (idx[i][c],))\n",
    "                    count += 1\n",
    "                    if count == redundancy:\n",
    "                        if adjoining:\n",
    "                            adjoining_map.append([len(triplets)-2,len(triplets)-1])\n",
    "                        break\n",
    "        adjoining_map = np.sort(adjoining_map,axis=1)\n",
    "        adjoining_map = np.unique(adjoining_map,axis=0)\n",
    "        \n",
    "        triplets = np.sort(triplets,axis=1)\n",
    "        triplets, _, unique_inverse = np.unique(triplets,return_index=True,\n",
    "                                                             return_inverse=True,axis=0)\n",
    "        if adjoining:\n",
    "            #map i to j\n",
    "            for i,ui in enumerate(unique_inverse):\n",
    "                adjoining_map = np.where(adjoining_map == i, ui, adjoining_map)\n",
    "            return path,triplets,adjoining_map\n",
    "            \n",
    "        return path,triplets\n",
    "    \n",
    "    def _numpy_unwrap(self,phi_wrap,phi_mask):\n",
    "        \"\"\"diagnostic method\"\"\"\n",
    "        \n",
    "        def _wrap(array):\n",
    "            return np.angle(np.exp(1j*array))\n",
    "        \n",
    "        K = np.zeros_like(phi_wrap,dtype=np.float32)\n",
    "        f = phi_wrap + (2*np.pi)*K\n",
    "        lse = np.inf\n",
    "        while lse > 0.1:\n",
    "            for p in self.path:\n",
    "                df = _wrap(_wrap(phi_wrap[p[1],...]) - _wrap(phi_wrap[p[0],...]))\n",
    "                f[p[1],...] += df\n",
    "            lse = (f - phi_wrap)/(2*np.pi)\n",
    "            K = np.around(lse)\n",
    "            #f = phi_wrap + (2*np.pi)*K\n",
    "            lse = np.max(np.abs(lse))\n",
    "            print(lse)\n",
    "        plt.hist(K.flatten(),bins=25)\n",
    "        plt.show()\n",
    "        print(K)\n",
    "        return lse\n",
    "        \n",
    "            \n",
    "                           \n",
    "                \n",
    "\n",
    "    def _build_phase_unwrap(self, phi_wrap_placeholder,phi_mask_placeholder,\n",
    "                            keep_prob_placeholder,learning_rate_placeholder,ent_w,\n",
    "                            directions,adjoining_map):\n",
    "        \"\"\"Main phase unwrapping ops.\n",
    "        Performs the unwrap with a smooth representation of K\n",
    "        then minimizes entropy to converge on an integer.\n",
    "        Returns also the greedy solution for termination checking.\n",
    "        \"\"\"\n",
    "        with self.graph.as_default():\n",
    "            with tf.name_scope(\"unwrapper\"):\n",
    "                \n",
    "                twopi = tf.constant(2*np.pi,shape=(),dtype=tf.float32)\n",
    "                \n",
    "                def _wrap(a):\n",
    "                    \"\"\"wraps given function\"\"\"\n",
    "                    return tf.cast(tf.angle(tf.exp(1j*tf.cast(a,tf.complex64))),tf.float32)\n",
    "                def trip_arc(f,i,j):\n",
    "                    return _wrap(tf.gather(f,triplets[:,j]) - tf.gather(f,triplets[:,i]))\n",
    "\n",
    "                triplets = self.pipeline.add_variable(\"triplets\",init_value=self.triplets,\n",
    "                        dtype=tf.int32,trainable=False)\n",
    "                pairs = self.pipeline.add_variable(\"pairs\",init_value=self.pairs,\n",
    "                        dtype=tf.int32,trainable=False)\n",
    "\n",
    "                K_logits = tf.get_variable(\"K\",shape=self.shape + (self.num_logits,),\n",
    "                        dtype=tf.float32,initializer=tf.zeros_initializer)\n",
    "                \n",
    "                \n",
    "                ### set logits op\n",
    "                self.K_zero_logits_placeholder = \\\n",
    "                    tf.placeholder(shape=self.shape+(self.num_logits,), dtype=tf.float32)\n",
    "                self.zero_logits_op = tf.assign(K_logits,self.K_zero_logits_placeholder)\n",
    "                \n",
    "                ### distribution representation\n",
    "                K_dist = tf.nn.softmax(K_logits)\n",
    "\n",
    "                indices = \\\n",
    "                    tf.constant((np.arange(self.num_logits)-(self.num_logits>>1))[None,None,:], dtype=tf.float32)\n",
    "                K = tf.reduce_sum(K_dist*indices,axis=-1)*twopi\n",
    "                \n",
    "                K_greedy = tf.cast(tf.argmax(K_logits,axis=-1) - (self.num_logits>>1),tf.float32)*twopi\n",
    "                \n",
    "                ### absoulte phase representation\n",
    "                f_noise = tf.get_variable(\"f_noise\",shape=self.shape,dtype=tf.float32,initializer=tf.zeros_initializer)\n",
    "                f = phi_wrap_placeholder + K + f_noise\n",
    "                f_greedy = phi_wrap_placeholder + K_greedy + f_noise\n",
    "\n",
    "#                 ##Plates\n",
    "#                 #get the x1,x2,x3 of each triplet\n",
    "#                 pos = tf.stack([tf.tile(directions[:,0,None,None],(1,)+self.shape[1:]),\n",
    "#                        tf.tile(directions[:,1,None,None],(1,)+self.shape[1:]),\n",
    "#                        K_cum],axis=-1)\n",
    "#                 x1 = tf.gather(pos,triplets[:,0],axis=0)\n",
    "#                 x2 = tf.gather(pos,triplets[:,1],axis=0)\n",
    "#                 x3 = tf.gather(pos,triplets[:,2],axis=0)\n",
    "#                 x1x2 = x1-x2\n",
    "#                 x1x3 = x1-x3\n",
    "#                 n = tf.cross(x1x2,x1x3)\n",
    "#                 nmag = tf.norm(n,axis=-1,keep_dims=True)\n",
    "#                 n /= nmag\n",
    "#                 n1 = tf.gather(n,adjoining_map[:,0],axis=0)\n",
    "#                 n2 = tf.gather(n,adjoining_map[:,1],axis=0)\n",
    "#                 plate_tension_loss = tf.square(tf.abs(tf.matmul(n1[:,:,:,None,:],n2[:,:,:,:,None]))-1.)\n",
    "                \n",
    "                ### Consistency condition\n",
    "                def _consistency(f):\n",
    "                    df = tf.gather(f,pairs[:,1]) - tf.gather(f,pairs[:,0])\n",
    "                    consistency = tf.sqrt(1.+tf.square(_wrap(tf.gather(phi_wrap_placeholder,pairs[:,1]) - tf.gather(phi_wrap_placeholder,pairs[:,0])) - df)) - 1.\n",
    "                    return consistency\n",
    "                \n",
    "                self.consistency = _consistency(f)\n",
    "                consistency = tf.nn.dropout(self.consistency,keep_prob_placeholder)\n",
    "                consistency_greedy = _consistency(f_greedy)\n",
    "                \n",
    "                ### Residue condition\n",
    "                def _residue(f):\n",
    "                    Wf = _wrap(f)\n",
    "                    df01_ = trip_arc(Wf,0,1) \n",
    "                    df01 = _wrap(df01_) \n",
    "                    df12_ = trip_arc(Wf,1,2)\n",
    "                    df12 = _wrap(df12_)\n",
    "                    df20_ = trip_arc(Wf,2,0)\n",
    "                    df20 = _wrap(df20_)\n",
    "                    residue = tf.sqrt(1. + tf.square(df01 + df12 + df20))-1.\n",
    "                    return residue\n",
    "                self.residue = _residue(f)\n",
    "                residue = tf.nn.dropout(self.residue,keep_prob_placeholder)\n",
    "                residue_greedy = _residue(f_greedy)\n",
    "                \n",
    "                #trainable lse and residue\n",
    "                loss_residue = tf.reduce_mean(residue,axis=0)\n",
    "                loss_lse = tf.reduce_mean(consistency,axis=0)\n",
    "                loss_tv = tf.reduce_mean(tf.square(f_noise),axis=0)\n",
    "                entropy = - tf.reduce_mean(tf.reduce_sum(K_dist*tf.log(K_dist+1e-10),axis=-1),axis=0)\n",
    "                total_loss = loss_lse+loss_residue+loss_tv+ent_w*entropy\n",
    "                total_loss *= phi_mask_placeholder\n",
    "                \n",
    "                opt = tf.train.AdamOptimizer(learning_rate=learning_rate_placeholder)\n",
    "                train_op = opt.minimize(tf.reduce_mean(tf.reduce_mean(total_loss)))\n",
    "                \n",
    "                loss_residue_greedy = tf.reduce_mean(residue_greedy,axis=0)\n",
    "                loss_lse_greedy = tf.reduce_mean(consistency_greedy,axis=0)\n",
    "\n",
    "                losses = [loss_lse_greedy ,loss_residue_greedy, entropy, loss_tv]\n",
    "                #with tf.control_dependencies([train_op]):\n",
    "                return train_op, losses, K_greedy\n",
    "\n",
    "    def plot_triplets(self,figname=None):\n",
    "        fig = plt.figure(figsize=(8,8))\n",
    "        for id,(i,j,k) in enumerate(self.triplets):\n",
    "            plt.plot([self.directions[i,0],self.directions[j,0],self.directions[k,0],\n",
    "                self.directions[i,0]],[self.directions[i,1],self.directions[j,1],\n",
    "                    self.directions[k,1],self.directions[i,1]])\n",
    "            plt.text((self.directions[i,0]+self.directions[j,0]+self.directions[k,0])/3.,\n",
    "                    (self.directions[i,1]+self.directions[j,1]+self.directions[k,1])/3.,\"{}\".format(id))\n",
    "        if figname is not None:\n",
    "            plt.savefig(figname)\n",
    "            plt.close(\"all\")\n",
    "        else:\n",
    "            plt.show()\n",
    "            \n",
    "    def plot_edge_dist(self,):\n",
    "        dist = np.linalg.norm(np.concatenate([self.directions[self.triplets[:,1],:]\\\n",
    "                - self.directions[self.triplets[:,0],:], self.directions[self.triplets[:,2],:]\\\n",
    "                - self.directions[self.triplets[:,1],:],self.directions[self.triplets[:,0],:]\\\n",
    "                - self.directions[self.triplets[:,2],:]],axis=0),axis=1)\n",
    "        plt.hist(dist,bins=20)\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "def generate_data_aliased(noise=0.,sample=100):\n",
    "    \"\"\"Generate Gaussian bump in phase.\n",
    "    noise : float\n",
    "        amount of gaussian noise to add as fraction of peak height\n",
    "    sample : int\n",
    "        number to sample\n",
    "    \"\"\"\n",
    "    #max gradient at b\n",
    "    a = 50\n",
    "    b = 1\n",
    "    max_slope = np.abs(a/np.sqrt(np.exp(1))/b)\n",
    "    \n",
    "    #in dx want max_slope*dx > np.pi\n",
    "    dx = 1.5*np.pi/max_slope\n",
    "    \n",
    "    N = 10\n",
    "    xvec = np.linspace(-dx*N, dx*N, N*2 + 1)\n",
    "    X,Y = np.meshgrid(xvec,xvec,indexing='ij')\n",
    "    phi = a * np.exp(-(X**2 + Y**2)/2./b**2)\n",
    "    X = np.array([X.flatten(),Y.flatten()]).T\n",
    "    \n",
    "    phi += a*noise*np.random.normal(size=phi.shape)\n",
    "    phi = phi.flatten()\n",
    "    if sample != 0:\n",
    "        mask = np.random.choice(phi.size,size=min(sample,phi.size),replace=False)\n",
    "        return X[mask,:],phi[mask]\n",
    "    return X,phi\n",
    "\n",
    "def generate_data_nonaliased(noise=0.,sample=100):\n",
    "    \"\"\"Generate Gaussian bump in phase.\n",
    "    noise : float\n",
    "        amount of gaussian noise to add as fraction of peak height\n",
    "    sample : int\n",
    "        number to sample\n",
    "    \"\"\"\n",
    "    #max gradient at b\n",
    "    a = 15\n",
    "    b = 1\n",
    "    max_slope = np.abs(a/np.sqrt(np.exp(1))/b)\n",
    "    \n",
    "    #in dx want max_slope*dx < np.pi\n",
    "    dx = 0.5*np.pi/max_slope\n",
    "    \n",
    "    N = 10\n",
    "    xvec = np.linspace(-dx*N, dx*N, N*2 + 1)\n",
    "    X,Y = np.meshgrid(xvec,xvec,indexing='ij')\n",
    "    phi = a * np.exp(-(X**2 + Y**2)/2./b**2)\n",
    "    X = np.array([X.flatten(),Y.flatten()]).T\n",
    "    \n",
    "    phi += a*noise*np.random.normal(size=phi.shape)\n",
    "    phi = phi.flatten()\n",
    "    if sample != 0:\n",
    "        mask = np.random.choice(phi.size,size=min(sample,phi.size),replace=False)\n",
    "        return X[mask,:],phi[mask]\n",
    "    return X,phi\n",
    "\n",
    "def generate_data_nonaliased_nonsquare(noise=0.,sample=100):\n",
    "    \"\"\"Generate Gaussian bump in phase.\n",
    "    noise : float\n",
    "        amount of gaussian noise to add as fraction of peak height\n",
    "    sample : int\n",
    "        number to sample\n",
    "    \"\"\"\n",
    "    #max gradient at a\n",
    "    assert sample > 0\n",
    "    \n",
    "    dx = int(np.ceil(2*np.pi/np.sqrt(sample)))\n",
    "\n",
    "    a = 2*np.pi/dx/(2*(np.cos(2.*3/8*np.pi) - np.sin(2.*3/8*np.pi)))\n",
    "    \n",
    "    #in dx want max_slope*dx < np.pi (nyquist limit)\n",
    "    \n",
    "    \n",
    "    X = np.random.uniform(low=-np.pi,high=np.pi,size=(sample,2))\n",
    "    \n",
    "    phi = a * (np.sin(2*X[:,0]) + np.cos(2*X[:,1])) \n",
    "    \n",
    "    phi += a*noise*np.random.normal(size=phi.shape)\n",
    "\n",
    "    return X,phi\n",
    "\n",
    "def generate_data_nonaliased_nonsquare(noise=0.,sample=100,a_base=0.1):\n",
    "    \"\"\"Generate Gaussian bump in phase.\n",
    "    noise : float\n",
    "        amount of gaussian noise to add as fraction of peak height\n",
    "    sample : int\n",
    "        number to sample\n",
    "    \"\"\"\n",
    "    #max gradient at a\n",
    "    assert sample > 0\n",
    "    \n",
    "    dx = int(np.ceil(2*np.pi/np.sqrt(sample)))\n",
    "\n",
    "    a = a_base*np.pi/dx\n",
    "    \n",
    "    #in dx want max_slope*dx < np.pi (nyquist limit)\n",
    "    \n",
    "    \n",
    "    X = np.random.uniform(low=0,high=1,size=(sample,2))\n",
    "    \n",
    "    phi = a * X[:,0] + a * X[:,1]\n",
    "    \n",
    "    phi += a*noise*np.random.normal(size=phi.shape)\n",
    "    \n",
    "    phi [sample >> 1] += np.pi\n",
    "\n",
    "    return X,phi\n",
    "\n",
    "def generate_data_nonaliased_nonsquare_many(noise=0.,sample=100,batch_size=100):\n",
    "    \"\"\"Generate fake (num_direction, cumulant, indepdenent)\"\"\"\n",
    "    #max gradient at a\n",
    "    assert sample > 0\n",
    "    \n",
    "    dx = int(np.ceil(2*np.pi/np.sqrt(sample)))\n",
    "\n",
    "    a = np.pi/dx*np.ones([1,batch_size])*5#*np.random.normal(size=[1,batch_size])\n",
    "    \n",
    "    #in dx want max_slope*dx < np.pi (nyquist limit)\n",
    "    \n",
    "    \n",
    "    X = np.random.uniform(low=0,high=1,size=(sample,2))\n",
    "    \n",
    "    phi = a * X[:,0,None] + a * X[:,1,None]\n",
    "    \n",
    "    phi += a*noise*np.random.normal(size=phi.shape)\n",
    "    \n",
    "    #phi [sample >> 1,:,:] += np.pi\n",
    "\n",
    "    return X,phi\n",
    "\n",
    "def plot_phase(X,phi,label=None,figname=None):\n",
    "    \"\"\"Plot the phase.\n",
    "    X : array (num_points, 2)\n",
    "        The coords\n",
    "    phi : array (num_points,)\n",
    "        The phases\n",
    "    \"\"\"\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    sc = ax.scatter(X[:,0],X[:,1],phi,c=np.angle(np.exp(1j*phi)),cmap=cmocean.cm.phase,s=10,vmin=-np.pi,vmax=np.pi,label=label or \"\")\n",
    "    plt.colorbar(sc)\n",
    "    if label is not None:\n",
    "        plt.legend(frameon=False)\n",
    "    if figname is not None:\n",
    "        plt.savefig(figname)\n",
    "        plt.close(\"all\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def test_phase_unwrap():\n",
    "    #X,phi = generate_data_nonaliased_nonsquare(0.1,sample=100)\n",
    "    X,phi = generate_data_nonaliased_nonsquare_many(noise=0.05,sample=42,batch_size=1)\n",
    "    phi_wrap = np.angle(np.exp(1j*phi))\n",
    "    real_mask = np.sum(np.abs(phi_wrap - phi) > np.pi/2.,axis=0)\n",
    "#     graph = tf.Graph()\n",
    "#     with tf.Session(graph = graph) as sess:\n",
    "#         pu = PhaseUnwrap(X, phi_wrap.shape, redundancy=2, sess = sess, graph = graph)\n",
    "#         c_mask = pu.maybe_unwrap(phi_wrap)\n",
    "        \n",
    "    graph = tf.Graph()\n",
    "    pu = PhaseUnwrap(X, phi_wrap.shape, redundancy=2, graph = graph)\n",
    "    pu._numpy_unwrap(phi_wrap,np.ones_like(phi_wrap))\n",
    "    pu.plot_triplets()\n",
    "    pu.plot_edge_dist()\n",
    "    with tf.Session(graph = pu.graph) as sess:\n",
    "        f_rec = pu.phase_unwrap(sess,phi_wrap,phi_mask=None)\n",
    "\n",
    "    plot_phase(X,phi_wrap[:,0],label='phi_wrap',figname=None)\n",
    "    plot_phase(X,f_rec[:,0],label='f_rec',figname=None)\n",
    "    plot_phase(X,phi[:,0],label='true',figname=None)\n",
    "    plot_phase(X,(f_rec-phi)[:,0],label='f_rec - true',figname=None)\n",
    "    plot_phase(X,(f_rec-np.angle(np.exp(1j*f_rec)))[:,0]/(2*np.pi),label='jumps',figname=None)\n",
    "    plot_phase(X,(phi-phi_wrap)[:,0]/(2*np.pi),label='true jumps',figname=None)\n",
    "\n",
    "def unwrap_script(datapack,save_file,ant_idx=-1,time_idx=-1,freq_idx=-1):\n",
    "    datapack = DataPack(filename=datapack)\n",
    "    reject_list = ['CS001HBA0']\n",
    "    datapack_smooth = datapack.clone()\n",
    "    directions, patch_names = datapack.get_directions(-1)\n",
    "    times,timestamps = datapack.get_times(time_idx)\n",
    "    antennas,antenna_labels = datapack.get_antennas(ant_idx)\n",
    "    freqs = datapack.get_freqs(freq_idx)\n",
    "    if ant_idx is -1:\n",
    "        ant_idx = range(len(antennas))\n",
    "    if time_idx is -1:\n",
    "        time_idx = range(len(times))\n",
    "    if freq_idx is -1:\n",
    "        freq_idx = range(len(freqs))\n",
    "    phase = datapack.get_phase(ant_idx,time_idx,-1,freq_idx)\n",
    "    phase = np.angle(np.exp(1j*phase))\n",
    "    phase_smooth = phase.copy()#np.angle(1j*phase)\n",
    "    variance_smooth = datapack.get_variance(ant_idx,time_idx,-1,freq_idx)\n",
    "    Na,Nt,Nd,Nf = phase.shape\n",
    "    uvw = UVW(location=datapack.radio_array.get_center(), obstime=times[0],\n",
    "              phase=datapack.get_center_direction())\n",
    "    dirs_uvw = directions.transform_to(uvw)\n",
    "    X = np.array([np.arctan2(dirs_uvw.u.value, dirs_uvw.w.value),\n",
    "                 np.arctan2(dirs_uvw.v.value, dirs_uvw.w.value)]).T\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    pu = PhaseUnwrap(X, (Nd,Nt), redundancy=2, graph = graph)\n",
    "    pu.plot_triplets()\n",
    "    with tf.Session(graph = pu.graph) as sess:\n",
    "        for i in range(Na):\n",
    "            if antenna_labels[i] in reject_list:\n",
    "                continue\n",
    "            print(\"Working on antenna {}\".format(antenna_labels[i]))\n",
    "            for l in range(Nf):\n",
    "                print(\"Working on freq {}\".format(freqs[l]))\n",
    "                phi_wrap = phase[i,:,:,l].transpose()\n",
    "                f_rec = pu.phase_unwrap(sess,phi_wrap,phi_mask=None).transpose()\n",
    "                stuck_directions = pu.stuck_directions(sess, f_rec)\n",
    "                phase_smooth[i,:,:,l] = f_rec\n",
    "                variance_smooth[i,:,:,l][stuck_directions.T] = -2*np.pi\n",
    "                K = f_rec - phase[i,:,:,l]\n",
    "                ##Difference applied to rest of freqs\n",
    "                phase[i,:,:,l+1:] += K[:,:,None]\n",
    "            datapack_smooth.set_phase(phase_smooth[i,:,:,:],ant_idx=[ant_idx[i]],time_idx=time_idx,dir_idx=-1,freq_idx=freq_idx)\n",
    "            datapack_smooth.set_variance(variance_smooth[i,:,:,:],ant_idx=[ant_idx[i]],time_idx=time_idx,dir_idx=-1,freq_idx=freq_idx)\n",
    "            datapack_smooth.save(save_file)\n",
    "\n",
    "from scipy.special import erf\n",
    "def outlier_detection(k, y_star, var_star, y_obs, obs_noise = 1.0):\n",
    "    def _pdf(z):\n",
    "        return np.exp(-0.5*z**2)/np.sqrt(np.pi*2)\n",
    "    def _cdf(z):\n",
    "        return 0.5*(1+erf(z/np.sqrt(2)))\n",
    "    residual = np.abs(y_obs - y_star)\n",
    "    \n",
    "    error_star = np.sqrt(var_star)\n",
    "    z = (k*obs_noise - residual)/error_star\n",
    "    prob = 1 - _cdf(z)\n",
    "    return prob\n",
    "\n",
    "def presmooth_outlier(phase,error,k=2):\n",
    "    mean = np.mean(phase,axis=2,keepdims=True)\n",
    "    std = np.std(phase,axis=2,keepdims=True)\n",
    "    return np.abs(mean-phase) > k*std\n",
    "    prob_mask = outlier_detection(k,mean, std**2,phase,error)\n",
    "    return prob_mask\n",
    "\n",
    "def calibrate_presmooth(datapack,ant_idx=-1,time_idx=-1,freq_idx=-1):\n",
    "    datapack = DataPack(filename=datapack)\n",
    "    reject_list = ['CS001HBA0']\n",
    "    directions, patch_names = datapack.get_directions(-1)\n",
    "    times,timestamps = datapack.get_times(time_idx)\n",
    "    antennas,antenna_labels = datapack.get_antennas(ant_idx)\n",
    "    freqs = datapack.get_freqs(freq_idx)\n",
    "    phase = datapack.get_phase(ant_idx,time_idx,-1,freq_idx)\n",
    "    error = np.sqrt(datapack.get_variance(ant_idx,time_idx,-1,freq_idx))\n",
    "    plot_datapack(datapack,ant_idx,time_idx,-1,freq_idx,phase_wrap=True,observable='phase',\n",
    "                     plot_facet_idx=True,plot_crosses=False)\n",
    "    variance_smooth = error**2\n",
    "    uvw = UVW(location=datapack.radio_array.get_center(), obstime=times[0],\n",
    "              phase=datapack.get_center_direction())\n",
    "    dirs_uvw = directions.transform_to(uvw)\n",
    "    d = np.array([np.arctan2(dirs_uvw.u.value, dirs_uvw.w.value),\n",
    "                 np.arctan2(dirs_uvw.v.value, dirs_uvw.w.value)]).T\n",
    "    spatial_scale = np.std(d)\n",
    "    d /= spatial_scale\n",
    "    X = d\n",
    "    Y = phase\n",
    "    with  gp.defer_build():\n",
    "        k_space = gp.kernels.RBF(2,active_dims = [0,1],lengthscales=[0.1])\n",
    "        k_time = gp.kernels.RBF(1,active_dims = [2],lengthscales=[0.25])\n",
    "        #+gp.kernels.White(1,variance=0.01)\n",
    "    #     k.lengthscales.transform = gp.transforms.Logistic(0.1,1)\n",
    "        #white = gp.kernels.White(1,variance=0.01)\n",
    "        #white.variance.set_trainable(False)\n",
    "        kern = k_space*k_time\n",
    "        mean = gp.mean_functions.Zero()\n",
    "        m = gp.models.GPR(X, Y, kern, mean_function=mean,var=var)\n",
    "        m.compile()\n",
    "        print(o.minimize(m,maxiter=1000))\n",
    "        print(m)\n",
    "        ystar,varstar = m.predict_f(X)\n",
    "        \n",
    "    for k in np.linspace(2.0,2.5,10):\n",
    "        \n",
    "        print(k)\n",
    "        prob_mask = presmooth_outlier(phase,error,k=k)\n",
    "        datapack.set_variance(prob_mask,ant_idx,time_idx,-1,freq_idx)\n",
    "        plot_datapack(datapack,ant_idx,time_idx,-1,freq_idx,phase_wrap=False,observable='variance',\n",
    "                     plot_facet_idx=True,plot_crosses=False)\n",
    "    \n",
    "\n",
    "def smooth_script(datapack, save_file, param_file=None, ant_idx=-1,time_idx=-1,freq_idx=-1,time_skip=1):\n",
    "    \"\"\"Script to smooth a datapack\"\"\"\n",
    "    datapack = DataPack(filename=datapack)\n",
    "    reject_list = ['CS001HBA0']\n",
    "    datapack_smooth = datapack.clone()\n",
    "    directions, patch_names = datapack.get_directions(-1)\n",
    "    times,timestamps = datapack.get_times(time_idx)\n",
    "    antennas,antenna_labels = datapack.get_antennas(ant_idx)\n",
    "    freqs = datapack.get_freqs(freq_idx)\n",
    "    if ant_idx is -1:\n",
    "        ant_idx = range(len(antennas))\n",
    "    if time_idx is -1:\n",
    "        time_idx = range(len(times))\n",
    "    if freq_idx is -1:\n",
    "        freq_idx = range(len(freqs))\n",
    "    if param_file is None:\n",
    "        param_file = 'param_file_{}_{}'.format(time_idx[0],time_idx[-1])\n",
    "    phase = datapack.get_phase(ant_idx,time_idx,-1,freq_idx)\n",
    "    error = np.sqrt(datapack.get_variance(ant_idx,time_idx,-1,freq_idx))\n",
    "    variance_smooth = error**2\n",
    "    \n",
    "#     ###\n",
    "#     # pre-outlier detection (which facets failed to unwrap or are large errors)\n",
    "#     mean = np.mean(phase,axis=2,keepdims=True)\n",
    "#     std = np.std(phase,axis=2,keepdims=True)\n",
    "#     mask = np.abs(phase-mean) > 3*std\n",
    "#     prob_mask = outlier_detection(2,mean, std**2,phase,error)\n",
    "    \n",
    "#     error[mask] = 2*np.pi # not contributing much 1 radian error\n",
    "    phase_smooth = phase.copy()#np.angle(1j*phase)\n",
    "    Na,Nt,Nd,Nf = phase.shape\n",
    "    uvw = UVW(location=datapack.radio_array.get_center(), obstime=times[0],\n",
    "              phase=datapack.get_center_direction())\n",
    "    dirs_uvw = directions.transform_to(uvw)\n",
    "    d = np.array([np.arctan2(dirs_uvw.u.value, dirs_uvw.w.value),\n",
    "                 np.arctan2(dirs_uvw.v.value, dirs_uvw.w.value)]).T\n",
    "    spatial_scale = np.std(d)\n",
    "    d /= spatial_scale\n",
    "    \n",
    "    t = times.gps\n",
    "    t -= np.mean(t)\n",
    "    time_scale = np.std(t)\n",
    "    t /= time_scale\n",
    "    \n",
    "    X = np.zeros([Nt,Nd,3],dtype=np.float64)\n",
    "    for j in range(Nt):\n",
    "        for k in range(Nd):\n",
    "            X[j,k,0:2] = d[k,:]\n",
    "            X[j,k,2] = t[j]   \n",
    "    X = np.reshape(X,(Nt*Nd,3))\n",
    "    data = {'antenna_labels':antenna_labels, \n",
    "            'freqs':freqs, \n",
    "            'length_scale':np.zeros([Na,Nt,Nf]), \n",
    "            'kernel_variance':np.zeros([Na,Nt,Nf]),\n",
    "            'time_scale':np.zeros([Na,Nt,Nf]), \n",
    "            'variance_scale':np.zeros([Na,Nt,Nf])}\n",
    "    \n",
    "    o = gp.train.ScipyOptimizer(method='BFGS')\n",
    "    with  gp.defer_build():\n",
    "        k_space = gp.kernels.RBF(2,active_dims = [0,1],lengthscales=[0.1])\n",
    "        k_time = gp.kernels.RBF(1,active_dims = [2],lengthscales=[0.25])\n",
    "        #+gp.kernels.White(1,variance=0.01)\n",
    "    #     k.lengthscales.transform = gp.transforms.Logistic(0.1,1)\n",
    "        #white = gp.kernels.White(1,variance=0.01)\n",
    "        #white.variance.set_trainable(False)\n",
    "        kern = k_space*k_time\n",
    "        mean = gp.mean_functions.Zero()\n",
    "    #     m = gp.models.GPR(X, Y, kern=kern, mean_function=mean)\n",
    "    #     m.likelihood = TorusGaussian()\n",
    "        for i, ai in enumerate(ant_idx):\n",
    "            if antenna_labels[i] in reject_list:\n",
    "                continue\n",
    "            print(\"Working on antenna {}\".format(antenna_labels[i]))\n",
    "            for l, fi in enumerate(freq_idx):\n",
    "                print(\"Working on freq {}\".format(freqs[l]))\n",
    "                Y = phase[i,:,:,l].flatten()[:,None]\n",
    "                y_mean = np.mean(Y)\n",
    "                y_scale = np.std(Y)\n",
    "                Y -= y_mean\n",
    "                Y /= y_scale\n",
    "                var = (error[i,:,:,l].flatten()/y_scale)**2\n",
    "                var[error[i,:,:,l].flatten() < 0] = 100.\n",
    "                \n",
    "                \n",
    "                m = gp.models.GPR(X, Y, kern, mean_function=mean,var=var)\n",
    "                m.likelihood.variance.set_trainable(False)\n",
    "                m.compile()\n",
    "                print(o.minimize(m,maxiter=1000))\n",
    "                print(m)\n",
    "                data['length_scale'][i,:,l] = m.kern.rbf_1.lengthscales.value[0]*spatial_scale\n",
    "                data['time_scale'][i,:,l] = m.kern.rbf_2.lengthscales.value[0]*time_scale\n",
    "                data['kernel_variance'][i,:,l] = m.kern.rbf_1.variance.value*m.kern.rbf_2.variance.value*y_scale**2\n",
    "                data['variance_scale'][i,:,l] = m.likelihood.variance.value*y_scale**2\n",
    "                ystar,varstar = m.predict_f(X)\n",
    "                \n",
    "                phase_smooth[i,:,:,l] = (ystar*y_scale + y_mean).reshape((Nt,Nd))\n",
    "                variance_smooth[i,:,:,l] = (varstar*y_scale**2).reshape((Nt,Nd))\n",
    "                \n",
    "                \n",
    "            with h5py.File(param_file,'w') as pf:\n",
    "                for key in data:\n",
    "                    pf[key] = data[key]\n",
    "            datapack_smooth.set_phase(phase_smooth[i,:,:,:],ant_idx=[ai],time_idx=time_idx,dir_idx=-1,freq_idx=freq_idx)\n",
    "            datapack_smooth.set_variance(variance_smooth[i,:,:,:],ant_idx=[ai],time_idx=time_idx,dir_idx=-1,freq_idx=freq_idx)\n",
    "            \n",
    "            datapack_smooth.save(save_file)\n",
    "        \n",
    "    \n",
    "def animate_datapack(datapack,**kwargs):\n",
    "    from ionotomo.plotting.plot_tools import animate_datapack\n",
    "    datapack = DataPack(filename=datapack)\n",
    "    phase = datapack.get_phase(-1,range(20),-1,-1)\n",
    "#     Na,Nt,Nd,Nf = phase.shape\n",
    "#     vmin = np.min(phase,axis=2,keepdims=True)\n",
    "#     K = np.zeros_like(phase)\n",
    "#     while np.any(vmin < -np.pi):\n",
    "#         dK = np.where(vmin < -np.pi, 2*np.pi, 0.)\n",
    "#         phase += dK\n",
    "#         K += dK\n",
    "#         vmin = np.min(phase,axis=2,keepdims=True)\n",
    "#     vmax = np.max(phase,axis=2,keepdims=True)\n",
    "#     while np.any(vmax > np.pi):\n",
    "#         dK = np.where(vmax > np.pi, -2*np.pi, 0.)\n",
    "#         phase += dK\n",
    "#         K += dK\n",
    "#         vmax = np.max(phase,axis=2,keepdims=True)\n",
    "#     import pylab as plt\n",
    "#     plt.figure(figsize=(16,16))\n",
    "#     [plt.hist(K[i,...].flatten(),alpha=0.1,label=\"D{}\".format(i)) for i in range(Na)]\n",
    "#     plt.show()\n",
    "#     datapack.set_phase(phase,-1,-1,-1,-1)\n",
    "#     #datapack.save(\"../data/rvw_datapack_full_phase_dec27_smooth_shianimate_datapackmate_datapackmate_datapackhdf5\")\n",
    "    animate_datapack(datapack,\"phase_unwrapped_animation_RS210_std_ex\", \n",
    "                     ant_idx=-1,time_idx=range(30),dir_idx=-1,num_threads=1,mode='perantenna',observable='std',\n",
    "                     phase_wrap=True,vmin=None,vmax=None,**kwargs)\n",
    "    animate_datapack(datapack,\"phase_unwrapped_animation_RS210_phase_ex\", \n",
    "                     ant_idx=-1,time_idx=range(30),dir_idx=-1,num_threads=1,mode='perantenna',observable='phase',\n",
    "                     phase_wrap=True,vmin=None,vmax=None,**kwargs)\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "#    calibrate_presmooth(\"../data/rvw_datapack_full_phase_dec27.hdf5\",ant_idx=-1,time_idx=[0],freq_idx=-1)\n",
    "    \n",
    "#    test_phase_unwrap()\n",
    "    if len(sys.argv) == 2:\n",
    "        starting_datapack = sys.argv[1]\n",
    "    else:\n",
    "        starting_datapack = \"../data/rvw_datapack_full_phase_dec27.hdf5\"\n",
    "    unwrap_script(starting_datapack,starting_datapack.replace('.hdf5','_unwrap.hdf5'),\n",
    "                 ant_idx=-1,time_idx=-1,freq_idx=-1)\n",
    "    smooth_script(starting_datapack.replace('.hdf5','_unwrap.hdf5'),starting_datapack.replace('.hdf5','_unwrap_smooth.hdf5'), \n",
    "                  ant_idx=-1,time_idx=range(100),freq_idx=-1,time_skip=3)\n",
    "#     animate_datapack(\"../data/rvw_datapack_full_phase_dec27_smooth_v2_bayes_ex.hdf5\",freq_idx=[10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
