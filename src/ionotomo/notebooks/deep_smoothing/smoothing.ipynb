{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(message)s')\n",
    "import pylab as plt\n",
    "import cmocean\n",
    "from scipy.spatial import cKDTree\n",
    "from ionotomo.tomography.pipeline import Pipeline\n",
    "from ionotomo.settings import TFSettings\n",
    "from timeit import default_timer\n",
    "from ionotomo import *\n",
    "import astropy.coordinates as ac\n",
    "import astropy.units as au\n",
    "import gpflow as gp\n",
    "import sys\n",
    "import h5py\n",
    "import threading\n",
    "from timeit import default_timer\n",
    "#%matplotlib notebook\n",
    "from concurrent import futures\n",
    "from functools import partial\n",
    "from threading import Lock\n",
    "import astropy.units as au\n",
    "import astropy.time as at\n",
    "from collections import deque\n",
    "from doubly_stochastic_dgp.dgp import DGP\n",
    "\n",
    "from ionotomo.bayes.gpflow_contrib import GPR_v2,Gaussian_v2\n",
    "from scipy.cluster.vq import kmeans2\n",
    "\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NNComposedKernel(gp.kernels.Kernel):\n",
    "    \"\"\"\n",
    "    This kernel class allows for easily adding a NN (or other function) to a GP model.\n",
    "    The kernel does not actually do anything with the NN.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, kern, f, f_scope):\n",
    "        \"\"\"\n",
    "        kern.input_dim needs to be consistent with the output dimension of f\n",
    "        \"\"\"\n",
    "        super().__init__(kern.input_dim,active_dims=kern.active_dims)\n",
    "        self.kern = kern\n",
    "        self._f = lambda x: tf.cast(f(x), gp.settings.float_type) #function to call on input\n",
    "        self._f_scope = f_scope #learnable variables that f depends on\n",
    "        \n",
    "    def f(self, X):\n",
    "        if X is not None:\n",
    "            return self._f(X)\n",
    "    \n",
    "    def _get_f_vars(self):        \n",
    "        return tf.trainable_variables(scope=self._f_scope)\n",
    "\n",
    "    @gp.autoflow([gp.settings.float_type, [None,None]])\n",
    "    def compute_f(self, X):\n",
    "        return self.f(X)\n",
    "    \n",
    "    def K(self, X, X2=None, presliced=False):\n",
    "        if not presliced:\n",
    "            X, X2 = self._slice(X, X2)        \n",
    "        return self.kern.K(self.f(X), self.f(X2), presliced=True)\n",
    "    \n",
    "    def Kdiag(self, X, presliced=False):\n",
    "        if not presliced:\n",
    "            X,_ = self._slice(X, None)\n",
    "        return self.kern.Kdiag(self.f(X))\n",
    "\n",
    "    \n",
    "# we need to add these extra functions to the model so the tensorflow variables get picked up\n",
    "class NN_SVGP(gp.models.svgp.SVGP):\n",
    "    def get_NNKernels(self,kern=None):\n",
    "        if kern is None:\n",
    "            kern = self.kern\n",
    "        out = []\n",
    "        for c in kern.children.values():\n",
    "            if isinstance(c,gp.kernels.Kernel):\n",
    "                if isinstance(c,NNComposedKernel):\n",
    "                    out.append(c)\n",
    "                else:\n",
    "                    out = out + self.get_NNKernels(c)\n",
    "        return out\n",
    "    @property\n",
    "    def all_f_vars(self):\n",
    "        NN_kerns = self.get_NNKernels()\n",
    "        f_vars = []\n",
    "        for k in NN_kerns:\n",
    "            f_vars = f_vars + k._get_f_vars()\n",
    "        return f_vars\n",
    "        \n",
    "            \n",
    "    @property\n",
    "    def trainable_tensors(self):\n",
    "        f_vars = self.all_f_vars\n",
    "        try:\n",
    "            return super().trainable_tensors + f_vars\n",
    "        except:\n",
    "            return super().trainable_tensors\n",
    "            \n",
    "    @property\n",
    "    def initializables(self):\n",
    "        f_vars = self.all_f_vars\n",
    "        try:\n",
    "            return super().initializables + f_vars\n",
    "        except:\n",
    "            return super().initializables\n",
    "        \n",
    "def scaled_square_dist(X,X2,lengthscales):\n",
    "    '''\n",
    "    r_ij = sum_k (x_ik - x_jk)*(x_ik - x_jk)\n",
    "        = sum_k x_ik*x_ik - x_ik*x_jk - x_jk*x_ik + x_jk*x_jk\n",
    "        =  Xs - 2 X.X^t + Xs.T\n",
    "        \n",
    "    r_ij = sum_k (x_ik - y_jk)*(x_ik - y_jk)\n",
    "        = sum_k x_ik*x_ik - x_ik*y_jk - y_jk*x_ik + y_jk*y_jk\n",
    "        =  Xs - 2 X.X^t + Ys.T\n",
    "    '''\n",
    "    X = X / lengthscales\n",
    "    Xs = tf.reduce_sum(tf.square(X), axis=1)\n",
    "\n",
    "    if X2 is None:\n",
    "        dist = -2 * tf.matmul(X, X, transpose_b=True)\n",
    "        dist += tf.reshape(Xs, (-1, 1))  + tf.reshape(Xs, (1, -1))\n",
    "        return dist\n",
    "\n",
    "    X2 = X2 / lengthscales\n",
    "    X2s = tf.reduce_sum(tf.square(X2), axis=1)\n",
    "    dist = -2 * tf.matmul(X, X2, transpose_b=True)\n",
    "    dist += tf.reshape(Xs, (-1, 1)) + tf.reshape(X2s, (1, -1))\n",
    "    return dist\n",
    "\n",
    "def anisotropic_modulation(ndim, M, scope):\n",
    "    def modulation(X,ndim=ndim,M=M,scope=scope):\n",
    "        with tf.variable_scope(scope,reuse=tf.AUTO_REUSE) as scope:\n",
    "            factor = tf.get_variable(\"factor\",shape=(M,1),dtype=gp.settings.float_type,\n",
    "                                     initializer=\\\n",
    "                                     tf.zeros_initializer(dtype=gp.settings.float_type))\n",
    "            factor = 1.5*tf.nn.sigmoid(factor) + 0.25 # between 0.25 and 1.75 modulations starting at 1.\n",
    "            points = tf.get_variable(\"points\",shape=(M,ndim),dtype=gp.settings.float_type,\n",
    "                                     initializer=\\\n",
    "                                     tf.random_uniform_initializer(minval=-2,maxval=2,dtype=gp.settings.float_type))\n",
    "            scale = tf.nn.softplus(tf.get_variable(\"scale\",shape=(),dtype=gp.settings.float_type,\n",
    "                                    initializer=tf.ones_initializer(dtype=gp.settings.float_type))) + 1e-6\n",
    "            dist = scaled_square_dist(X,points,scale)\n",
    "            weights = tf.exp(-dist/2.) #N, M\n",
    "            weights /= tf.reduce_sum(weights,axis=1,keepdims=True,name='weights')# N,1\n",
    "            factor = tf.matmul(weights, factor,name='factor')#N, 1\n",
    "            res = X/factor      \n",
    "            return res\n",
    "    return modulation\n",
    "        \n",
    "def _synced_minibatch(*X,minibatch_size=100,seed=0, sess=None, shuffle = True):\n",
    "    init_placeholders = tuple([tf.placeholder(gp.settings.tf_float,shape=x.shape) for x in X])\n",
    "    data = tf.data.Dataset.from_tensor_slices(init_placeholders)\n",
    "    data = data.repeat()\n",
    "    if shuffle:\n",
    "        data = data.shuffle(buffer_size=X[0].shape[0], seed=seed)\n",
    "    data = data.batch(batch_size=tf.constant(minibatch_size,dtype=tf.int64))\n",
    "    iterator_tensor = data.make_initializable_iterator()\n",
    "    if sess is not None:\n",
    "        sess.run(iterator_tensor.initializer, feed_dict={p:x for p,x in zip(init_placeholders,X)})\n",
    "    return init_placeholders, iterator_tensor.initializer, iterator_tensor.get_next()\n",
    "\n",
    "class WeightedSVGP(NN_SVGP):\n",
    "    def __init__(self, obs_weight, X, Y, kern, likelihood, feat=None,\n",
    "                 mean_function=None,\n",
    "                 num_latent=None,\n",
    "                 q_diag=False,\n",
    "                 whiten=True,\n",
    "                 minibatch_size=None,\n",
    "                 Z=None,\n",
    "                 num_data=None,\n",
    "                 **kwargs):\n",
    "        super(WeightedSVGP,self).__init__(X, Y, kern, likelihood, feat=feat,\n",
    "                 mean_function=mean_function,\n",
    "                 num_latent=num_latent,\n",
    "                 q_diag=q_diag,\n",
    "                 whiten=whiten,\n",
    "                 minibatch_size=None,\n",
    "                 Z=Z,\n",
    "                 num_data=num_data,\n",
    "                 **kwargs)\n",
    "        self.obs_weight = gp.DataHolder(obs_weight) \\\n",
    "            if minibatch_size is None else gp.Minibatch(obs_weight,batch_size=minibatch_size, seed=0)\n",
    "\n",
    "        \n",
    "    @gp.params_as_tensors\n",
    "    def _build_likelihood(self):\n",
    "        \"\"\"\n",
    "        This gives a variational bound on the model likelihood.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get prior KL.\n",
    "        KL = self.build_prior_KL()\n",
    "\n",
    "        # Get conditionals\n",
    "        fmean, fvar = self._build_predict(self.X, full_cov=False)\n",
    "\n",
    "        # Get variational expectations.\n",
    "        var_exp = self.likelihood.variational_expectations(fmean, fvar, self.Y) * self.obs_weight\n",
    "\n",
    "        # re-scale for minibatch size\n",
    "        scale = tf.cast(self.num_data, gp.settings.float_type) / tf.cast(tf.shape(self.X)[0], gp.settings.float_type)\n",
    "        scale = scale / tf.reduce_mean(self.obs_weight)\n",
    "        return tf.reduce_sum(var_exp) * scale - KL\n",
    "\n",
    "\n",
    "def get_only_vars_in_model(variables, model):\n",
    "    reader = tf.train.NewCheckpointReader(model)\n",
    "    var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "    vars_in_model = [k for k in sorted(var_to_shape_map)]\n",
    "    out_vars = []\n",
    "    for var in variables:\n",
    "        v = var.name.split(\":\")[0]\n",
    "        \n",
    "        if v in vars_in_model:\n",
    "            if tuple(var.shape.as_list()) != reader.get_tensor(v).shape:\n",
    "                logging.warning(\"{} has shape mis-match: {} {}\".format(v,\n",
    "                    tuple(var.shape.as_list()), reader.get_tensor(v).shape))\n",
    "                continue\n",
    "            out_vars.append(var)\n",
    "    return out_vars\n",
    "\n",
    "def rename(model,prefix='WeightedSVGP',index=\"\"):\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        reader = tf.train.NewCheckpointReader(model)\n",
    "        var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "        vars_in_model = [k for k in sorted(var_to_shape_map)]\n",
    "        for v in vars_in_model:\n",
    "            t = reader.get_tensor(v)\n",
    "            if 'WeightedSVGP' in v:\n",
    "                new_name = \"/\".join(['WeightedSVGP-{}'.format(index)] + v.split('/')[1:])\n",
    "                var =  tf.Variable(t,name=new_name)\n",
    "            else:\n",
    "                var = tf.Variable(t,name=v)\n",
    "        saver = tf.train.Saver()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.save(sess, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-19 01:45:22,990 Working on shapes (2, 3595, 42, 2)\n",
      "2018-04-19 01:45:23,004 Total masked phases: 0\n",
      "2018-04-19 01:45:23,067 Working on time chunk (0) 2016-12-08T23:20:29.006 to (29) 2016-12-08T23:24:21.328\n",
      "/home/josh/anaconda3/envs/kerastf/lib/python3.6/site-packages/scipy/cluster/vq.py:523: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  warnings.warn(\"One of the clusters is empty. \"\n",
      "2018-04-19 01:45:24,892 'numpy.float64' object has no attribute 'name'\n",
      "2018-04-19 01:45:24,893 Unable to load /home/josh/git/IonoTomo/src/ionotomo/notebooks/deep_smoothing/projects/model_0_30\n",
      "2018-04-19 01:45:25,754 Initial log-likelihood -14588.896607918328\n",
      "/home/josh/anaconda3/envs/kerastf/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "2018-04-19 01:45:36,749 Final log-likelihood -11178.950805736109\n",
      "2018-04-19 01:45:36,812 f_space_var/factor:0 [[0.09614395]\n",
      " [0.09586118]\n",
      " [0.09722018]\n",
      " [0.09578728]\n",
      " [0.09601061]\n",
      " [0.0974034 ]\n",
      " [0.0974423 ]]\n",
      "2018-04-19 01:45:36,815 f_space_var/points:0 [[-1.1375604   0.01731014]\n",
      " [-0.74153517 -1.45393899]\n",
      " [ 1.11002096 -1.18809097]\n",
      " [-1.85034615 -0.19950268]\n",
      " [-0.59986216 -1.12541186]\n",
      " [ 1.99638507 -0.24206543]\n",
      " [ 1.49612542  0.86835485]]\n",
      "2018-04-19 01:45:36,817 f_space_var/scale:0 1.0382613499181506\n",
      "2018-04-19 01:45:36,819 f_space_ls/factor:0 [[0.09758525]\n",
      " [0.09754768]\n",
      " [0.09761619]\n",
      " [0.09761459]\n",
      " [0.09760234]\n",
      " [0.09760925]\n",
      " [0.09761834]]\n",
      "2018-04-19 01:45:36,821 f_space_ls/points:0 [[-0.7877235   1.1913603 ]\n",
      " [-0.93551677  2.0000371 ]\n",
      " [-1.10404056 -1.11192915]\n",
      " [ 1.62975471  1.82730957]\n",
      " [-1.90456688 -1.94012221]\n",
      " [-0.85670675 -1.73946086]\n",
      " [ 1.01417269  0.40681615]]\n",
      "2018-04-19 01:45:36,823 f_space_ls/scale:0 0.9844767714973746\n",
      "2018-04-19 01:45:36,825 f_time_ls/factor:0 [[0.09898249]\n",
      " [0.09907512]\n",
      " [0.0989474 ]\n",
      " [0.09903546]]\n",
      "2018-04-19 01:45:36,828 f_time_ls/points:0 [[ 0.68948777]\n",
      " [-1.87660627]\n",
      " [ 1.68515886]\n",
      " [-0.85661151]]\n",
      "2018-04-19 01:45:36,829 f_time_ls/scale:0 1.0033569377758385\n",
      "2018-04-19 01:45:37,336 Saved model to /home/josh/git/IonoTomo/src/ionotomo/notebooks/deep_smoothing/projects/model_0_30\n",
      "2018-04-19 01:45:37,338                                                         class  \\\n",
      "WeightedSVGP/kern/nncomposedkernel_1/kern/variance  Parameter   \n",
      "WeightedSVGP/kern/nncomposedkernel_1/kern/offset    Parameter   \n",
      "WeightedSVGP/kern/nncomposedkernel_2/kern/variance  Parameter   \n",
      "WeightedSVGP/kern/nncomposedkernel_2/kern/lengt...  Parameter   \n",
      "WeightedSVGP/kern/nncomposedkernel_3/kern/variance  Parameter   \n",
      "WeightedSVGP/kern/nncomposedkernel_3/kern/lengt...  Parameter   \n",
      "WeightedSVGP/kern/rbf/variance                      Parameter   \n",
      "WeightedSVGP/kern/rbf/lengthscales                  Parameter   \n",
      "WeightedSVGP/kern/polynomial/variance               Parameter   \n",
      "WeightedSVGP/kern/polynomial/offset                 Parameter   \n",
      "WeightedSVGP/likelihood/variance                    Parameter   \n",
      "WeightedSVGP/feature/Z                              Parameter   \n",
      "WeightedSVGP/q_mu                                   Parameter   \n",
      "WeightedSVGP/q_sqrt                                 Parameter   \n",
      "\n",
      "                                                                           prior  \\\n",
      "WeightedSVGP/kern/nncomposedkernel_1/kern/variance                          None   \n",
      "WeightedSVGP/kern/nncomposedkernel_1/kern/offset                            None   \n",
      "WeightedSVGP/kern/nncomposedkernel_2/kern/variance                          None   \n",
      "WeightedSVGP/kern/nncomposedkernel_2/kern/lengt...        N([0.],[188.48023099])   \n",
      "WeightedSVGP/kern/nncomposedkernel_3/kern/variance                          None   \n",
      "WeightedSVGP/kern/nncomposedkernel_3/kern/lengt...          N([0.],[2.88434855])   \n",
      "WeightedSVGP/kern/rbf/variance                                              None   \n",
      "WeightedSVGP/kern/rbf/lengthscales                  N([0.36047264],[0.36047264])   \n",
      "WeightedSVGP/kern/polynomial/variance                                       None   \n",
      "WeightedSVGP/kern/polynomial/offset                                         None   \n",
      "WeightedSVGP/likelihood/variance                                            None   \n",
      "WeightedSVGP/feature/Z                                                      None   \n",
      "WeightedSVGP/q_mu                                                           None   \n",
      "WeightedSVGP/q_sqrt                                                         None   \n",
      "\n",
      "                                                     transform  trainable  \\\n",
      "WeightedSVGP/kern/nncomposedkernel_1/kern/variance         +ve       True   \n",
      "WeightedSVGP/kern/nncomposedkernel_1/kern/offset           +ve       True   \n",
      "WeightedSVGP/kern/nncomposedkernel_2/kern/variance         +ve       True   \n",
      "WeightedSVGP/kern/nncomposedkernel_2/kern/lengt...         +ve       True   \n",
      "WeightedSVGP/kern/nncomposedkernel_3/kern/variance         +ve       True   \n",
      "WeightedSVGP/kern/nncomposedkernel_3/kern/lengt...         +ve       True   \n",
      "WeightedSVGP/kern/rbf/variance                             +ve       True   \n",
      "WeightedSVGP/kern/rbf/lengthscales                         +ve       True   \n",
      "WeightedSVGP/kern/polynomial/variance                      +ve       True   \n",
      "WeightedSVGP/kern/polynomial/offset                        +ve       True   \n",
      "WeightedSVGP/likelihood/variance                           +ve       True   \n",
      "WeightedSVGP/feature/Z                                  (none)      False   \n",
      "WeightedSVGP/q_mu                                       (none)       True   \n",
      "WeightedSVGP/q_sqrt                                 LoTri->vec       True   \n",
      "\n",
      "                                                            shape  \\\n",
      "WeightedSVGP/kern/nncomposedkernel_1/kern/variance             ()   \n",
      "WeightedSVGP/kern/nncomposedkernel_1/kern/offset               ()   \n",
      "WeightedSVGP/kern/nncomposedkernel_2/kern/variance             ()   \n",
      "WeightedSVGP/kern/nncomposedkernel_2/kern/lengt...           (1,)   \n",
      "WeightedSVGP/kern/nncomposedkernel_3/kern/variance             ()   \n",
      "WeightedSVGP/kern/nncomposedkernel_3/kern/lengt...           (1,)   \n",
      "WeightedSVGP/kern/rbf/variance                                 ()   \n",
      "WeightedSVGP/kern/rbf/lengthscales                           (1,)   \n",
      "WeightedSVGP/kern/polynomial/variance                          ()   \n",
      "WeightedSVGP/kern/polynomial/offset                            ()   \n",
      "WeightedSVGP/likelihood/variance                               ()   \n",
      "WeightedSVGP/feature/Z                                   (600, 6)   \n",
      "WeightedSVGP/q_mu                                        (600, 1)   \n",
      "WeightedSVGP/q_sqrt                                 (1, 600, 600)   \n",
      "\n",
      "                                                    fixed_shape  \\\n",
      "WeightedSVGP/kern/nncomposedkernel_1/kern/variance         True   \n",
      "WeightedSVGP/kern/nncomposedkernel_1/kern/offset           True   \n",
      "WeightedSVGP/kern/nncomposedkernel_2/kern/variance         True   \n",
      "WeightedSVGP/kern/nncomposedkernel_2/kern/lengt...         True   \n",
      "WeightedSVGP/kern/nncomposedkernel_3/kern/variance         True   \n",
      "WeightedSVGP/kern/nncomposedkernel_3/kern/lengt...         True   \n",
      "WeightedSVGP/kern/rbf/variance                             True   \n",
      "WeightedSVGP/kern/rbf/lengthscales                         True   \n",
      "WeightedSVGP/kern/polynomial/variance                      True   \n",
      "WeightedSVGP/kern/polynomial/offset                        True   \n",
      "WeightedSVGP/likelihood/variance                           True   \n",
      "WeightedSVGP/feature/Z                                     True   \n",
      "WeightedSVGP/q_mu                                          True   \n",
      "WeightedSVGP/q_sqrt                                        True   \n",
      "\n",
      "                                                                                                value  \n",
      "WeightedSVGP/kern/nncomposedkernel_1/kern/variance                                 0.9397639628277381  \n",
      "WeightedSVGP/kern/nncomposedkernel_1/kern/offset                                   0.9392136892426752  \n",
      "WeightedSVGP/kern/nncomposedkernel_2/kern/variance                                 0.9395197657087988  \n",
      "WeightedSVGP/kern/nncomposedkernel_2/kern/lengt...                                [94.14010788348479]  \n",
      "WeightedSVGP/kern/nncomposedkernel_3/kern/variance                                 0.9395197657087989  \n",
      "WeightedSVGP/kern/nncomposedkernel_3/kern/lengt...                                 [1.07364317238242]  \n",
      "WeightedSVGP/kern/rbf/variance                                                     0.9395197657087988  \n",
      "WeightedSVGP/kern/rbf/lengthscales                                              [0.11877454872776008]  \n",
      "WeightedSVGP/kern/polynomial/variance                                               0.939429454599527  \n",
      "WeightedSVGP/kern/polynomial/offset                                                0.9396140631864742  \n",
      "WeightedSVGP/likelihood/variance                                                   1.0623941131783272  \n",
      "WeightedSVGP/feature/Z                              [[-1.1603383198776926, -0.8084179562834524, -0...  \n",
      "WeightedSVGP/q_mu                                   [[-0.0577427003484081], [0.04850091802512029],...  \n",
      "WeightedSVGP/q_sqrt                                 [[[0.33845369426936944, 0.0, 0.0, 0.0, 0.0, 0....  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class WeightedDGP(DGP):\n",
    "    def __init__(self,obs_weight, X, Y, Z, kernels, likelihood, \n",
    "                 num_outputs=None,num_data=None,\n",
    "                 mean_function=gp.mean_functions.Zero(),  # the final layer mean function\n",
    "                 **kwargs):\n",
    "        pass\n",
    "\n",
    "class Smoothing(object):\n",
    "    \"\"\"\n",
    "    Class for all types of GP smoothing/conditioned prediction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,datapack, proj_dir):\n",
    "        if isinstance(datapack, str):\n",
    "            datapack = DataPack(filename=datapack)\n",
    "        self.datapack = datapack\n",
    "        self.proj_dir = os.path.abspath(proj_dir)\n",
    "        try:\n",
    "            os.makedirs(self.proj_dir)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def _make_coord_array(t,d):\n",
    "        \"\"\"Static method to pack coordinates\n",
    "        \"\"\"\n",
    "        Nt,Nd = t.shape[0],d.shape[0]\n",
    "        X = np.zeros([Nt,Nd,3],dtype=np.float64)\n",
    "        for j in range(Nt):\n",
    "            for k in range(Nd):\n",
    "                X[j,k,0] = t[j]\n",
    "                X[j,k,1:3] = d[k,:]\n",
    "        X = np.reshape(X,(Nt*Nd,3))\n",
    "        return X\n",
    "    \n",
    "    def _make_coord_array_full(a,t,d,f):\n",
    "        \"\"\"Static method to pack coordinates\n",
    "        \"\"\"\n",
    "        Na,Nt,Nd,Nf = a.shape[0],t.shape[0],d.shape[0],f.shape[0]\n",
    "        X = np.zeros([Na,Nt,Nd,Nf,6],dtype=np.float64)\n",
    "        for i in range(Na):\n",
    "            for j in range(Nt):\n",
    "                for k in range(Nd):\n",
    "                    for l in range(Nf):\n",
    "                        X[i,j,k,l,0:2] = a[i,:]\n",
    "                        X[i,j,k,l,2] = t[j]\n",
    "                        X[i,j,k,l,3:5] = d[k,:]\n",
    "                        X[i,j,k,l,5] = f[l]\n",
    "        X = np.reshape(X,(Na*Nt*Nd*Nf,6))\n",
    "        return X\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _build_sgp_model(self, sess, weights, X, Y, ls_scale, y_scale, \n",
    "                         minibatch_size=500, M=1000,Z=None, feature_trainable=False,ls_init=(200,1.),\n",
    "                         ls_trainable=(True,True), likelihood_var_trainable=True, verbose=False):\n",
    "        \"\"\"\n",
    "        Build svgp model\n",
    "        \"\"\"\n",
    "        N, num_latent = Y.shape\n",
    "        Z = kmeans2(X, M, minit='points')[0] if Z is None else Z\n",
    "        num_data = X.shape[0]\n",
    "        _,_, data = _synced_minibatch(weights, X, Y,minibatch_size=minibatch_size, sess = sess,shuffle=True)\n",
    "        weights,X,Y = data\n",
    "        \n",
    "        with gp.defer_build():\n",
    "            k_time = gp.kernels.RBF(1,active_dims = [0],\n",
    "                                    lengthscales=[0.5 if ls_init[0] is None else ls_init[0]/ls_scale[0]])\n",
    "            k_space = gp.kernels.RBF(2,active_dims = [1,2],\n",
    "                                    lengthscales=[1.0 if ls_init[1] is None else ls_init[1]/ls_scale[1]])\n",
    "            for k,f in zip([k_time,k_space],ls_trainable):\n",
    "                k.lengthscales.set_trainable(f)\n",
    "                if not f:\n",
    "                    logging.warning(\"Setting {} non-trainable\".format(k))\n",
    "            k_time.lengthscales.prior = gp.priors.Gaussian(0.,200./ls_scale[0])\n",
    "            k_space.lengthscales.prior = gp.priors.Gaussian(1./ls_scale[1],1./ls_scale[1])\n",
    "            kern = k_time*k_space\n",
    "            mean = gp.mean_functions.Zero()\n",
    "            m = WeightedSVGP(weights, X, Y, kern, mean_function = mean, \n",
    "                    likelihood=gp.likelihoods.Gaussian(), \n",
    "                    Z=Z, num_latent=num_latent,num_data=num_data,\n",
    "                             minibatch_size=None, whiten=True)\n",
    "            m.likelihood.variance.set_trainable(likelihood_var_trainable)\n",
    "            m.q_sqrt = m.q_sqrt.value * 0.4\n",
    "            m.feature.set_trainable(feature_trainable)\n",
    "            m.compile()\n",
    "        if verbose:\n",
    "            logging.warning(m)\n",
    "        return m\n",
    "    \n",
    "    def _build_sgp_model_full(self, sess, weights, X, Y, ls_scale, y_scale, \n",
    "                         minibatch_size=500, M=1000,Z=None, feature_trainable=False,ls_init=(5.,200,1.),\n",
    "                         ls_trainable=(True, True,True), likelihood_var_trainable=True, verbose=False):\n",
    "        \"\"\"\n",
    "        Build svgp model\n",
    "        \"\"\"\n",
    "        N, num_latent = Y.shape\n",
    "        Z = kmeans2(X, M, minit='points')[0] if Z is None else Z\n",
    "        num_data = X.shape[0]\n",
    "        _,_, data = _synced_minibatch(weights, X, Y,minibatch_size=minibatch_size, sess = sess,shuffle=True)\n",
    "        weights,X,Y = data\n",
    "        \n",
    "        \n",
    "        with gp.defer_build():\n",
    "            k_space = gp.kernels.RBF(2,active_dims = [0,1],\n",
    "                                    lengthscales=[1.0 if ls_init[0] is None else ls_init[0]/ls_scale[0]])\n",
    "            k_time = gp.kernels.RBF(1,active_dims = [2],\n",
    "                                    lengthscales=[0.5 if ls_init[1] is None else ls_init[1]/ls_scale[1]])\n",
    "            k_dir = gp.kernels.RBF(2,active_dims = [3,4],\n",
    "                                    lengthscales=[1.0 if ls_init[2] is None else ls_init[2]/ls_scale[2]])\n",
    "            k_freq = gp.kernels.Polynomial(1,active_dims = [5], degree=1.,variance=1.)\n",
    "            for k,f in zip([k_space,k_time,k_dir],ls_trainable):\n",
    "                k.lengthscales.set_trainable(f)\n",
    "                if not f:\n",
    "                    logging.warning(\"Setting {} non-trainable\".format(k))\n",
    "            k_space.lengthscales.prior = gp.priors.Gaussian(0.,10./ls_scale[0])\n",
    "            k_time.lengthscales.prior = gp.priors.Gaussian(0.,200./ls_scale[1])\n",
    "            k_dir.lengthscales.prior = gp.priors.Gaussian(1./ls_scale[2],1./ls_scale[2])\n",
    "            \n",
    "            # allow length scale to change depending on loc |X - X'|^2/ls^2 -> |X/f(X) - X'/f(X')|^2/ls^2\n",
    "            k_space = NNComposedKernel(k_space, anisotropic_modulation(ndim=2, M=7, scope='f_space_ls'), 'f_space_ls')\n",
    "            k_time = NNComposedKernel(k_time, anisotropic_modulation(ndim=1, M=4, scope='f_time_ls'), 'f_time_ls')\n",
    "            k_variance = NNComposedKernel(\n",
    "                gp.kernels.Polynomial(2,active_dims = [0,1], degree=1.,variance=1.), \n",
    "                anisotropic_modulation(ndim=2, M=7, scope='f_space_var'), 'f_space_var')\n",
    "            \n",
    "            kern = k_variance*k_space*k_time*k_dir*k_freq\n",
    "            mean = gp.mean_functions.Zero()\n",
    "            m = WeightedSVGP(weights, X, Y, kern, mean_function = mean, \n",
    "                    likelihood=gp.likelihoods.Gaussian(), \n",
    "                    Z=Z, num_latent=num_latent,num_data=num_data,\n",
    "                             minibatch_size=None, whiten=True)\n",
    "            m.likelihood.variance.set_trainable(likelihood_var_trainable)\n",
    "            m.q_sqrt = m.q_sqrt.value * 0.4\n",
    "            m.feature.set_trainable(feature_trainable)\n",
    "            m.compile()\n",
    "        if verbose:\n",
    "            logging.warning(m)\n",
    "        return m\n",
    "    \n",
    "    def _build_dgp_model(self, depth, sess, weight, X, Y, ls_scale, y_scale,  \n",
    "                         minibatch_size=500, Z=None,M=100,feature_trainable=False,ls_init=(None,None,None),\n",
    "                         ls_trainable=(True,True,True),likelihood_var_trainable=True, verbose=False):\n",
    "        \"\"\"\n",
    "        Build svgp model\n",
    "        \"\"\"\n",
    "        N, num_latent = Y.shape\n",
    "        Z = kmeans2(X, M, minit='points')[0]\n",
    "        with gp.defer_build():\n",
    "            k_time = gp.kernels.RBF(1,active_dims = [0],\n",
    "                                    lengthscales=[0.3 if ls_init[0] is None else ls_init[0]/ls_scale[0]])\n",
    "            k_space = gp.kernels.RBF(2,active_dims = [1,2],\n",
    "                                    lengthscales=[0.3 if ls_init[1] is None else ls_init[1]/ls_scale[1]])\n",
    "            k_freq = gp.kernels.RBF(1,active_dims = [3],\n",
    "                                    lengthscales=[10. if ls_init[2] is None else ls_init[2]/ls_scale[2]])\n",
    "            for k,f in zip([k_time,k_space,k_freq],ls_trainable):\n",
    "                k.lengthscales.set_trainable(f)\n",
    "                if not f:\n",
    "                    logging.warning(\"Setting {} non-trainable\".format(k))\n",
    "            k_time.lengthscales.prior = gp.priors.Gaussian(0,1./3.)\n",
    "            k_space.lengthscales.prior = gp.priors.Gaussian(1./ls_scale[1],0.5/ls_scale[1])\n",
    "            kern = k_time*k_space*k_freq\n",
    "            \n",
    "            mean = gp.mean_functions.Zero()\n",
    "            kernels = [kern]\n",
    "            for l in range(1,depth):\n",
    "                kernels.append(RBF(4-l, lengthscales=2., variance=2.,ARD=True))\n",
    "                #kernels[-1].lengthscales.prior = gp.priors.Gaussian(0,1./3.)\n",
    "            m = DGP(X, Y, Z, kernels, gp.likelihoods.Gaussian(), \n",
    "                        minibatch_size=minibatch_size,\n",
    "                        num_outputs=num_latent,num_samples=1)\n",
    "\n",
    "            # start things deterministic \n",
    "            for layer in m.layers[:-1]:\n",
    "                layer.q_sqrt = layer.q_sqrt.value * 1e-5 \n",
    "            for layer in m.layers:\n",
    "                layer.feature.Z.set_trainable(feature_trainable)\n",
    "            m.compile()\n",
    "        if verbose:\n",
    "            logging.warning(m)\n",
    "        return m\n",
    "        \n",
    "    def _build_model(self,m_type, sess, weight, X, Y, ls_scale, y_scale, **kwargs):\n",
    "        \"\"\"\n",
    "        Build a GP model depending on m_type\n",
    "        m_type: str, one of 'sgp', 'dgp2', 'dgp3'\n",
    "        **kwargs are passes to the constructor of the model type.\n",
    "        \"\"\"\n",
    "        if m_type == 'sgp':\n",
    "            return self._build_sgp_model(sess, weight, X, Y, ls_scale, y_scale,**kwargs)\n",
    "        elif m_type == 'sgp_full':\n",
    "            return self._build_sgp_model_full(sess, weight, X, Y, ls_scale, y_scale,**kwargs)\n",
    "#         elif m_type == 'dgp2':\n",
    "#             return self._build_dgp_model(2,sess,weight, X, Y, ls_scale, y_scale,**kwargs)\n",
    "#         elif m_type == 'dgp3':\n",
    "#             return self._build_dgp_model(3,sess, weight, X, Y, ls_scale, y_scale,**kwargs)\n",
    "        raise ValueError(\"{} is invalid model type\".format(m_type))\n",
    "\n",
    "    def _solve_interval(self,phase, error, coords, lock, error_sigma_clip=None, m_type='sgp', \n",
    "                        iterations=1000, pargs=None,verbose=False,model_kwargs={}):\n",
    "        \"\"\"\n",
    "        Solve the block of data independently over antennas assuming homogeneity.\n",
    "        phase: array of shape (Na, Nt, Nd, Nf)\n",
    "        errors: array of shape (Na, Nt, Nd, Nf), -1 in an element means to mask\n",
    "        coords: tuple of arrays of shape (Nt,) (Nd,2) (Nf,)\n",
    "        lock: a mutable lock or None\n",
    "        m_type: str the model type to use, see build_model\n",
    "        pargs: str or None, thing to print on start of block\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if pargs is not None:\n",
    "                logging.warning(\"{}\".format(pargs))\n",
    "            \n",
    "            Na,Nt,Nd,Nf = phase.shape\n",
    "            \n",
    "            y = phase.transpose((1,2,0,3)).reshape((Nt*Nd,Na*Nf))#Nt*Nd,Na*Nf\n",
    "            sigma_y = error.transpose((1,2,0,3)).reshape((Nt*Nd,Na*Nf))#Nt*Nd,Na*Nf\n",
    "            mask = sigma_y < 0.#Nt*Nd,Na*Nf\n",
    "            \n",
    "            y_mean = (y*np.bitwise_not(mask)).sum(axis=0) / (np.bitwise_not(mask).sum(axis=0))#Na*Nf\n",
    "            y_scale = np.sqrt((y**2*np.bitwise_not(mask)).sum(axis=0) \\\n",
    "                              / (np.bitwise_not(mask).sum(axis=0)) - y_mean**2) + 1e-6#Na*Nf\n",
    "            y = (y - y_mean)/y_scale#Nt*Nd,Na*Nf\n",
    "            var_y = (sigma_y/y_scale)**2#Nt*Nd,Na*Nf\n",
    "            \n",
    "            if error_sigma_clip is not None:\n",
    "                log_var_y = np.log(var_y)#Nt*Nd,Na*Nf\n",
    "                log_var_y[mask] = np.nan\n",
    "                E_log_var_y = np.nanmean(log_var_y,axis=0)#Na*Nf\n",
    "                std_log_var_y = np.nanstd(log_var_y,axis=0)#Na*Nf\n",
    "                clip_mask = (log_var_y - E_log_var_y) > error_sigma_clip*std_log_var_y#Nt*Nd,Na*Nf\n",
    "                ignore_mask = np.bitwise_or(mask,clip_mask)#Nt*Nd,Na*Nf\n",
    "            else:\n",
    "                ignore_mask = mask\n",
    "            keep_mask = np.bitwise_not(ignore_mask)#Nt*Nd,Na*Nf\n",
    "            weight = 1./(var_y+1e-6)#Nt*Nd,Na*Nf\n",
    "            weight_norm = np.stack([np.percentile(weight[keep_mask[:,i],i],50) for i in range(Na*Nf)],axis=-1)\n",
    "            weight /= weight_norm + 1e-6\n",
    "#             plt.hist(weight.flatten(),bins=20)\n",
    "#             plt.show()\n",
    "            weight = np.ones(y.shape)\n",
    "            weight[ignore_mask] = 0.\n",
    "             \n",
    "            t,d = coords\n",
    "            t_scale = t.std() + 1e-6\n",
    "            d_scale = np.sqrt((d.std(axis=0)**2).mean()) + 1e-6\n",
    "            ls_scale = (t_scale,d_scale)\n",
    "            t = (t - t.mean()) / t_scale\n",
    "            d = (d - d.mean(axis=0)) / d_scale\n",
    "            X = Smoothing._make_coord_array(t,d)#Nt*Nd,3\n",
    "            ###\n",
    "            # set Z explicitly to spacing of ::3 in time\n",
    "            model_kwargs['Z'] = Smoothing._make_coord_array(t[::3],d)\n",
    "            \n",
    "            with tf.Session(graph=tf.Graph()) as sess:\n",
    "                lock.acquire() if lock is not None else None\n",
    "                try:\n",
    "                    model = self._build_model(m_type, sess, weight, X, y, ls_scale, y_scale, **model_kwargs)\n",
    "                finally:\n",
    "                    lock.release() if lock is not None else None\n",
    "                logging.warning(\"Initial log-likelihood {}\".format(model.compute_log_likelihood()))\n",
    "                opt = gp.train.AdamOptimizer(1e-2)\n",
    "                opt.minimize(model, maxiter=iterations)\n",
    "                logging.warning(\"Final log-likelihood {}\".format(model.compute_log_likelihood()))\n",
    "                # smooth\n",
    "                kern_lengthscales = (\n",
    "                        model.kern.rbf_1.lengthscales.value[0]*ls_scale[0],\n",
    "                        model.kern.rbf_2.lengthscales.value[0]*ls_scale[1]\n",
    "                        )\n",
    "                kern_variance = model.kern.rbf_1.variance.value*model.kern.rbf_2.variance.value*y_scale**2\n",
    "                if verbose:\n",
    "                    logging.warning(model)\n",
    "                    logging.warning(kern_lengthscales)\n",
    "                    logging.warning(kern_variance)\n",
    "                predict_minibatch = 1000\n",
    "                for start in range(0,X.shape[0],predict_minibatch):\n",
    "                    stop = min(start+predict_minibatch,X.shape[0])\n",
    "                    Xs = X[start:stop,:]\n",
    "                    ystar,varstar = model.predict_y(Xs)#batch,Na\n",
    "                    ystar = ystar * y_scale + y_mean\n",
    "                    varstar = varstar * y_scale**2\n",
    "                    \n",
    "                    y[start:stop,:] = ystar\n",
    "                    var_y[start:stop,:] = varstar\n",
    "            phase = y.reshape([Nt,Nd,Na,Nf]).transpose((2,0,1,3))\n",
    "            variance = var_y.reshape([Nt,Nd,Na,Nf]).transpose((2,0,1,3))\n",
    "            return phase,variance,kern_lengthscales, kern_variance\n",
    "        except Exception as e:\n",
    "            logging.warning(e)\n",
    "            \n",
    "    def _solve_interval_full(self,model_file, phase, error, coords, lock, load_model = None, error_sigma_clip=None, m_type='sgp_full', \n",
    "                        iterations=1000, pargs=None,verbose=False,model_kwargs={}):\n",
    "        \"\"\"\n",
    "        Solve the block of data independently over antennas assuming homogeneity.\n",
    "        phase: array of shape (Na, Nt, Nd, Nf)\n",
    "        errors: array of shape (Na, Nt, Nd, Nf), -1 in an element means to mask\n",
    "        coords: tuple of arrays of shape (Nt,) (Nd,2) (Nf,)\n",
    "        lock: a mutable lock or None\n",
    "        m_type: str the model type to use, see build_model\n",
    "        pargs: str or None, thing to print on start of block\n",
    "        \"\"\"\n",
    "        assert \"_full\" in m_type\n",
    "#         try:\n",
    "        if pargs is not None:\n",
    "            logging.warning(\"{}\".format(pargs))\n",
    "\n",
    "        Na,Nt,Nd,Nf = phase.shape\n",
    "        freqs = coords[3]\n",
    "\n",
    "        y = (phase*(freqs/-8.4480e9)).reshape((Na*Nt*Nd*Nf,1)) #Na*Nt*Nd*Nf, 1\n",
    "        sigma_y = (error*(freqs/8.4480e9)).reshape((Na*Nt*Nd*Nf,1)) #Na*Nt*Nd*Nf, 1\n",
    "        mask = sigma_y < 0. #Na*Nt*Nd*Nf, 1\n",
    "\n",
    "        y_mean = np.average(y,weights = np.bitwise_not(mask))# scalar\n",
    "        y_scale = np.sqrt(np.average(y**2,weights = np.bitwise_not(mask))\\\n",
    "                          - y_mean**2) + 1e-6 #scalar\n",
    "        y = (y - y_mean)/y_scale#Na*Nt*Nd*Nf, 1\n",
    "        var_y = (sigma_y/y_scale)**2#Na*Nt*Nd*Nf, 1\n",
    "\n",
    "        if error_sigma_clip is not None:\n",
    "            log_var_y = np.log(var_y)#Na*Nt*Nd*Nf, 1\n",
    "            log_var_y[mask] = np.nan\n",
    "            E_log_var_y = np.nanmean(log_var_y,axis=0)#1\n",
    "            std_log_var_y = np.nanstd(log_var_y,axis=0)#1\n",
    "            clip_mask = (log_var_y - E_log_var_y) > error_sigma_clip*std_log_var_y##Na*Nt*Nd*Nf, 1\n",
    "            ignore_mask = np.bitwise_or(mask,clip_mask)#Na*Nt*Nd*Nf, 1\n",
    "        else:\n",
    "            ignore_mask = mask\n",
    "        keep_mask = np.bitwise_not(ignore_mask)#Na*Nt*Nd*Nf, 1\n",
    "#             weight = 1./(var_y+1e-6)#Na*Nt*Nd*Nf, 1\n",
    "#             weight_norm = np.stack([np.percentile(weight[keep_mask[:,i],i],50) for i in range(Na*Nf)],axis=-1)\n",
    "#             weight /= weight_norm + 1e-6\n",
    "# #             plt.hist(weight.flatten(),bins=20)\n",
    "# #             plt.show()\n",
    "        weight = np.ones(y.shape)\n",
    "        weight[ignore_mask] = 0.\n",
    "\n",
    "        x,t,d,f = coords\n",
    "        x_scale = np.sqrt((x.std(axis=0)**2).mean()) + 1e-6\n",
    "        t_scale = t.std() + 1e-6\n",
    "        d_scale = np.sqrt((d.std(axis=0)**2).mean()) + 1e-6\n",
    "        f_scale = f.std() + 1e-6\n",
    "        ls_scale = (x_scale,t_scale,d_scale,f_scale)\n",
    "        x = (x - x.mean(axis=0)) / x_scale\n",
    "        t = (t - t.mean()) / t_scale\n",
    "        d = (d - d.mean(axis=0)) / d_scale\n",
    "        f = (f - f.mean()) / f_scale\n",
    "        X = Smoothing._make_coord_array_full(x,t,d,f)#Na*Nt*Nd*Nf,6\n",
    "        ###\n",
    "        # set Z explicitly to spacing of ::3 in time\n",
    "        model_kwargs['Z'] = None#Smoothing._make_coord_array(t[::3],d)\n",
    "\n",
    "\n",
    "        with tf.Session(graph=tf.Graph()) as sess:\n",
    "            lock.acquire() if lock is not None else None\n",
    "            try:\n",
    "                model = self._build_model(m_type, sess, weight, X, y, ls_scale, y_scale, **model_kwargs)\n",
    "            finally:\n",
    "                lock.release() if lock is not None else None\n",
    "            if load_model is not None:\n",
    "                try:\n",
    "                    all_vars = model.trainable_tensors\n",
    "                    rename(load_model,prefix='WeightedSVGP',index=model.index)\n",
    "                    all_vars = get_only_vars_in_model(all_vars,load_model)\n",
    "                    saver = tf.train.Saver(all_vars)\n",
    "                    saver.restore(sess, load_model)\n",
    "                    model.compile()\n",
    "                    logging.warning(\"Loaded model {}\".format(load_model))\n",
    "                    logging.warning(model)\n",
    "                except Exception as e:\n",
    "                    logging.warning(e)\n",
    "                    logging.warning(\"Unable to load {}\".format(load_model))\n",
    "            \n",
    "\n",
    "            logging.warning(\"Initial log-likelihood {}\".format(model.compute_log_likelihood()))\n",
    "            opt = gp.train.AdamOptimizer(1e-3)\n",
    "            opt.minimize(model, maxiter=iterations)\n",
    "            logging.warning(\"Final log-likelihood {}\".format(model.compute_log_likelihood()))\n",
    "            f_vars = model.all_f_vars\n",
    "            for var,val in zip(f_vars,sess.run(f_vars)):\n",
    "                logging.warning(\"{} {}\".format(var.name,val))\n",
    "            all_vars = model.trainable_tensors\n",
    "            saver = tf.train.Saver(all_vars)\n",
    "            save_path = saver.save(sess, model_file)\n",
    "            logging.warning(\"Saved model to {}\".format(save_path))\n",
    "\n",
    "            if verbose:\n",
    "                logging.warning(model)\n",
    "            predict_minibatch = 1000\n",
    "            for start in range(0,X.shape[0],predict_minibatch):\n",
    "                stop = min(start+predict_minibatch,X.shape[0])\n",
    "                Xs = X[start:stop,:]\n",
    "                ystar,varstar = model.predict_y(Xs)#minibatch,1\n",
    "                ystar = ystar * y_scale + y_mean\n",
    "                varstar = varstar * y_scale**2#minibaatch,1\n",
    "\n",
    "                y[start:stop,:] = ystar\n",
    "                var_y[start:stop,:] = varstar\n",
    "        phase = y.reshape([Na,Nt,Nd,Nf])*(-8.4480e9/freqs)**2\n",
    "        variance = var_y.reshape([Na,Nt,Nd,Nf])*(-8.4480e9/freqs)**2\n",
    "        return phase,variance\n",
    "#         except Exception as e:\n",
    "#             logging.warning(e)\n",
    "#             logging.warning(\"Failed interval solve {}\".format(model_file))\n",
    "            \n",
    "    def solve_and_apply_ensemble(self, save_datapack, ant_idx, time_idx, dir_idx, freq_idx, iterations,\n",
    "                                 interval, shift, init_solutions = None, num_threads=1,verbose=False,\n",
    "                                 model_kwargs = {}):\n",
    "        \"\"\"\n",
    "        Solve the problem using model_kwargs and then take an ensemble average over interval\n",
    "        and shift.\n",
    "        \"\"\"\n",
    "        if init_solutions is not None:\n",
    "            data = np.load(init_solutions)\n",
    "\n",
    "            kern_ls = data['kern_ls']\n",
    "            kern_var = data['kern_var']\n",
    "            kern_times = data['time']\n",
    "            kern_antenna_labels = data['antenna']\n",
    "\n",
    "        \n",
    "        datapack = self.datapack\n",
    "        directions, patch_names = datapack.get_directions(dir_idx)\n",
    "        times,timestamps = datapack.get_times(time_idx)\n",
    "        antennas,antenna_labels = datapack.get_antennas(ant_idx)\n",
    "        freqs = datapack.get_freqs(freq_idx)\n",
    "\n",
    "        if ant_idx is -1:\n",
    "            ant_idx = range(len(antennas))\n",
    "        if time_idx is -1:\n",
    "            time_idx = range(len(times))\n",
    "        if freq_idx is -1:\n",
    "            freq_idx = range(len(freqs))\n",
    "        if dir_idx is -1:\n",
    "            dir_idx = range(len(directions))\n",
    "        \n",
    "\n",
    "        phase = datapack.get_phase(ant_idx,time_idx,dir_idx,freq_idx)\n",
    "        Na,Nt,Nd,Nf = phase.shape\n",
    "        logging.warning(\"Working on shapes {}\".format(phase.shape))\n",
    "        \n",
    "        if interval is None:\n",
    "            interval = Nt\n",
    "\n",
    "        assert interval <= Nt\n",
    "\n",
    "        variance = datapack.get_variance(ant_idx,time_idx,dir_idx,freq_idx)\n",
    "        error = np.sqrt(variance)\n",
    "        data_mask = variance < 0\n",
    "        error[data_mask] = -1\n",
    "        logging.warning(\"Total masked phases: {}\".format(np.sum(data_mask)))\n",
    "\n",
    "        enu = ENU(obstime=times[0],location = self.datapack.radio_array.get_center())\n",
    "        \n",
    "        ant_enu = antennas.transform_to(enu)\n",
    "        x = np.array([ant_enu.east.to(au.km).value, ant_enu.north.to(au.km).value]).T\n",
    "        t = times.mjd*86400.#mjs\n",
    "        d = np.array([directions.ra.deg, directions.dec.deg]).T\n",
    "        f = freqs\n",
    "        \n",
    "        lock = Lock()\n",
    "        with futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "            jobs = []\n",
    "\n",
    "            for j,aj in enumerate(time_idx[::shift]):\n",
    "                start = j*shift\n",
    "                stop = min(start+interval,Nt)\n",
    "                time_slice = slice(start,stop,1)\n",
    "                \n",
    "#                 if init_solutions is not None:\n",
    "#                     ###\n",
    "#                     # interpolate kern_params with this interval/shift\n",
    "#                     mean_time = np.mean(times.gps[time_slice])\n",
    "\n",
    "#                     model_kwargs['ls_init'] = (\n",
    "#                             np.interp(mean_time, kern_times, kern_ls[ant_idx,:,1].mean(0)),\n",
    "#                             np.interp(mean_time, kern_times, kern_ls[ant_idx,:,0].mean(0))\n",
    "#                             )\n",
    "                    #logging.warning(model_kwargs['ls_init'])\n",
    "                    \n",
    "                # initial ls_scale (if they exist)\n",
    "\n",
    "# (phase, error, coords, lock, error_sigma_clip=4., m_type='sgp', \n",
    "#                         iterations=1000, pargs=None,verbose=False,model_kwargs={}):\n",
    "                model_file = os.path.join(self.proj_dir,\"model_{}_{}\".format(start,stop))\n",
    "\n",
    "                jobs.append(executor.submit(\n",
    "                    self._solve_interval_full,\n",
    "                    model_file,\n",
    "                    phase[:,time_slice,:,:],\n",
    "                    error[:,time_slice,:,:],\n",
    "                    (x, t[time_slice],d,f),\n",
    "                    lock,\n",
    "                    load_model = model_file,\n",
    "                    error_sigma_clip = None,\n",
    "                    m_type='sgp_full',\n",
    "                    iterations=iterations,\n",
    "                    pargs=\"Working on time chunk ({}) {} to ({}) {}\".format(\n",
    "                        start,timestamps[start],stop-1,timestamps[stop-1]),\n",
    "                    verbose=verbose,\n",
    "                    model_kwargs = model_kwargs\n",
    "                    )\n",
    "                           )\n",
    "            results = futures.wait(jobs)\n",
    "            if verbose:\n",
    "                logging.warning(results)\n",
    "            results = [j.result() for j in jobs]\n",
    "\n",
    "            phase_mean = np.zeros(phase.shape)\n",
    "            phase_weights = np.zeros(phase.shape)\n",
    "            variance_mean = np.zeros(variance.shape)\n",
    "            res_idx = 0\n",
    "            for j,aj in enumerate(time_idx[::shift]):\n",
    "                start = j*interval\n",
    "                stop = min((j+1)*interval,Nt)\n",
    "                time_slice = slice(start,stop,1)\n",
    "                res = results[res_idx]\n",
    "                p,v = res\n",
    "                phase_mean[:,time_slice,:,:] += p/(v+1e-6)\n",
    "                variance_mean[:,time_slice,:,:] += 1.\n",
    "                phase_weights[:,time_slice,:,:] += 1./(v+1e-6)\n",
    "                res_idx += 1\n",
    "            phase_mean /= (phase_weights+1e-6)\n",
    "            variance_mean /= (phase_weights+1e-6)\n",
    "            datapack.set_phase(phase_mean, ant_idx=ant_idx,time_idx=time_idx,dir_idx=dir_idx,freq_idx=freq_idx)\n",
    "            datapack.set_variance(variance_mean, ant_idx=ant_idx,time_idx=time_idx,dir_idx=dir_idx,freq_idx=freq_idx)\n",
    "            datapack.save(save_datapack)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    import os\n",
    "\n",
    "    if len(sys.argv) == 2:\n",
    "        starting_datapack = sys.argv[1]\n",
    "    else:\n",
    "        starting_datapack = \"../../data/rvw_datapack_full_phase_dec27_unwrap.hdf5\"\n",
    "    smoothing = Smoothing(starting_datapack,'projects')\n",
    "    model_kwargs = {'minibatch_size':500, 'M':1000,'feature_trainable':False,'ls_init':(5.,70,0.3),\n",
    "                         'ls_trainable':(True,True,True), 'verbose':False,'likelihood_var_trainable':True}\n",
    "    smoothing.solve_and_apply_ensemble(starting_datapack.replace('.hdf5','_smoothed_ensemble.hdf5'),\n",
    "                                       -1, -1, -1, -1, iterations=1000,\n",
    "                                 interval = None, shift = 6, init_solutions='../../bayes/gp_params_fixed_scales.npz',\n",
    "                                       num_threads=1,verbose=True,model_kwargs = model_kwargs)\n",
    "\n",
    "# #     smoothing.solve_time_intervals(\"gp_params.npz\",range(1,62),-1,-1,range(0,20),32,32,num_threads=16,verbose=True)\n",
    "#    refined_params = smoothing.refine_statistics_timeonly('gp_params.npz')\n",
    "#    print(refined_params.shape)\n",
    "#    smoothing.solve_time_intervals(\"gp_params_fixed_scales.npz\",range(1,62),-1,-1,range(0,20),32,32,num_threads=16,verbose=True,refined_params=refined_params)\n",
    "#    plt.ion()\n",
    "#    smoothing.refine_statistics_timeonly('gp_params.npz')\n",
    "#    smoothing.refine_statistics('gp_params.npz')\n",
    "#    smoothing.refine_statistics_timeonly('gp_params_fixed_scales.npz')\n",
    "#    smoothing.refine_statistics('gp_params_fixed_scales.npz')\n",
    "#    plt.ioff()\n",
    "#     smoothing.apply_solutions(starting_datapack.replace('.hdf5','_refined_smoothed.hdf5'), \n",
    "#             \"gp_params_fixed_scales.npz\",range(1,62), -1, -1, range(0,20), 32, 32, num_threads=1,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May 4: \n",
    "    Pick-up Truck (Early as possible)\n",
    "    Drive to Antsirabe (170km 4h)\n",
    "    City/Lakes/Fill Gas/Fill travel food\n",
    "May 5:\n",
    "    Drive to Ranomafana National Park (230km 5h)\n",
    "May 6: \n",
    "    Hike Ranomafana\n",
    "May 7: \n",
    "    Ranomafana to Isalo National Park (350km 6h)\n",
    "May 8: \n",
    "    Hike Isalo\n",
    "May 9: \n",
    "    Isalo to Antsirabe (530km 10h)\n",
    "May 10: \n",
    "    Antsirabe to Mahambo (590km 12h)\n",
    "    -- OR --\n",
    "    Antsirabe to Toamasina (500km 10h)\n",
    "May 11: \n",
    "    Mahambo to Ile Sainte Marie (Boat)\n",
    "    -- OR --\n",
    "    Toamasina to Mahambo (90km 3h)\n",
    "    Mahambo to Ile Sainte Marie (Boat)\n",
    "May 12: \n",
    "    Ile Sainte Marie / Ile Aux Nattes\n",
    "May 13: \n",
    "    Ile Sainte Marie / Ile Aux Nattes\n",
    "May 14: \n",
    "    Ile Sainte Marie / Ile Aux Nattes\n",
    "May 15: \n",
    "    Ile Sainte Marie to Mahambo (early Boat)\n",
    "    Mahambo to Antanarivo (450km 10h)\n",
    "May 16: \n",
    "    Antananrivo to Andasibe (150km 4-6h)\n",
    "May 17: \n",
    "    Hike Andasibe\n",
    "May 18: \n",
    "    Andasibe to Antanarivo (150km 4-6h)\n",
    "May 19: \n",
    "    2 am flight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
