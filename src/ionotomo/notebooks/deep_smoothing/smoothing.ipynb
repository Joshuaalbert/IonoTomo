{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/anaconda3/envs/kerastf/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/josh/anaconda3/envs/kerastf/lib/python3.6/site-packages/cmocean/tools.py:76: MatplotlibDeprecationWarning: The is_string_like function was deprecated in version 2.1.\n",
      "  if not mpl.cbook.is_string_like(rgbin[0]):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(message)s')\n",
    "import pylab as plt\n",
    "import cmocean\n",
    "from scipy.spatial import cKDTree\n",
    "from ionotomo.tomography.pipeline import Pipeline\n",
    "from ionotomo.settings import TFSettings\n",
    "from timeit import default_timer\n",
    "from ionotomo import *\n",
    "import astropy.coordinates as ac\n",
    "import astropy.units as au\n",
    "import gpflow as gp\n",
    "import sys\n",
    "import h5py\n",
    "import threading\n",
    "from timeit import default_timer\n",
    "#%matplotlib notebook\n",
    "from concurrent import futures\n",
    "from functools import partial\n",
    "from threading import Lock\n",
    "import astropy.units as au\n",
    "import astropy.time as at\n",
    "from collections import deque\n",
    "from doubly_stochastic_dgp.dgp import DGP\n",
    "\n",
    "from ionotomo.bayes.gpflow_contrib import GPR_v2,Gaussian_v2\n",
    "from scipy.cluster.vq import kmeans2\n",
    "\n",
    "\n",
    "def _synced_minibatch(*X,minibatch_size=100,seed=0, sess=None, shuffle = True):\n",
    "    init_placeholders = tuple([tf.placeholder(gp.settings.tf_float,shape=x.shape) for x in X])\n",
    "    data = tf.data.Dataset.from_tensor_slices(init_placeholders)\n",
    "    data = data.repeat()\n",
    "    if shuffle:\n",
    "        data = data.shuffle(buffer_size=X[0].shape[0], seed=seed)\n",
    "    data = data.batch(batch_size=tf.constant(minibatch_size,dtype=tf.int64))\n",
    "    iterator_tensor = data.make_initializable_iterator()\n",
    "    if sess is not None:\n",
    "        sess.run(iterator_tensor.initializer, feed_dict={p:x for p,x in zip(init_placeholders,X)})\n",
    "    return init_placeholders, iterator_tensor.initializer, iterator_tensor.get_next()\n",
    "\n",
    "class WeightedSVGP(gp.models.svgp.SVGP):\n",
    "    def __init__(self, obs_weight, X, Y, kern, likelihood, feat=None,\n",
    "                 mean_function=None,\n",
    "                 num_latent=None,\n",
    "                 q_diag=False,\n",
    "                 whiten=True,\n",
    "                 minibatch_size=None,\n",
    "                 Z=None,\n",
    "                 num_data=None,\n",
    "                 **kwargs):\n",
    "        super(WeightedSVGP,self).__init__(X, Y, kern, likelihood, feat=feat,\n",
    "                 mean_function=mean_function,\n",
    "                 num_latent=num_latent,\n",
    "                 q_diag=q_diag,\n",
    "                 whiten=whiten,\n",
    "                 minibatch_size=None,\n",
    "                 Z=Z,\n",
    "                 num_data=num_data,\n",
    "                 **kwargs)\n",
    "        self.obs_weight = gp.DataHolder(obs_weight) \\\n",
    "            if minibatch_size is None else gp.Minibatch(obs_weight,batch_size=minibatch_size, seed=0)\n",
    "\n",
    "        \n",
    "    @gp.params_as_tensors\n",
    "    def _build_likelihood(self):\n",
    "        \"\"\"\n",
    "        This gives a variational bound on the model likelihood.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get prior KL.\n",
    "        KL = self.build_prior_KL()\n",
    "\n",
    "        # Get conditionals\n",
    "        fmean, fvar = self._build_predict(self.X, full_cov=False)\n",
    "\n",
    "        # Get variational expectations.\n",
    "        var_exp = self.likelihood.variational_expectations(fmean, fvar, self.Y) * self.obs_weight\n",
    "\n",
    "        # re-scale for minibatch size\n",
    "        scale = tf.cast(self.num_data, gp.settings.float_type) / tf.cast(tf.shape(self.X)[0], gp.settings.float_type)\n",
    "\n",
    "        return tf.reduce_sum(var_exp) * scale - KL\n",
    "\n",
    "class WeightedDGP(DGP):\n",
    "    def __init__(self,obs_weight, X, Y, Z, kernels, likelihood, \n",
    "                 num_outputs=None,num_data=None,\n",
    "                 mean_function=gp.mean_functions.Zero(),  # the final layer mean function\n",
    "                 **kwargs):\n",
    "        pass\n",
    "\n",
    "class Smoothing(object):\n",
    "    \"\"\"\n",
    "    Class for all types of GP smoothing/conditioned prediction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,datapack):\n",
    "        if isinstance(datapack, str):\n",
    "            datapack = DataPack(filename=datapack)\n",
    "        self.datapack = datapack\n",
    "\n",
    "    def _make_coord_array(t,d):\n",
    "        \"\"\"Static method to pack coordinates\n",
    "        \"\"\"\n",
    "        Nt,Nd = t.shape[0],d.shape[0]\n",
    "        X = np.zeros([Nt,Nd,3],dtype=np.float64)\n",
    "        for j in range(Nt):\n",
    "            for k in range(Nd):\n",
    "                X[j,k,0] = t[j]\n",
    "                X[j,k,1:3] = d[k,:]\n",
    "        X = np.reshape(X,(Nt*Nd,3))\n",
    "        return X\n",
    "    \n",
    "    def _build_sgp_model(self, sess, weights, X, Y, ls_scale, y_scale, \n",
    "                         minibatch_size=500, M=1000,Z=None, feature_trainable=False,ls_init=(200,1.),\n",
    "                         ls_trainable=(True,True), likelihood_var_trainable=True, verbose=False):\n",
    "        \"\"\"\n",
    "        Build svgp model\n",
    "        \"\"\"\n",
    "        N, num_latent = Y.shape\n",
    "        Z = kmeans2(X, M, minit='points')[0] if Z is None else Z\n",
    "        num_data = X.shape[0]\n",
    "        _,_, data = _synced_minibatch(weights, X, Y,minibatch_size=minibatch_size, sess = sess,shuffle=True)\n",
    "        weights,X,Y = data\n",
    "        \n",
    "        with gp.defer_build():\n",
    "            k_time = gp.kernels.RBF(1,active_dims = [0],\n",
    "                                    lengthscales=[0.5 if ls_init[0] is None else ls_init[0]/ls_scale[0]])\n",
    "            k_space = gp.kernels.RBF(2,active_dims = [1,2],\n",
    "                                    lengthscales=[1.0 if ls_init[1] is None else ls_init[1]/ls_scale[1]])\n",
    "            for k,f in zip([k_time,k_space],ls_trainable):\n",
    "                k.lengthscales.set_trainable(f)\n",
    "                if not f:\n",
    "                    logging.warning(\"Setting {} non-trainable\".format(k))\n",
    "            k_time.lengthscales.prior = gp.priors.Gaussian(0.,200./ls_scale[0])\n",
    "            k_space.lengthscales.prior = gp.priors.Gaussian(1./ls_scale[1],1./ls_scale[1])\n",
    "            kern = k_time*k_space\n",
    "            mean = gp.mean_functions.Zero()\n",
    "            m = WeightedSVGP(weights, X, Y, kern, mean_function = mean, \n",
    "                    likelihood=gp.likelihoods.Gaussian(), \n",
    "                    Z=Z, num_latent=num_latent,num_data=num_data,\n",
    "                             minibatch_size=None, whiten=True)\n",
    "            m.likelihood.variance.set_trainable(likelihood_var_trainable)\n",
    "            m.q_sqrt = m.q_sqrt.value * 1e-5\n",
    "            m.feature.set_trainable(feature_trainable)\n",
    "            m.compile()\n",
    "        if verbose:\n",
    "            logging.warning(m)\n",
    "        return m\n",
    "    \n",
    "    def _build_dgp_model(self, depth, sess, weight, X, Y, ls_scale, y_scale,  \n",
    "                         minibatch_size=500, Z=None,M=100,feature_trainable=False,ls_init=(None,None,None),\n",
    "                         ls_trainable=(True,True,True),likelihood_var_trainable=True, verbose=False):\n",
    "        \"\"\"\n",
    "        Build svgp model\n",
    "        \"\"\"\n",
    "        N, num_latent = Y.shape\n",
    "        Z = kmeans2(X, M, minit='points')[0]\n",
    "        with gp.defer_build():\n",
    "            k_time = gp.kernels.RBF(1,active_dims = [0],\n",
    "                                    lengthscales=[0.3 if ls_init[0] is None else ls_init[0]/ls_scale[0]])\n",
    "            k_space = gp.kernels.RBF(2,active_dims = [1,2],\n",
    "                                    lengthscales=[0.3 if ls_init[1] is None else ls_init[1]/ls_scale[1]])\n",
    "            k_freq = gp.kernels.RBF(1,active_dims = [3],\n",
    "                                    lengthscales=[10. if ls_init[2] is None else ls_init[2]/ls_scale[2]])\n",
    "            for k,f in zip([k_time,k_space,k_freq],ls_trainable):\n",
    "                k.lengthscales.set_trainable(f)\n",
    "                if not f:\n",
    "                    logging.warning(\"Setting {} non-trainable\".format(k))\n",
    "            k_time.lengthscales.prior = gp.priors.Gaussian(0,1./3.)\n",
    "            k_space.lengthscales.prior = gp.priors.Gaussian(1./ls_scale[1],0.5/ls_scale[1])\n",
    "            kern = k_time*k_space*k_freq\n",
    "            \n",
    "            mean = gp.mean_functions.Zero()\n",
    "            kernels = [kern]\n",
    "            for l in range(1,depth):\n",
    "                kernels.append(RBF(4-l, lengthscales=2., variance=2.,ARD=True))\n",
    "                #kernels[-1].lengthscales.prior = gp.priors.Gaussian(0,1./3.)\n",
    "            m = DGP(X, Y, Z, kernels, gp.likelihoods.Gaussian(), \n",
    "                        minibatch_size=minibatch_size,\n",
    "                        num_outputs=num_latent,num_samples=1)\n",
    "\n",
    "            # start things deterministic \n",
    "            for layer in m.layers[:-1]:\n",
    "                layer.q_sqrt = layer.q_sqrt.value * 1e-5 \n",
    "            for layer in m.layers:\n",
    "                layer.feature.Z.set_trainable(feature_trainable)\n",
    "            m.compile()\n",
    "        if verbose:\n",
    "            logging.warning(m)\n",
    "        return m\n",
    "        \n",
    "    def _build_model(self,m_type, sess, weight, X, Y, ls_scale, y_scale, **kwargs):\n",
    "        \"\"\"\n",
    "        Build a GP model depending on m_type\n",
    "        m_type: str, one of 'sgp', 'dgp2', 'dgp3'\n",
    "        **kwargs are passes to the constructor of the model type.\n",
    "        \"\"\"\n",
    "        if m_type == 'sgp':\n",
    "            return self._build_sgp_model(sess, weight, X, Y, ls_scale, y_scale,**kwargs)\n",
    "        elif m_type == 'dgp2':\n",
    "            return self._build_dgp_model(2,sess,weight, X, Y, ls_scale, y_scale,**kwargs)\n",
    "        elif m_type == 'dgp3':\n",
    "            return self._build_dgp_model(3,sess, weight, X, Y, ls_scale, y_scale,**kwargs)\n",
    "        raise ValueError(\"{} is invalid model type\".format(m_type))\n",
    "\n",
    "    def _solve_interval(self,phase, error, coords, lock, error_sigma_clip=None, m_type='sgp', \n",
    "                        iterations=1000, pargs=None,verbose=False,model_kwargs={}):\n",
    "        \"\"\"\n",
    "        Solve the block of data independently over antennas assuming homogeneity.\n",
    "        phase: array of shape (Na, Nt, Nd, Nf)\n",
    "        errors: array of shape (Na, Nt, Nd, Nf), -1 in an element means to mask\n",
    "        coords: tuple of arrays of shape (Nt,) (Nd,2) (Nf,)\n",
    "        lock: a mutable lock or None\n",
    "        m_type: str the model type to use, see build_model\n",
    "        pargs: str or None, thing to print on start of block\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if pargs is not None:\n",
    "                logging.warning(\"{}\".format(pargs))\n",
    "            \n",
    "            Na,Nt,Nd,Nf = phase.shape\n",
    "            \n",
    "            y = phase.transpose((1,2,0,3)).reshape((Nt*Nd,Na*Nf))#Nt*Nd,Na*Nf\n",
    "            sigma_y = error.transpose((1,2,0,3)).reshape((Nt*Nd,Na*Nf))#Nt*Nd,Na*Nf\n",
    "            mask = sigma_y < 0.#Nt*Nd,Na*Nf\n",
    "            \n",
    "            y_mean = (y*np.bitwise_not(mask)).sum(axis=0) / (np.bitwise_not(mask).sum(axis=0))#Na*Nf\n",
    "            y_scale = np.sqrt((y**2*np.bitwise_not(mask)).sum(axis=0) \\\n",
    "                              / (np.bitwise_not(mask).sum(axis=0)) - y_mean**2) + 1e-6#Na*Nf\n",
    "            y = (y - y_mean)/y_scale#Nt*Nd,Na*Nf\n",
    "            var_y = (sigma_y/y_scale)**2#Nt*Nd,Na*Nf\n",
    "            \n",
    "            if error_sigma_clip is not None:\n",
    "                log_var_y = np.log(var_y)#Nt*Nd,Na*Nf\n",
    "                log_var_y[mask] = np.nan\n",
    "                E_log_var_y = np.nanmean(log_var_y,axis=0)#Na*Nf\n",
    "                std_log_var_y = np.nanstd(log_var_y,axis=0)#Na*Nf\n",
    "                clip_mask = (log_var_y - E_log_var_y) > error_sigma_clip*std_log_var_y#Nt*Nd,Na*Nf\n",
    "                ignore_mask = np.bitwise_or(mask,clip_mask)#Nt*Nd,Na*Nf\n",
    "            else:\n",
    "                ignore_mask = mask\n",
    "            keep_mask = np.bitwise_not(ignore_mask)#Nt*Nd,Na*Nf\n",
    "            weight = 1./(var_y+1e-6)#Nt*Nd,Na*Nf\n",
    "            weight_norm = np.stack([np.percentile(weight[keep_mask[:,i],i],50) for i in range(Na*Nf)],axis=-1)\n",
    "            weight /= weight_norm + 1e-6\n",
    "#             plt.hist(weight.flatten(),bins=20)\n",
    "#             plt.show()\n",
    "            weight = np.ones(y.shape)\n",
    "            weight[ignore_mask] = 0.\n",
    "             \n",
    "            t,d = coords\n",
    "            t_scale = t.std() + 1e-6\n",
    "            d_scale = np.sqrt((d.std(axis=0)**2).mean()) + 1e-6\n",
    "            ls_scale = (t_scale,d_scale)\n",
    "            t = (t - t.mean()) / t_scale\n",
    "            d = (d - d.mean(axis=0)) / d_scale\n",
    "            X = Smoothing._make_coord_array(t,d)#Nt*Nd,3\n",
    "            ###\n",
    "            # set Z explicitly to spacing of ::3 in time\n",
    "            model_kwargs['Z'] = Smoothing._make_coord_array(t[::3],d)\n",
    "            \n",
    "            with tf.Session(graph=tf.Graph()) as sess:\n",
    "                lock.acquire() if lock is not None else None\n",
    "                try:\n",
    "                    model = self._build_model(m_type, sess, weight, X, y, ls_scale, y_scale, **model_kwargs)\n",
    "                finally:\n",
    "                    lock.release() if lock is not None else None\n",
    "                logging.warning(\"Initial log-likelihood {}\".format(model.compute_log_likelihood()))\n",
    "                opt = gp.train.AdamOptimizer(1e-2)\n",
    "                opt.minimize(model, maxiter=iterations)\n",
    "                logging.warning(\"Final log-likelihood {}\".format(model.compute_log_likelihood()))\n",
    "                # smooth\n",
    "                kern_lengthscales = (\n",
    "                        model.kern.rbf_1.lengthscales.value[0]*ls_scale[0],\n",
    "                        model.kern.rbf_2.lengthscales.value[0]*ls_scale[1]\n",
    "                        )\n",
    "                kern_variance = model.kern.rbf_1.variance.value*model.kern.rbf_2.variance.value*y_scale**2\n",
    "                if verbose:\n",
    "                    logging.warning(model)\n",
    "                    logging.warning(kern_lengthscales)\n",
    "                    logging.warning(kern_variance)\n",
    "                predict_minibatch = 1000\n",
    "                for start in range(0,X.shape[0],predict_minibatch):\n",
    "                    stop = min(start+predict_minibatch,X.shape[0])\n",
    "                    Xs = X[start:stop,:]\n",
    "                    ystar,varstar = model.predict_y(Xs)#batch,Na\n",
    "                    ystar = ystar * y_scale + y_mean\n",
    "                    varstar = varstar * y_scale**2\n",
    "                    \n",
    "                    y[start:stop,:] = ystar\n",
    "                    var_y[start:stop,:] = varstar\n",
    "            phase = y.reshape([Nt,Nd,Na,Nf]).transpose((2,0,1,3))\n",
    "            variance = var_y.reshape([Nt,Nd,Na,Nf]).transpose((2,0,1,3))\n",
    "            return phase,variance,kern_lengthscales, kern_variance\n",
    "        except Exception as e:\n",
    "            logging.warning(e)\n",
    "            \n",
    "    def solve_and_apply_ensemble(self, save_datapack, ant_idx, time_idx, dir_idx, freq_idx, iterations,\n",
    "                                 interval, shift, init_solutions = None, num_threads=1,verbose=False,\n",
    "                                 model_kwargs = {}):\n",
    "        \"\"\"\n",
    "        Solve the problem using model_kwargs and then take an ensemble average over interval\n",
    "        and shift.\n",
    "        \"\"\"\n",
    "        if init_solutions is not None:\n",
    "            data = np.load(init_solutions)\n",
    "\n",
    "            kern_ls = data['kern_ls']\n",
    "            kern_var = data['kern_var']\n",
    "            kern_times = data['time']\n",
    "            kern_antenna_labels = data['antenna']\n",
    "\n",
    "        \n",
    "        datapack = self.datapack\n",
    "        directions, patch_names = datapack.get_directions(dir_idx)\n",
    "        times,timestamps = datapack.get_times(time_idx)\n",
    "        antennas,antenna_labels = datapack.get_antennas(ant_idx)\n",
    "        freqs = datapack.get_freqs(freq_idx)\n",
    "\n",
    "        if ant_idx is -1:\n",
    "            ant_idx = range(len(antennas))\n",
    "        if time_idx is -1:\n",
    "            time_idx = range(len(times))\n",
    "        if freq_idx is -1:\n",
    "            freq_idx = range(len(freqs))\n",
    "        if dir_idx is -1:\n",
    "            dir_idx = range(len(directions))\n",
    "\n",
    "        phase = datapack.get_phase(ant_idx,time_idx,dir_idx,freq_idx)\n",
    "        Na,Nt,Nd,Nf = phase.shape\n",
    "        logging.warning(\"Working on shapes {}\".format(phase.shape))\n",
    "\n",
    "        assert interval <= Nt\n",
    "\n",
    "        variance = datapack.get_variance(ant_idx,time_idx,dir_idx,freq_idx)\n",
    "        error = np.sqrt(variance)\n",
    "        data_mask = variance < 0\n",
    "        error[data_mask] = -1\n",
    "        logging.warning(\"Total masked phases: {}\".format(np.sum(data_mask)))\n",
    "\n",
    "        t = times.mjd*86400.#mjs\n",
    "        d = np.array([directions.ra.deg, directions.dec.deg]).T\n",
    "        \n",
    "        lock = Lock()\n",
    "        with futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "            jobs = []\n",
    "\n",
    "            for j,aj in enumerate(time_idx[::shift]):\n",
    "                start = j*shift\n",
    "                stop = min(start+interval,Nt)\n",
    "                time_slice = slice(start,stop,1)\n",
    "                \n",
    "                if init_solutions is not None:\n",
    "                    ###\n",
    "                    # interpolate kern_params with this interval/shift\n",
    "                    mean_time = np.mean(times.gps[time_slice])\n",
    "\n",
    "                    model_kwargs['ls_init'] = (\n",
    "                            np.interp(mean_time, kern_times, kern_ls[ant_idx,:,1].mean(0)),\n",
    "                            np.interp(mean_time, kern_times, kern_ls[ant_idx,:,0].mean(0))\n",
    "                            )\n",
    "                    #logging.warning(model_kwargs['ls_init'])\n",
    "                    \n",
    "                # initial ls_scale (if they exist)\n",
    "\n",
    "# (phase, error, coords, lock, error_sigma_clip=4., m_type='sgp', \n",
    "#                         iterations=1000, pargs=None,verbose=False,model_kwargs={}):\n",
    "                jobs.append(executor.submit(\n",
    "                    self._solve_interval,\n",
    "                    phase[:,time_slice,:,:],\n",
    "                    error[:,time_slice,:,:],\n",
    "                    (t[time_slice],d),\n",
    "                    lock,\n",
    "                    error_sigma_clip = None,\n",
    "                    m_type='sgp',\n",
    "                    iterations=iterations,\n",
    "                    pargs=\"Working on time chunk ({}) {} to ({}) {}\".format(\n",
    "                        start,timestamps[start],stop-1,timestamps[stop-1]),\n",
    "                    verbose=verbose,\n",
    "                    model_kwargs = model_kwargs\n",
    "                    )\n",
    "                    )\n",
    "            results = futures.wait(jobs)\n",
    "            if verbose:\n",
    "                logging.warning(results)\n",
    "            results = [j.result() for j in jobs]\n",
    "\n",
    "            phase_mean = np.zeros(phase.shape)\n",
    "            phase_weights = np.zeros(phase.shape)\n",
    "            variance_mean = np.zeros(variance.shape)\n",
    "            res_idx = 0\n",
    "            for j,aj in enumerate(time_idx[::shift]):\n",
    "                start = j*interval\n",
    "                stop = min((j+1)*interval,Nt)\n",
    "                time_slice = slice(start,stop,1)\n",
    "                res = results[res_idx]\n",
    "                p,v,kern_ls,kern_var = res\n",
    "                phase_mean[:,time_slice,:,:] += p/(v+1e-3)\n",
    "                phase_weights[:,time_slice,:,:] += 1./(v+1e-3)\n",
    "                res_idx += 1\n",
    "            variance_mean = 1./(phase_weights+1e-3)\n",
    "            phase_mean /= (phase_weights+1e-3)\n",
    "            datapack.set_phase(phase_mean, ant_idx=ant_idx,time_idx=time_idx,dir_idx=dir_idx,freq_idx=freq_idx)\n",
    "            datapack.set_variance(variance_mean, ant_idx=ant_idx,time_idx=time_idx,dir_idx=dir_idx,freq_idx=freq_idx)\n",
    "            datapack.save(save_datapack)\n",
    "            \n",
    "    \n",
    "    def solve_and_apply_ensemble_blocked(self, save_datapack, ant_idx, time_idx, dir_idx, freq_idx, \n",
    "                                 interval, shift, num_blocks = 4, num_threads=1,verbose=False):\n",
    "        \n",
    "        datapack = self.datapack\n",
    "        directions, patch_names = datapack.get_directions(dir_idx)\n",
    "        times,timestamps = datapack.get_times(time_idx)\n",
    "        antennas,antenna_labels = datapack.get_antennas(ant_idx)\n",
    "        freqs = datapack.get_freqs(freq_idx)\n",
    "\n",
    "        if ant_idx is -1:\n",
    "            ant_idx = range(len(antennas))\n",
    "        if time_idx is -1:\n",
    "            time_idx = range(len(times))\n",
    "        if freq_idx is -1:\n",
    "            freq_idx = range(len(freqs))\n",
    "        if dir_idx is -1:\n",
    "            dir_idx = range(len(directions))\n",
    "\n",
    "        phase = datapack.get_phase(ant_idx,time_idx,dir_idx,freq_idx)\n",
    "        Na,Nt,Nd,Nf = phase.shape\n",
    "        logging.warning(\"Working on shapes {}\".format(phase.shape))\n",
    "\n",
    "        assert interval <= Nt\n",
    "\n",
    "        variance = datapack.get_variance(ant_idx,time_idx,dir_idx,freq_idx)\n",
    "        error = np.sqrt(variance)\n",
    "        data_mask = variance < 0\n",
    "        error[data_mask] = -1\n",
    "        logging.warning(\"Total masked phases: {}\".format(np.sum(data_mask)))\n",
    "\n",
    "        t = times.mjd*86400.#mjs\n",
    "        d = np.array([directions.ra.deg, directions.dec.deg]).T\n",
    "        f = freqs\n",
    "        coords = (t,d,f)\n",
    "        \n",
    "        lock = Lock()\n",
    "        with futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "            jobs = []\n",
    "            mean_count = np.zeros(phase.shape)\n",
    "\n",
    "            for i,ai in enumerate(ant_idx):\n",
    "                for j,aj in enumerate(time_idx[::shift]):\n",
    "                    start = j*shift\n",
    "                    stop = min(start+interval,Nt)\n",
    "                    time_slice = slice(start,stop,time_sampling)\n",
    "\n",
    "                    ###\n",
    "                    # interpolate kern_params with this interval/shift\n",
    "                    mean_time = np.mean(times.gps[time_slice])\n",
    "                    \n",
    "                    # d, t, f, v\n",
    "                    kern_params = [\n",
    "                            np.interp(mean_time, kern_times, kern_ls[i,:,0]),\n",
    "                            np.interp(mean_time, kern_times, kern_ls[i,:,1]),\n",
    "                            np.interp(mean_time, kern_times, kern_ls[i,:,2]),\n",
    "                            np.interp(mean_time, kern_times, kern_var[i,:,0])\n",
    "                            ]\n",
    "\n",
    "                    mean_count[i,time_slice,directional_slice,freq_slice] += 1\n",
    "                    \n",
    "                    \n",
    "#                    for l,al in enumerate(freq_idx):\n",
    "#                        freq_slice = slice(l,l+1)\n",
    "                    jobs.append(executor.submit(\n",
    "                        Smoothing._apply_block_svgp,\n",
    "                        phase[i,time_slice,directional_slice,freq_slice].copy(),\n",
    "                        error[i,time_slice,directional_slice,freq_slice].copy(),\n",
    "                        (t[time_slice],d[directional_slice],f[freq_slice]),\n",
    "                        lock,\n",
    "                        kern_params=kern_params,\n",
    "                        pargs=\"Working on {} time chunk ({}) {} to ({}) {} at {} to {} MHz\".format(antenna_labels[i],\n",
    "                            start,timestamps[start],stop-1,timestamps[stop-1], freqs[0]/1e6, freqs[-1]/1e6),\n",
    "                        verbose=verbose\n",
    "                        )\n",
    "                        )\n",
    "            results = futures.wait(jobs)\n",
    "            if verbose:\n",
    "                logging.warning(results)\n",
    "            results = [j.result() for j in jobs]\n",
    "\n",
    "            phase_mean = np.zeros(phase.shape)\n",
    "            variance_mean = np.zeros(variance.shape)\n",
    "            res_idx = 0\n",
    "            for i,ai in enumerate(ant_idx):\n",
    "                for j,aj in enumerate(time_idx[::shift]):\n",
    "                    start = j*interval\n",
    "                    stop = min((j+1)*interval,Nt)\n",
    "                    time_slice = slice(start,stop,time_sampling)\n",
    "                    res = results[res_idx]\n",
    "                    phase_mean[i,time_slice,directional_slice,freq_slice] += res[0]\n",
    "                    variance_mean[i,time_slice,directional_slice,freq_slice] += res[1]\n",
    "                    res_idx += 1\n",
    "            phase_mean /= mean_count\n",
    "            variance_mean /= mean_count\n",
    "            datapack.set_phase(phase_mean, ant_idx=ant_idx,time_idx=time_idx,dir_idx=dir_idx,freq_idx=freq_idx)\n",
    "            datapack.set_variance(variance_mean, ant_idx=ant_idx,time_idx=time_idx,dir_idx=dir_idx,freq_idx=freq_idx)\n",
    "\n",
    "            datapack.save(save_datapack)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    import os\n",
    "\n",
    "    if len(sys.argv) == 2:\n",
    "        starting_datapack = sys.argv[1]\n",
    "    else:\n",
    "        starting_datapack = \"../../data/rvw_datapack_full_phase_dec27.hdf5\"\n",
    "    smoothing = Smoothing(starting_datapack)\n",
    "    model_kwargs = {'minibatch_size':500, 'M':40*15,'feature_trainable':False,'ls_init':(300,1.5),\n",
    "                         'ls_trainable':(True,True,True), 'verbose':False,'likelihood_var_trainable':True}\n",
    "    smoothing.solve_and_apply_ensemble(starting_datapack.replace('.hdf5','_smoothed_ensemble.hdf5'),\n",
    "                                       range(5,7), -1, -1, range(2), iterations=1000,\n",
    "                                 interval = 30, shift = 6, init_solutions='../../bayes/gp_params_fixed_scales.npz',\n",
    "                                       num_threads=1,verbose=True,model_kwargs = model_kwargs)\n",
    "\n",
    "#     smoothing.solve_time_intervals(\"gp_params.npz\",range(1,62),-1,-1,range(0,20),32,32,num_threads=16,verbose=True)\n",
    "#    refined_params = smoothing.refine_statistics_timeonly('gp_params.npz')\n",
    "#    print(refined_params.shape)\n",
    "#    smoothing.solve_time_intervals(\"gp_params_fixed_scales.npz\",range(1,62),-1,-1,range(0,20),32,32,num_threads=16,verbose=True,refined_params=refined_params)\n",
    "#    plt.ion()\n",
    "#    smoothing.refine_statistics_timeonly('gp_params.npz')\n",
    "#    smoothing.refine_statistics('gp_params.npz')\n",
    "#    smoothing.refine_statistics_timeonly('gp_params_fixed_scales.npz')\n",
    "#    smoothing.refine_statistics('gp_params_fixed_scales.npz')\n",
    "#    plt.ioff()\n",
    "#     smoothing.apply_solutions(starting_datapack.replace('.hdf5','_refined_smoothed.hdf5'), \n",
    "#             \"gp_params_fixed_scales.npz\",range(1,62), -1, -1, range(0,20), 32, 32, num_threads=1,verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
